{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Homework2.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"c8hVRhtfpmhA","colab_type":"text"},"source":["# MLiM 25440 Homework 2 : Programming Assignment\n","### Brendan Sanderson"]},{"cell_type":"markdown","metadata":{"id":"vvXkmyMxPyLQ","colab_type":"text"},"source":["#### Imports and Mounting"]},{"cell_type":"code","metadata":{"id":"Kmh9XyVApuHJ","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","import time\n","from sklearn.feature_selection import SelectKBest, chi2\n","from sklearn.gaussian_process.kernels import RBF\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import f1_score\n","\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers, models, optimizers, utils"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d1F17G7Sp6zy","colab_type":"code","outputId":"03a931bd-cb2a-4324-bc9b-de024a634a66","executionInfo":{"status":"ok","timestamp":1571361919847,"user_tz":300,"elapsed":1643,"user":{"displayName":"Brendan Sanderson","photoUrl":"","userId":"05405297474563600402"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LuRu4jUN8Mjw","colab_type":"text"},"source":["#### Helper functions"]},{"cell_type":"code","metadata":{"id":"x7iwMqsc9EEb","colab_type":"code","colab":{}},"source":["# Timer helper class for benchmarking reading methods\n","class Timer(object):\n","    \"\"\"Timer class\n","       Wrap a will with a timing function\n","    \"\"\"\n","    \n","    def __init__(self, elapsed):\n","        self.elapsed = elapsed\n","        \n","    def __enter__(self):\n","        self.t = time.time()\n","        \n","    def __exit__(self, *args, **kwargs):\n","        tim =  time.time() - self.t\n","        self.elapsed.append(tim)\n","        \n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-KHrO1MIrMTQ","colab_type":"text"},"source":["This function will run the 5-fold cross validation with a given model, X and Y. It will return the confusions matrix, the f1 score, the percent error and the total runtime to fit the model."]},{"cell_type":"code","metadata":{"id":"eblSsWWLlnuV","colab_type":"code","colab":{}},"source":["#Run cross validation with 5-folds and returns the confusion matrix\n","\n","def five_fold_predict_stats(model, X, Y, c=2):\n","  cm = np.zeros((c,c), dtype=int)\n","  f1s = []\n","  accs = []\n","  tim = []\n","  unique_labels = np.unique(Y)\n","  kf = KFold(n_splits=5)\n","  #Run 5 times with each fold\n","  for train_index, test_index in kf.split(X):\n","      X_train, X_test = X.values[train_index], X.values[test_index]\n","      y_train, y_test = Y[train_index], Y[test_index]\n","      with Timer(tim):\n","        model.fit(X_train, y_train)\n","      y_hat = model.predict(X_test)\n","      cm +=confusion_matrix(y_test, y_hat, labels=unique_labels)\n","      f1s.append(f1_score(y_test, y_hat, average='micro'))\n","      accs.append(np.mean(y_hat != y_test))\n","  \n","  #Return the aggregate of the scores for all folds\n","  return (cm, np.mean(f1s), np.mean(accs),np.mean(tim))\n","\n","#Print statement for printing the results from the five fold confusion matrix function\n","\n","def print_five_fold_results(dataset_name, model_name, results):\n","  print(\"\")\n","  print(\"----------------------------------------------------\")\n","  print(\"Results for running\", model_name, \"with the\", dataset_name,\"Dataset\")\n","  print(\"The confusion matrix is:\")\n","  print(results[0])\n","  print(\"The f1 score is:\", results[1])\n","  print(\"The percent error is:\", results[2])\n","  print(\"The average runtime for training is:\", results[3], \"seconds\")\n","  print(\"----------------------------------------------------\")\n","  print(\"\")\n","  \n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_V9slb8Rpu_X","colab_type":"text"},"source":["## Part 1. Normal and Tumor Match Pair Analysis (30 points)"]},{"cell_type":"markdown","metadata":{"id":"OLXJEA5swvWM","colab_type":"text"},"source":["#### Loading and Preprocessing the data"]},{"cell_type":"code","metadata":{"id":"TS0OPFh1p7Il","colab_type":"code","colab":{}},"source":["nt_coding = pd.read_csv(\n","    \"/content/drive/My Drive/MLiM-Datasets/HW2/nt.coding.csv\",\n","    dtype=float,\n","    keep_default_na=False,\n","    na_values=[])\n","\n","nt_all = pd.read_csv(\n","    \"/content/drive/My Drive/MLiM-Datasets/HW2/nt.all.csv\",\n","    dtype=float,\n","    keep_default_na=False,\n","    na_values=[])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjO-kv4Vw2Ks","colab_type":"code","colab":{}},"source":["#Savings the labels as a seperate data frame and removes them from the original dataframe.\n","\n","nt_labels = nt_coding.pop(\"Type\").values\n","_ = nt_all.pop(\"Type\").values\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tVnOqIa5yv8t","colab_type":"code","colab":{}},"source":["# Normalize the data\n","\n","def normalize_large_data(df):\n","  scaler = preprocessing.StandardScaler()\n","  columns = df.columns\n","  scaled_df = scaler.fit_transform(df)\n","  return pd.DataFrame(scaled_df, columns=columns)\n","\n","nt_coding = normalize_large_data(nt_coding)\n","nt_all = normalize_large_data(nt_all)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vHV75G5Hpzff","colab_type":"text"},"source":["### a) Using SciKit Learn build a machine learning classifier that takes RNAseq profiles from matched normal tumor pairs and classifies the sample as Normal or Tumor. Compare the nt.coding.csv vs the nt.all.csv."]},{"cell_type":"markdown","metadata":{"id":"6sV04jharp0C","colab_type":"text"},"source":["I am chosing to use a Linear SVM model for this part. I chose this model because it gives very accurate results in a shorter runtime. It is a fairly standard model, which will provide a good measure of which dataset is better."]},{"cell_type":"code","metadata":{"id":"iu-Dz5sXw1l1","colab_type":"code","outputId":"d7afdd7b-7ed3-481d-c13a-230c4ad1110b","executionInfo":{"status":"ok","timestamp":1571344229260,"user_tz":300,"elapsed":151100,"user":{"displayName":"Brendan Sanderson","photoUrl":"","userId":"05405297474563600402"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"source":["#Run the predict function on both data sets and print\n","cr = five_fold_predict_stats(LinearSVC(tol=1e-5, random_state=0),nt_coding, nt_labels)\n","ar = five_fold_predict_stats(LinearSVC(tol=1e-5, random_state=0),nt_all, nt_labels)\n","\n","print_five_fold_results(\"Protein Coding Genes\", \"Linear SVM\", cr)\n","print_five_fold_results(\"All Genes\", \"Linear SVM\", ar)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","----------------------------------------------------\n","Results for running Linear SVM with the Protein Coding Genes Dataset\n","The confusion matrix is:\n","[[680  20]\n"," [ 20 680]]\n","The f1 score is: 0.9714285714285715\n","The percent error is: 0.02857142857142857\n","The average runtime for training is: 4.023158025741577 seconds\n","----------------------------------------------------\n","\n","\n","----------------------------------------------------\n","Results for running Linear SVM with the All Genes Dataset\n","The confusion matrix is:\n","[[682  18]\n"," [ 26 674]]\n","The f1 score is: 0.9685714285714286\n","The percent error is: 0.03142857142857143\n","The average runtime for training is: 6.419310569763184 seconds\n","----------------------------------------------------\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t_z4g0cPT9aW","colab_type":"text"},"source":["The protein coding gene dataset did slightly better than the all gene dataset in terms of testing error by a slight margin. This seems to allude that the protein coding genes are definitely the more relevant ones and thus in the dataset with all genes, there is a lot of data that is just noise. This could lead the model to slightly overfit to the data and fit to genes that just coincidentally appear to be correlated with the classification. In addition, the coding data run faster. The only thing that the all data set performed better on was the false positives. \n","\n","It is important to note that this difference is extremely small. However, as the coding dataset did perform better, I will use only that one going forward."]},{"cell_type":"markdown","metadata":{"id":"Ga5ZXn5rD0E0","colab_type":"text"},"source":["### b) Using model selection methods of your choice determine which classical ML method performs best on the NT classification problem.\n"]},{"cell_type":"markdown","metadata":{"id":"_kCNSUxIa6rN","colab_type":"text"},"source":["Becaues the coding dataset performed better and has much faster run times, I am going to continue with only using that dataset."]},{"cell_type":"code","metadata":{"id":"N1bPV4GuD0NI","colab_type":"code","outputId":"f88203fb-4713-41ab-aed8-2ef1d9346ffb","executionInfo":{"status":"ok","timestamp":1571345527233,"user_tz":300,"elapsed":903752,"user":{"displayName":"Brendan Sanderson","photoUrl":"","userId":"05405297474563600402"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#Names of different classifiers that we will use\n","names = [\"Linear SVM\", \"Auto SVM\", \"Nearest Neighbors\", \n","         \"Decision Tree\", \"Random Forest\", \"AdaBoost\"]\n","\n","#Instantiating list of classifiers\n","classifiers = [\n","    LinearSVC(random_state=0, tol=1e-5),\n","    SVC(random_state=0, gamma='auto'),\n","    KNeighborsClassifier(3),\n","    DecisionTreeClassifier(random_state=0),\n","    RandomForestClassifier(random_state=0, n_estimators = 25),\n","    AdaBoostClassifier(random_state=0)\n","]\n","\n","#Run the 5-fold fit and prediction test on each classifier.\n","classifier_results = []\n","for name, clf in zip(names, classifiers):\n","  results = five_fold_predict_stats(clf, nt_coding, nt_labels)\n","  print_five_fold_results(\"Protein Coding Genes\", name, results)\n","  classifier_results.append(results)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","----------------------------------------------------\n","Results for running Linear SVM with the Protein Coding Genes Dataset\n","The confusion matrix is:\n","[[680  20]\n"," [ 20 680]]\n","The f1 score is: 0.9714285714285715\n","The percent error is: 0.02857142857142857\n","The average runtime for training is: 4.1091948509216305 seconds\n","----------------------------------------------------\n","\n","\n","----------------------------------------------------\n","Results for running Auto SVM with the Protein Coding Genes Dataset\n","The confusion matrix is:\n","[[673  27]\n"," [ 16 684]]\n","The f1 score is: 0.9692857142857143\n","The percent error is: 0.03071428571428571\n","The average runtime for training is: 15.001606607437134 seconds\n","----------------------------------------------------\n","\n","\n","----------------------------------------------------\n","Results for running Nearest Neighbors with the Protein Coding Genes Dataset\n","The confusion matrix is:\n","[[687  13]\n"," [125 575]]\n","The f1 score is: 0.9014285714285715\n","The percent error is: 0.09857142857142856\n","The average runtime for training is: 0.785841417312622 seconds\n","----------------------------------------------------\n","\n","\n","----------------------------------------------------\n","Results for running Decision Tree with the Protein Coding Genes Dataset\n","The confusion matrix is:\n","[[647  53]\n"," [ 59 641]]\n","The f1 score is: 0.9199999999999999\n","The percent error is: 0.08\n","The average runtime for training is: 13.976138973236084 seconds\n","----------------------------------------------------\n","\n","\n","----------------------------------------------------\n","Results for running Random Forest with the Protein Coding Genes Dataset\n","The confusion matrix is:\n","[[679  21]\n"," [ 21 679]]\n","The f1 score is: 0.97\n","The percent error is: 0.03\n","The average runtime for training is: 1.9077746868133545 seconds\n","----------------------------------------------------\n","\n","\n","----------------------------------------------------\n","Results for running AdaBoost with the Protein Coding Genes Dataset\n","The confusion matrix is:\n","[[680  20]\n"," [ 20 680]]\n","The f1 score is: 0.9714285714285715\n","The percent error is: 0.02857142857142857\n","The average runtime for training is: 125.74347743988037 seconds\n","----------------------------------------------------\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4HNXjpcfWGx9","colab_type":"text"},"source":["All of the models performed fairly well and non had an error more than 0.1. The Linear SVM tied for the best error with the AdaBoost. However, the Linear SVM performed much faster. Thus, I would conclude that the Linear SVM is generally the best model.\n","\n","Although the Linear SVM is generally the best model, there are still specific cases where other models may be better. \n","\n","The RandomForest performed only slightly worse, but has a significantly faster runtime. This means that if your dataset is extremely large, it may be better to use the RandomForest instead of LinearSVM. \n","\n","The BestKNeighbors may be the best if you really don't want false positives. It had the least amount of false positives, but a low accuracy. However, if the tradeoff is worth it to you, then this could be a good option."]},{"cell_type":"markdown","metadata":{"id":"esSUzEmuD0WE","colab_type":"text"},"source":["### c) Using feature selection methods of your choice determine a < 100 gene signature that can be used to classify Normal vs Tumor."]},{"cell_type":"markdown","metadata":{"id":"htF_J8dizY0e","colab_type":"text"},"source":["First, we are going to use SelectKBest to reduce the features down to 10,000. After, this I will run ExtraTreesClassifier and remove the bottom 20% of useful features iterateively. Everytime I remove features, I will calculate the error and the runtime of training and testing the model with Linear SVM."]},{"cell_type":"code","metadata":{"id":"G5M9zRsF0ugG","colab_type":"code","colab":{}},"source":["#A wrapper function for get k best\n","def get_K_best(df, labels, k):\n","  mini = np.min(np.min(df))\n","  selectKBest = SelectKBest(chi2, k=k)\n","  select_df = selectKBest.fit_transform(df-mini, labels)+mini\n","  column_inds = selectKBest.get_support(indices = True)\n","  new_columns = [df.columns[i] for i in column_inds]\n","  return pd.DataFrame(df, columns = new_columns)\n","\n","#Get the 10,000 best features\n","nt_10k_coding = get_K_best(nt_coding, nt_labels, 10000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WuMZ6pdUD0fZ","colab_type":"code","colab":{}},"source":["nt_select_coding = nt_10k_coding.copy()\n","nt_select_labels = nt_labels\n","\n","model = ExtraTreesClassifier(n_estimators = 50)\n","errors = []\n","times = []\n","\n","#First, get the error with all features\n","r = five_fold_predict_stats(LinearSVC(tol=1e-5, random_state=0),nt_coding, nt_labels)\n","features = [len(nt_coding.columns)]\n","errors.append(r[2])\n","times.append(r[3])\n","\n","#Next, get the error with 10000 features\n","r = five_fold_predict_stats(LinearSVC(tol=1e-5, random_state=0),nt_select_coding, nt_labels)\n","errors.append(r[2])\n","times.append(r[3])\n","features.append(10000)\n","\n","#Set the number of max iterations\n","mi = 1000\n","\n","#Run 20 interations of removing the bottom 20% features.\n","for i in range(20):\n","  model.fit(nt_select_coding, nt_select_labels)\n","  feature_rankings = model.feature_importances_\n","  sorted_test = sorted(list(enumerate(feature_rankings)),key= lambda x: x[1])\n","  nt_select_coding = nt_select_coding.drop(columns=[nt_select_coding.columns[x[0]] for x in sorted_test[:int(len(sorted_test)*0.2)]])\n","  \n","  #With less features, the model can take longer to converge, so we will increase the max iterations\n","  if (len(nt_select_coding.columns) < 3000):\n","    mi = 2500\n","  if (len(nt_select_coding.columns) < 1000):\n","    mi = 5000\n","    \n","  #Run the 5-fold predict test\n","  r = five_fold_predict_stats(LinearSVC(max_iter= mi, tol=1e-5, random_state=0),nt_select_coding, nt_labels)\n","  errors.append(r[2])\n","  times.append(r[3])\n","  features.append(len(nt_select_coding.columns))\n","\n","   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hp0tr9zFiYY-","colab_type":"code","outputId":"bf219b6f-647c-4cdd-e9aa-9a133aedee95","executionInfo":{"status":"ok","timestamp":1571345639127,"user_tz":300,"elapsed":302766,"user":{"displayName":"Brendan Sanderson","photoUrl":"","userId":"05405297474563600402"}},"colab":{"base_uri":"https://localhost:8080/","height":350}},"source":["plt.figure(figsize=(16, 5))\n","plt.subplot(1,2,1)\n","plt.plot(features,errors)\n","plt.title(\"Testing Error with Different Numbers of Features\")\n","plt.xlabel(\"Number of Features\")\n","plt.ylabel(\"Testing Error\")\n","plt.subplot(1,2,2)\n","plt.plot(features,times)\n","plt.title(\"Average Training Runtime with Different Numbers of Features\")\n","plt.xlabel(\"Number of Features\")\n","plt.ylabel(\"Average Runtime\")\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAA8IAAAFNCAYAAADLktaJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 1152x360 with 2 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"3fUZoG-hXh0R","colab_type":"text"},"source":["The testing error generally trended towards going up. This is not surprising as less features seem to indicate less data. Interestingly though, that at only around 2,750 features, the testing error still had not increased dramatically. It increased by less than 0.5%. The significant thing here is the dramatic decrease in runtime. At 2,750, the runtime is 1/8 what it was for all of the features. This shows that feature reduction is definitely a valid method if the dataset is too large and the runtime is too long. "]},{"cell_type":"markdown","metadata":{"id":"6gxxKRHP8gEV","colab_type":"text"},"source":["### d) Using Keras, build a deep learning classifier that performs the same classification task, and determine the learning curve (relationship of number of training samples to prediction accuracy) for your network, recommend using at least 10 training set sizes to estimate the learning curve."]},{"cell_type":"markdown","metadata":{"id":"u8Yoc4Octg1K","colab_type":"text"},"source":["For this section, I tried it first with two different models: A CNN similar to that in class and a fully connected NN similar to the other one in class. I found that they both performed well, but the fully connected one was way faster, so I decided to use that one. I have left the functions for both networks before. Also, I used only 10 epochs as I found that this was enough for the error to converge."]},{"cell_type":"code","metadata":{"id":"7f3cyJbHDVrg","colab_type":"code","colab":{}},"source":["#A cnn model for classification\n","def classification_model_cnn(i,c):\n","  model = models.Sequential()\n","  model.add(layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='valid', input_shape=(i,1)))\n","  model.add(layers.Activation('relu'))\n","  model.add(layers.MaxPooling1D(pool_size=1))\n","  model.add(layers.Conv1D(filters=16, kernel_size=1, strides=1, padding='valid'))\n","  model.add(layers.Activation('relu'))\n","  model.add(layers.MaxPooling1D(pool_size=10))\n","  model.add(layers.Flatten())\n","  model.add(layers.Dense(200))\n","  model.add(layers.Activation('relu'))\n","  model.add(layers.Dropout(0.1))\n","  model.add(layers.Dense(20))\n","  model.add(layers.Activation('relu'))\n","  model.add(layers.Dropout(0.1))\n","  model.add(layers.Dense(c))\n","  model.add(layers.Activation('softmax'))\n","  model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.SGD(), metrics=['accuracy'])\n","  return model\n","\n","\n","#A fully connected model for classification\n","def classification_model_fc(i,c):\n","  model = models.Sequential()\n","  model.add(layers.InputLayer(i,))\n","  model.add(layers.Dense(512, activation='relu'))\n","  model.add(layers.Dense(256, activation='relu'))\n","  model.add(layers.Dense(c, activation='softmax'))\n","  model.compile(loss=\"categorical_crossentropy\", optimizer='rmsprop', metrics=['accuracy'])\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FsFEl6GkVQo4","colab_type":"code","colab":{}},"source":["#Runs 5 fold on the model and returns the error on a seperate testing set\n","def five_fold_nn_samples(model, X, y, eps, samples = 1000):\n","  train_errors = []\n","  valid_errors = []\n","  test_errors = []\n","  ns = 5\n","  if samples < 10:\n","    ns = 2\n","  kf = KFold(n_splits=ns)\n","  X_test = X[-400:]\n","  Y_test = y[-400:]\n","  model.save_weights('empty')\n","  for train_index, test_index in kf.split(X[:samples]):\n","      X_train, X_val = X.values[train_index], X.values[test_index]\n","      y_train, y_val = utils.to_categorical(y)[train_index], utils.to_categorical(y)[test_index]\n","      hist = model.fit(x=X_train, y=y_train, epochs=eps, batch_size=25, shuffle=True, validation_data=(np.array(X_val), y_val))\n","      train_errors.append(hist.history.get('acc')[-1])\n","      valid_errors.append(hist.history.get('val_acc')[-1])\n","      test_errors.append(np.mean([x[1] for x in np.rint(model.predict(X_test))] != Y_test))\n","      model.load_weights('empty')\n","  return np.mean(test_errors)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JJamJNE0WXg-","colab_type":"code","outputId":"24caa90b-71cc-4c12-e7ea-1a8cfcb3d043","executionInfo":{"status":"ok","timestamp":1571349367621,"user_tz":300,"elapsed":75471,"user":{"displayName":"Brendan Sanderson","photoUrl":"","userId":"05405297474563600402"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["set_sizes = [3,7,15,30,60,125,250,500,1000]\n","testing_errors = []\n","for s in set_sizes:\n","  testing_errors.append(five_fold_nn_samples(classification_model_fc(len(nt_coding.columns),2), nt_coding, nt_labels, 10, samples=s))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","Train on 1 samples, validate on 2 samples\n","Epoch 1/10\n","1/1 [==============================] - 1s 1s/sample - loss: 0.6154 - acc: 1.0000 - val_loss: 2.5266 - val_acc: 0.5000\n","Epoch 2/10\n","1/1 [==============================] - 0s 26ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.5266 - val_acc: 0.5000\n","Epoch 3/10\n","1/1 [==============================] - 0s 17ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.5266 - val_acc: 0.5000\n","Epoch 4/10\n","1/1 [==============================] - 0s 17ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.5266 - val_acc: 0.5000\n","Epoch 5/10\n","1/1 [==============================] - 0s 17ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.5266 - val_acc: 0.5000\n","Epoch 6/10\n","1/1 [==============================] - 0s 18ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.5266 - val_acc: 0.5000\n","Epoch 7/10\n","1/1 [==============================] - 0s 18ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.5266 - val_acc: 0.5000\n","Epoch 8/10\n","1/1 [==============================] - 0s 19ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.5266 - val_acc: 0.5000\n","Epoch 9/10\n","1/1 [==============================] - 0s 21ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.5266 - val_acc: 0.5000\n","Epoch 10/10\n","1/1 [==============================] - 0s 17ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.5266 - val_acc: 0.5000\n","Train on 2 samples, validate on 1 samples\n","Epoch 1/10\n","2/2 [==============================] - 0s 15ms/sample - loss: 0.6306 - acc: 0.5000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 2/10\n","2/2 [==============================] - 0s 8ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 3/10\n","2/2 [==============================] - 0s 8ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 4/10\n","2/2 [==============================] - 0s 9ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 5/10\n","2/2 [==============================] - 0s 8ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 6/10\n","2/2 [==============================] - 0s 9ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 7/10\n","2/2 [==============================] - 0s 8ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 8/10\n","2/2 [==============================] - 0s 8ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 9/10\n","2/2 [==============================] - 0s 9ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 10/10\n","2/2 [==============================] - 0s 9ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Train on 3 samples, validate on 4 samples\n","Epoch 1/10\n","3/3 [==============================] - 0s 43ms/sample - loss: 1.6371 - acc: 0.0000e+00 - val_loss: 4.1538 - val_acc: 0.7500\n","Epoch 2/10\n","3/3 [==============================] - 0s 5ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 4.1538 - val_acc: 0.7500\n","Epoch 3/10\n","3/3 [==============================] - 0s 5ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 4.1538 - val_acc: 0.7500\n","Epoch 4/10\n","3/3 [==============================] - 0s 6ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 4.1538 - val_acc: 0.7500\n","Epoch 5/10\n","3/3 [==============================] - 0s 5ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 4.1538 - val_acc: 0.7500\n","Epoch 6/10\n","3/3 [==============================] - 0s 5ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 4.1538 - val_acc: 0.7500\n","Epoch 7/10\n","3/3 [==============================] - 0s 5ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 4.1538 - val_acc: 0.7500\n","Epoch 8/10\n","3/3 [==============================] - 0s 6ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 4.1538 - val_acc: 0.7500\n","Epoch 9/10\n","3/3 [==============================] - 0s 6ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 4.1538 - val_acc: 0.7500\n","Epoch 10/10\n","3/3 [==============================] - 0s 5ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 4.1538 - val_acc: 0.7500\n","Train on 4 samples, validate on 3 samples\n","Epoch 1/10\n","4/4 [==============================] - 0s 4ms/sample - loss: 1.5307 - acc: 0.2500 - val_loss: 2.6424e-05 - val_acc: 1.0000\n","Epoch 2/10\n","4/4 [==============================] - 0s 4ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.6424e-05 - val_acc: 1.0000\n","Epoch 3/10\n","4/4 [==============================] - 0s 4ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.6424e-05 - val_acc: 1.0000\n","Epoch 4/10\n","4/4 [==============================] - 0s 4ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.6424e-05 - val_acc: 1.0000\n","Epoch 5/10\n","4/4 [==============================] - 0s 4ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.6424e-05 - val_acc: 1.0000\n","Epoch 6/10\n","4/4 [==============================] - 0s 4ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.6424e-05 - val_acc: 1.0000\n","Epoch 7/10\n","4/4 [==============================] - 0s 4ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.6424e-05 - val_acc: 1.0000\n","Epoch 8/10\n","4/4 [==============================] - 0s 5ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.6424e-05 - val_acc: 1.0000\n","Epoch 9/10\n","4/4 [==============================] - 0s 4ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.6424e-05 - val_acc: 1.0000\n","Epoch 10/10\n","4/4 [==============================] - 0s 5ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 2.6424e-05 - val_acc: 1.0000\n","Train on 12 samples, validate on 3 samples\n","Epoch 1/10\n","12/12 [==============================] - 0s 15ms/sample - loss: 0.9332 - acc: 0.5000 - val_loss: 10.8042 - val_acc: 0.6667\n","Epoch 2/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0105 - acc: 1.0000 - val_loss: 11.0611 - val_acc: 0.6667\n","Epoch 3/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 11.0611 - val_acc: 0.6667\n","Epoch 4/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 11.0611 - val_acc: 0.6667\n","Epoch 5/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 11.0611 - val_acc: 0.6667\n","Epoch 6/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 11.0611 - val_acc: 0.6667\n","Epoch 7/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 11.0611 - val_acc: 0.6667\n","Epoch 8/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 11.0611 - val_acc: 0.6667\n","Epoch 9/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 11.0611 - val_acc: 0.6667\n","Epoch 10/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 11.0611 - val_acc: 0.6667\n","Train on 12 samples, validate on 3 samples\n","Epoch 1/10\n","12/12 [==============================] - 0s 1ms/sample - loss: 1.0782 - acc: 0.3333 - val_loss: 3.4173e-06 - val_acc: 1.0000\n","Epoch 2/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 4.9671e-08 - acc: 1.0000 - val_loss: 3.4173e-06 - val_acc: 1.0000\n","Epoch 3/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 4.9671e-08 - acc: 1.0000 - val_loss: 3.4571e-06 - val_acc: 1.0000\n","Epoch 4/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 4.9671e-08 - acc: 1.0000 - val_loss: 3.4571e-06 - val_acc: 1.0000\n","Epoch 5/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 4.9671e-08 - acc: 1.0000 - val_loss: 3.4571e-06 - val_acc: 1.0000\n","Epoch 6/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 3.9736e-08 - acc: 1.0000 - val_loss: 3.4571e-06 - val_acc: 1.0000\n","Epoch 7/10\n","12/12 [==============================] - 0s 1ms/sample - loss: 3.9736e-08 - acc: 1.0000 - val_loss: 3.4571e-06 - val_acc: 1.0000\n","Epoch 8/10\n","12/12 [==============================] - 0s 1ms/sample - loss: 3.9736e-08 - acc: 1.0000 - val_loss: 3.4968e-06 - val_acc: 1.0000\n","Epoch 9/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 3.9736e-08 - acc: 1.0000 - val_loss: 3.4968e-06 - val_acc: 1.0000\n","Epoch 10/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 3.9736e-08 - acc: 1.0000 - val_loss: 3.4968e-06 - val_acc: 1.0000\n","Train on 12 samples, validate on 3 samples\n","Epoch 1/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 1.0056 - acc: 0.4167 - val_loss: 0.8391 - val_acc: 0.6667\n","Epoch 2/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0364 - acc: 1.0000 - val_loss: 1.7057 - val_acc: 0.6667\n","Epoch 3/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 1.7057 - val_acc: 0.6667\n","Epoch 4/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 1.7057 - val_acc: 0.6667\n","Epoch 5/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 1.7057 - val_acc: 0.6667\n","Epoch 6/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 1.7057 - val_acc: 0.6667\n","Epoch 7/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 1.7057 - val_acc: 0.6667\n","Epoch 8/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 1.7057 - val_acc: 0.6667\n","Epoch 9/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 1.7057 - val_acc: 0.6667\n","Epoch 10/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 1.7057 - val_acc: 0.6667\n","Train on 12 samples, validate on 3 samples\n","Epoch 1/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.9842 - acc: 0.4167 - val_loss: 0.1435 - val_acc: 1.0000\n","Epoch 2/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 2.8014e-06 - acc: 1.0000 - val_loss: 0.1326 - val_acc: 1.0000\n","Epoch 3/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 2.2649e-06 - acc: 1.0000 - val_loss: 0.1273 - val_acc: 1.0000\n","Epoch 4/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 1.9371e-06 - acc: 1.0000 - val_loss: 0.1234 - val_acc: 1.0000\n","Epoch 5/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 1.7285e-06 - acc: 1.0000 - val_loss: 0.1200 - val_acc: 1.0000\n","Epoch 6/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 1.5696e-06 - acc: 1.0000 - val_loss: 0.1171 - val_acc: 1.0000\n","Epoch 7/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 1.4603e-06 - acc: 1.0000 - val_loss: 0.1144 - val_acc: 1.0000\n","Epoch 8/10\n","12/12 [==============================] - 0s 1ms/sample - loss: 1.3610e-06 - acc: 1.0000 - val_loss: 0.1119 - val_acc: 1.0000\n","Epoch 9/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 1.2815e-06 - acc: 1.0000 - val_loss: 0.1096 - val_acc: 1.0000\n","Epoch 10/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 1.2020e-06 - acc: 1.0000 - val_loss: 0.1074 - val_acc: 1.0000\n","Train on 12 samples, validate on 3 samples\n","Epoch 1/10\n","12/12 [==============================] - 0s 1ms/sample - loss: 1.0427 - acc: 0.3333 - val_loss: 16.6257 - val_acc: 0.3333\n","Epoch 2/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 16.6257 - val_acc: 0.3333\n","Epoch 3/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 16.6257 - val_acc: 0.3333\n","Epoch 4/10\n","12/12 [==============================] - 0s 1ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 16.6257 - val_acc: 0.3333\n","Epoch 5/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 16.6257 - val_acc: 0.3333\n","Epoch 6/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 16.6257 - val_acc: 0.3333\n","Epoch 7/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 16.6257 - val_acc: 0.3333\n","Epoch 8/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 16.6257 - val_acc: 0.3333\n","Epoch 9/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 16.6257 - val_acc: 0.3333\n","Epoch 10/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 16.6257 - val_acc: 0.3333\n","Train on 24 samples, validate on 6 samples\n","Epoch 1/10\n","24/24 [==============================] - 0s 7ms/sample - loss: 0.6364 - acc: 0.6667 - val_loss: 4.9961e-05 - val_acc: 1.0000\n","Epoch 2/10\n","24/24 [==============================] - 0s 987us/sample - loss: 1.0840 - acc: 0.9583 - val_loss: 4.5885e-04 - val_acc: 1.0000\n","Epoch 3/10\n","24/24 [==============================] - 0s 885us/sample - loss: 3.9871 - acc: 0.9167 - val_loss: 0.4936 - val_acc: 0.8333\n","Epoch 4/10\n","24/24 [==============================] - 0s 802us/sample - loss: 0.4895 - acc: 0.9167 - val_loss: 0.2165 - val_acc: 0.8333\n","Epoch 5/10\n","24/24 [==============================] - 0s 1ms/sample - loss: 0.1836 - acc: 0.9583 - val_loss: 9.1590e-06 - val_acc: 1.0000\n","Epoch 6/10\n","24/24 [==============================] - 0s 781us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 9.1590e-06 - val_acc: 1.0000\n","Epoch 7/10\n","24/24 [==============================] - 0s 758us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 9.1590e-06 - val_acc: 1.0000\n","Epoch 8/10\n","24/24 [==============================] - 0s 853us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 9.1590e-06 - val_acc: 1.0000\n","Epoch 9/10\n","24/24 [==============================] - 0s 828us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 9.1590e-06 - val_acc: 1.0000\n","Epoch 10/10\n","24/24 [==============================] - 0s 815us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 9.1590e-06 - val_acc: 1.0000\n","Train on 24 samples, validate on 6 samples\n","Epoch 1/10\n","24/24 [==============================] - 0s 787us/sample - loss: 0.5636 - acc: 0.6667 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 2/10\n","24/24 [==============================] - 0s 775us/sample - loss: 2.6523e-06 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 3/10\n","24/24 [==============================] - 0s 864us/sample - loss: 1.7086e-06 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 4/10\n","24/24 [==============================] - 0s 903us/sample - loss: 1.3212e-06 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 5/10\n","24/24 [==============================] - 0s 794us/sample - loss: 1.1325e-06 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 6/10\n","24/24 [==============================] - 0s 801us/sample - loss: 1.0580e-06 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 7/10\n","24/24 [==============================] - 0s 793us/sample - loss: 9.9340e-07 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 8/10\n","24/24 [==============================] - 0s 807us/sample - loss: 9.2883e-07 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 9/10\n","24/24 [==============================] - 0s 780us/sample - loss: 8.7419e-07 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 10/10\n","24/24 [==============================] - 0s 857us/sample - loss: 8.1956e-07 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Train on 24 samples, validate on 6 samples\n","Epoch 1/10\n","24/24 [==============================] - 0s 844us/sample - loss: 0.6288 - acc: 0.6667 - val_loss: 15.2301 - val_acc: 0.3333\n","Epoch 2/10\n","24/24 [==============================] - 0s 948us/sample - loss: 0.5727 - acc: 0.9583 - val_loss: 9.6493 - val_acc: 0.8333\n","Epoch 3/10\n","24/24 [==============================] - 0s 951us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 9.6493 - val_acc: 0.8333\n","Epoch 4/10\n","24/24 [==============================] - 0s 975us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 9.6493 - val_acc: 0.8333\n","Epoch 5/10\n","24/24 [==============================] - 0s 835us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 9.6493 - val_acc: 0.8333\n","Epoch 6/10\n","24/24 [==============================] - 0s 874us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 9.6493 - val_acc: 0.8333\n","Epoch 7/10\n","24/24 [==============================] - 0s 813us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 9.6493 - val_acc: 0.8333\n","Epoch 8/10\n","24/24 [==============================] - 0s 879us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 9.6493 - val_acc: 0.8333\n","Epoch 9/10\n","24/24 [==============================] - 0s 950us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 9.6493 - val_acc: 0.8333\n","Epoch 10/10\n","24/24 [==============================] - 0s 869us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 9.6493 - val_acc: 0.8333\n","Train on 24 samples, validate on 6 samples\n","Epoch 1/10\n","24/24 [==============================] - 0s 863us/sample - loss: 0.6814 - acc: 0.5833 - val_loss: 9.3803 - val_acc: 0.6667\n","Epoch 2/10\n","24/24 [==============================] - 0s 924us/sample - loss: 0.4490 - acc: 0.9583 - val_loss: 6.3604 - val_acc: 0.6667\n","Epoch 3/10\n","24/24 [==============================] - 0s 859us/sample - loss: 2.1752 - acc: 0.8750 - val_loss: 14.5001 - val_acc: 0.6667\n","Epoch 4/10\n","24/24 [==============================] - 0s 825us/sample - loss: 2.4595 - acc: 0.9583 - val_loss: 11.2604 - val_acc: 0.6667\n","Epoch 5/10\n","24/24 [==============================] - 0s 790us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 11.2604 - val_acc: 0.6667\n","Epoch 6/10\n","24/24 [==============================] - 0s 775us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 11.2604 - val_acc: 0.6667\n","Epoch 7/10\n","24/24 [==============================] - 0s 785us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 11.2604 - val_acc: 0.6667\n","Epoch 8/10\n","24/24 [==============================] - 0s 820us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 11.2604 - val_acc: 0.6667\n","Epoch 9/10\n","24/24 [==============================] - 0s 751us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 11.2604 - val_acc: 0.6667\n","Epoch 10/10\n","24/24 [==============================] - 0s 1ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 11.2604 - val_acc: 0.6667\n","Train on 24 samples, validate on 6 samples\n","Epoch 1/10\n","24/24 [==============================] - 0s 766us/sample - loss: 0.6914 - acc: 0.5833 - val_loss: 6.4375 - val_acc: 0.8333\n","Epoch 2/10\n","24/24 [==============================] - 0s 890us/sample - loss: 4.3213e-07 - acc: 1.0000 - val_loss: 6.4373 - val_acc: 0.8333\n","Epoch 3/10\n","24/24 [==============================] - 0s 831us/sample - loss: 4.2716e-07 - acc: 1.0000 - val_loss: 6.4370 - val_acc: 0.8333\n","Epoch 4/10\n","24/24 [==============================] - 0s 850us/sample - loss: 4.2220e-07 - acc: 1.0000 - val_loss: 6.4367 - val_acc: 0.8333\n","Epoch 5/10\n","24/24 [==============================] - 0s 763us/sample - loss: 4.2220e-07 - acc: 1.0000 - val_loss: 6.4364 - val_acc: 0.8333\n","Epoch 6/10\n","24/24 [==============================] - 0s 777us/sample - loss: 4.1723e-07 - acc: 1.0000 - val_loss: 6.4361 - val_acc: 0.8333\n","Epoch 7/10\n","24/24 [==============================] - 0s 843us/sample - loss: 4.1723e-07 - acc: 1.0000 - val_loss: 6.4357 - val_acc: 0.8333\n","Epoch 8/10\n","24/24 [==============================] - 0s 781us/sample - loss: 4.1226e-07 - acc: 1.0000 - val_loss: 6.4354 - val_acc: 0.8333\n","Epoch 9/10\n","24/24 [==============================] - 0s 831us/sample - loss: 4.0730e-07 - acc: 1.0000 - val_loss: 6.4351 - val_acc: 0.8333\n","Epoch 10/10\n","24/24 [==============================] - 0s 845us/sample - loss: 4.0730e-07 - acc: 1.0000 - val_loss: 6.4347 - val_acc: 0.8333\n","Train on 48 samples, validate on 12 samples\n","Epoch 1/10\n","48/48 [==============================] - 0s 4ms/sample - loss: 4.2994 - acc: 0.5208 - val_loss: 5.4889 - val_acc: 0.5833\n","Epoch 2/10\n","48/48 [==============================] - 0s 808us/sample - loss: 3.5982 - acc: 0.8750 - val_loss: 2.1067 - val_acc: 0.9167\n","Epoch 3/10\n","48/48 [==============================] - 0s 862us/sample - loss: 0.1510 - acc: 0.9583 - val_loss: 0.0100 - val_acc: 1.0000\n","Epoch 4/10\n","48/48 [==============================] - 0s 961us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0100 - val_acc: 1.0000\n","Epoch 5/10\n","48/48 [==============================] - 0s 791us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0100 - val_acc: 1.0000\n","Epoch 6/10\n","48/48 [==============================] - 0s 795us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0100 - val_acc: 1.0000\n","Epoch 7/10\n","48/48 [==============================] - 0s 788us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0100 - val_acc: 1.0000\n","Epoch 8/10\n","48/48 [==============================] - 0s 797us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0100 - val_acc: 1.0000\n","Epoch 9/10\n","48/48 [==============================] - 0s 785us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0100 - val_acc: 1.0000\n","Epoch 10/10\n","48/48 [==============================] - 0s 805us/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.0100 - val_acc: 1.0000\n","Train on 48 samples, validate on 12 samples\n","Epoch 1/10\n","48/48 [==============================] - 0s 825us/sample - loss: 2.6634 - acc: 0.6042 - val_loss: 12.5196 - val_acc: 0.4167\n","Epoch 2/10\n","48/48 [==============================] - 0s 829us/sample - loss: 1.4997 - acc: 0.9167 - val_loss: 7.2024 - val_acc: 0.5833\n","Epoch 3/10\n","48/48 [==============================] - 0s 789us/sample - loss: 0.7232 - acc: 0.9792 - val_loss: 7.3429 - val_acc: 0.5833\n","Epoch 4/10\n","48/48 [==============================] - 0s 808us/sample - loss: 1.0093e-04 - acc: 1.0000 - val_loss: 7.3094 - val_acc: 0.5833\n","Epoch 5/10\n","48/48 [==============================] - 0s 827us/sample - loss: 5.4284e-06 - acc: 1.0000 - val_loss: 7.3077 - val_acc: 0.5833\n","Epoch 6/10\n","48/48 [==============================] - 0s 766us/sample - loss: 4.6661e-06 - acc: 1.0000 - val_loss: 7.3062 - val_acc: 0.5833\n","Epoch 7/10\n","48/48 [==============================] - 0s 805us/sample - loss: 4.1248e-06 - acc: 1.0000 - val_loss: 7.3049 - val_acc: 0.5833\n","Epoch 8/10\n","48/48 [==============================] - 0s 793us/sample - loss: 3.6704e-06 - acc: 1.0000 - val_loss: 7.3036 - val_acc: 0.5833\n","Epoch 9/10\n","48/48 [==============================] - 0s 907us/sample - loss: 3.2781e-06 - acc: 1.0000 - val_loss: 7.3023 - val_acc: 0.5833\n","Epoch 10/10\n","48/48 [==============================] - 0s 828us/sample - loss: 2.9403e-06 - acc: 1.0000 - val_loss: 7.3009 - val_acc: 0.5833\n","Train on 48 samples, validate on 12 samples\n","Epoch 1/10\n","48/48 [==============================] - 0s 844us/sample - loss: 1.7475 - acc: 0.6458 - val_loss: 6.9750 - val_acc: 0.8333\n","Epoch 2/10\n","48/48 [==============================] - 0s 843us/sample - loss: 1.3520 - acc: 0.9167 - val_loss: 8.6091 - val_acc: 0.6667\n","Epoch 3/10\n","48/48 [==============================] - 0s 797us/sample - loss: 0.5687 - acc: 0.9583 - val_loss: 5.7883 - val_acc: 0.7500\n","Epoch 4/10\n","48/48 [==============================] - 0s 819us/sample - loss: 0.9624 - acc: 0.9375 - val_loss: 4.5169 - val_acc: 0.9167\n","Epoch 5/10\n","48/48 [==============================] - 0s 879us/sample - loss: 1.9868e-08 - acc: 1.0000 - val_loss: 4.5169 - val_acc: 0.9167\n","Epoch 6/10\n","48/48 [==============================] - 0s 797us/sample - loss: 1.9868e-08 - acc: 1.0000 - val_loss: 4.5169 - val_acc: 0.9167\n","Epoch 7/10\n","48/48 [==============================] - 0s 786us/sample - loss: 1.9868e-08 - acc: 1.0000 - val_loss: 4.5169 - val_acc: 0.9167\n","Epoch 8/10\n","48/48 [==============================] - 0s 792us/sample - loss: 1.9868e-08 - acc: 1.0000 - val_loss: 4.5169 - val_acc: 0.9167\n","Epoch 9/10\n","48/48 [==============================] - 0s 832us/sample - loss: 1.9868e-08 - acc: 1.0000 - val_loss: 4.5169 - val_acc: 0.9167\n","Epoch 10/10\n","48/48 [==============================] - 0s 907us/sample - loss: 1.9868e-08 - acc: 1.0000 - val_loss: 4.5169 - val_acc: 0.9167\n","Train on 48 samples, validate on 12 samples\n","Epoch 1/10\n","48/48 [==============================] - 0s 846us/sample - loss: 3.7377 - acc: 0.6250 - val_loss: 1.3366 - val_acc: 0.8333\n","Epoch 2/10\n","48/48 [==============================] - 0s 799us/sample - loss: 2.6672 - acc: 0.8125 - val_loss: 0.3401 - val_acc: 0.9167\n","Epoch 3/10\n","48/48 [==============================] - 0s 819us/sample - loss: 0.1431 - acc: 0.9583 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 4/10\n","48/48 [==============================] - 0s 764us/sample - loss: 0.6900 - acc: 0.9792 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 5/10\n","48/48 [==============================] - 0s 847us/sample - loss: 1.7707e-06 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 6/10\n","48/48 [==============================] - 0s 999us/sample - loss: 1.3535e-06 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 7/10\n","48/48 [==============================] - 0s 771us/sample - loss: 1.0778e-06 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 8/10\n","48/48 [==============================] - 0s 822us/sample - loss: 8.5432e-07 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 9/10\n","48/48 [==============================] - 0s 881us/sample - loss: 7.1524e-07 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Epoch 10/10\n","48/48 [==============================] - 0s 790us/sample - loss: 6.0597e-07 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n","Train on 48 samples, validate on 12 samples\n","Epoch 1/10\n","48/48 [==============================] - 0s 836us/sample - loss: 2.3814 - acc: 0.5833 - val_loss: 0.9069 - val_acc: 0.9167\n","Epoch 2/10\n","48/48 [==============================] - 0s 776us/sample - loss: 0.4413 - acc: 0.9583 - val_loss: 0.2743 - val_acc: 0.8333\n","Epoch 3/10\n","48/48 [==============================] - 0s 765us/sample - loss: 1.0313 - acc: 0.9375 - val_loss: 0.1276 - val_acc: 0.9167\n","Epoch 4/10\n","48/48 [==============================] - 0s 805us/sample - loss: 5.0461e-06 - acc: 1.0000 - val_loss: 0.1276 - val_acc: 0.9167\n","Epoch 5/10\n","48/48 [==============================] - 0s 777us/sample - loss: 4.7581e-06 - acc: 1.0000 - val_loss: 0.1277 - val_acc: 0.9167\n","Epoch 6/10\n","48/48 [==============================] - 0s 844us/sample - loss: 4.4899e-06 - acc: 1.0000 - val_loss: 0.1277 - val_acc: 0.9167\n","Epoch 7/10\n","48/48 [==============================] - 0s 810us/sample - loss: 4.2640e-06 - acc: 1.0000 - val_loss: 0.1277 - val_acc: 0.9167\n","Epoch 8/10\n","48/48 [==============================] - 0s 770us/sample - loss: 4.0156e-06 - acc: 1.0000 - val_loss: 0.1277 - val_acc: 0.9167\n","Epoch 9/10\n","48/48 [==============================] - 0s 781us/sample - loss: 3.7847e-06 - acc: 1.0000 - val_loss: 0.1277 - val_acc: 0.9167\n","Epoch 10/10\n","48/48 [==============================] - 0s 814us/sample - loss: 3.5885e-06 - acc: 1.0000 - val_loss: 0.1277 - val_acc: 0.9167\n","Train on 100 samples, validate on 25 samples\n","Epoch 1/10\n","100/100 [==============================] - 0s 3ms/sample - loss: 3.5918 - acc: 0.7000 - val_loss: 19.3995 - val_acc: 0.7200\n","Epoch 2/10\n","100/100 [==============================] - 0s 677us/sample - loss: 4.0928 - acc: 0.8800 - val_loss: 1.7387 - val_acc: 0.9200\n","Epoch 3/10\n","100/100 [==============================] - 0s 681us/sample - loss: 0.5956 - acc: 0.9800 - val_loss: 2.5877 - val_acc: 0.8800\n","Epoch 4/10\n","100/100 [==============================] - 0s 677us/sample - loss: 0.1107 - acc: 0.9700 - val_loss: 3.6174 - val_acc: 0.9200\n","Epoch 5/10\n","100/100 [==============================] - 0s 673us/sample - loss: 4.7071e-06 - acc: 1.0000 - val_loss: 3.6174 - val_acc: 0.9200\n","Epoch 6/10\n","100/100 [==============================] - 0s 653us/sample - loss: 4.6154e-06 - acc: 1.0000 - val_loss: 3.6175 - val_acc: 0.9200\n","Epoch 7/10\n","100/100 [==============================] - 0s 666us/sample - loss: 4.5522e-06 - acc: 1.0000 - val_loss: 3.6175 - val_acc: 0.9200\n","Epoch 8/10\n","100/100 [==============================] - 0s 686us/sample - loss: 4.4950e-06 - acc: 1.0000 - val_loss: 3.6176 - val_acc: 0.9200\n","Epoch 9/10\n","100/100 [==============================] - 0s 673us/sample - loss: 4.4306e-06 - acc: 1.0000 - val_loss: 3.6177 - val_acc: 0.9200\n","Epoch 10/10\n","100/100 [==============================] - 0s 688us/sample - loss: 4.3520e-06 - acc: 1.0000 - val_loss: 3.6178 - val_acc: 0.9200\n","Train on 100 samples, validate on 25 samples\n","Epoch 1/10\n","100/100 [==============================] - 0s 699us/sample - loss: 3.5788 - acc: 0.7400 - val_loss: 7.1625 - val_acc: 0.6000\n","Epoch 2/10\n","100/100 [==============================] - 0s 683us/sample - loss: 3.6595 - acc: 0.8300 - val_loss: 1.7718 - val_acc: 0.9200\n","Epoch 3/10\n","100/100 [==============================] - 0s 677us/sample - loss: 2.6019 - acc: 0.9400 - val_loss: 0.8440 - val_acc: 0.9200\n","Epoch 4/10\n","100/100 [==============================] - 0s 667us/sample - loss: 0.2954 - acc: 0.9600 - val_loss: 0.9478 - val_acc: 0.9600\n","Epoch 5/10\n","100/100 [==============================] - 0s 669us/sample - loss: 0.1315 - acc: 0.9800 - val_loss: 0.9232 - val_acc: 0.9600\n","Epoch 6/10\n","100/100 [==============================] - 0s 689us/sample - loss: 0.0026 - acc: 1.0000 - val_loss: 0.9018 - val_acc: 0.9600\n","Epoch 7/10\n","100/100 [==============================] - 0s 700us/sample - loss: 0.0484 - acc: 0.9800 - val_loss: 0.9411 - val_acc: 0.9200\n","Epoch 8/10\n","100/100 [==============================] - 0s 700us/sample - loss: 0.0328 - acc: 0.9900 - val_loss: 1.0290 - val_acc: 0.9200\n","Epoch 9/10\n","100/100 [==============================] - 0s 696us/sample - loss: 0.0353 - acc: 0.9900 - val_loss: 1.1083 - val_acc: 0.9200\n","Epoch 10/10\n","100/100 [==============================] - 0s 687us/sample - loss: 2.2624e-06 - acc: 1.0000 - val_loss: 1.1085 - val_acc: 0.9200\n","Train on 100 samples, validate on 25 samples\n","Epoch 1/10\n","100/100 [==============================] - 0s 696us/sample - loss: 2.5837 - acc: 0.7600 - val_loss: 0.8254 - val_acc: 0.9600\n","Epoch 2/10\n","100/100 [==============================] - 0s 690us/sample - loss: 1.3318 - acc: 0.9200 - val_loss: 19.8437 - val_acc: 0.7200\n","Epoch 3/10\n","100/100 [==============================] - 0s 696us/sample - loss: 2.3416 - acc: 0.9300 - val_loss: 3.4527 - val_acc: 0.9200\n","Epoch 4/10\n","100/100 [==============================] - 0s 687us/sample - loss: 0.2653 - acc: 0.9700 - val_loss: 3.2883 - val_acc: 0.8800\n","Epoch 5/10\n","100/100 [==============================] - 0s 650us/sample - loss: 0.2340 - acc: 0.9800 - val_loss: 3.0238 - val_acc: 0.9200\n","Epoch 6/10\n","100/100 [==============================] - 0s 651us/sample - loss: 1.6594e-05 - acc: 1.0000 - val_loss: 3.0233 - val_acc: 0.9200\n","Epoch 7/10\n","100/100 [==============================] - 0s 644us/sample - loss: 1.4244e-05 - acc: 1.0000 - val_loss: 3.0228 - val_acc: 0.9200\n","Epoch 8/10\n","100/100 [==============================] - 0s 642us/sample - loss: 1.1903e-05 - acc: 1.0000 - val_loss: 3.0223 - val_acc: 0.9200\n","Epoch 9/10\n","100/100 [==============================] - 0s 654us/sample - loss: 1.0058e-05 - acc: 1.0000 - val_loss: 3.0218 - val_acc: 0.9200\n","Epoch 10/10\n","100/100 [==============================] - 0s 655us/sample - loss: 8.4770e-06 - acc: 1.0000 - val_loss: 3.0213 - val_acc: 0.9200\n","Train on 100 samples, validate on 25 samples\n","Epoch 1/10\n","100/100 [==============================] - 0s 666us/sample - loss: 5.2110 - acc: 0.6800 - val_loss: 5.0019 - val_acc: 0.7600\n","Epoch 2/10\n","100/100 [==============================] - 0s 668us/sample - loss: 1.6024 - acc: 0.9200 - val_loss: 5.2295 - val_acc: 0.8400\n","Epoch 3/10\n","100/100 [==============================] - 0s 674us/sample - loss: 1.1653 - acc: 0.9600 - val_loss: 1.4890 - val_acc: 0.8800\n","Epoch 4/10\n","100/100 [==============================] - 0s 697us/sample - loss: 0.5952 - acc: 0.9900 - val_loss: 1.3992 - val_acc: 0.8400\n","Epoch 5/10\n","100/100 [==============================] - 0s 690us/sample - loss: 1.3911e-06 - acc: 1.0000 - val_loss: 1.3991 - val_acc: 0.8400\n","Epoch 6/10\n","100/100 [==============================] - 0s 703us/sample - loss: 1.3458e-06 - acc: 1.0000 - val_loss: 1.3990 - val_acc: 0.8400\n","Epoch 7/10\n","100/100 [==============================] - 0s 710us/sample - loss: 1.2838e-06 - acc: 1.0000 - val_loss: 1.3989 - val_acc: 0.8400\n","Epoch 8/10\n","100/100 [==============================] - 0s 654us/sample - loss: 1.2218e-06 - acc: 1.0000 - val_loss: 1.3988 - val_acc: 0.8400\n","Epoch 9/10\n","100/100 [==============================] - 0s 645us/sample - loss: 1.1527e-06 - acc: 1.0000 - val_loss: 1.3987 - val_acc: 0.8400\n","Epoch 10/10\n","100/100 [==============================] - 0s 696us/sample - loss: 1.0824e-06 - acc: 1.0000 - val_loss: 1.3985 - val_acc: 0.8400\n","Train on 100 samples, validate on 25 samples\n","Epoch 1/10\n","100/100 [==============================] - 0s 702us/sample - loss: 3.5153 - acc: 0.7000 - val_loss: 8.4782 - val_acc: 0.8400\n","Epoch 2/10\n","100/100 [==============================] - 0s 646us/sample - loss: 2.4811 - acc: 0.9200 - val_loss: 3.1152 - val_acc: 0.8800\n","Epoch 3/10\n","100/100 [==============================] - 0s 712us/sample - loss: 0.2748 - acc: 0.9700 - val_loss: 2.2098 - val_acc: 0.8800\n","Epoch 4/10\n","100/100 [==============================] - 0s 650us/sample - loss: 1.3529e-06 - acc: 1.0000 - val_loss: 2.2102 - val_acc: 0.8800\n","Epoch 5/10\n","100/100 [==============================] - 0s 658us/sample - loss: 1.3303e-06 - acc: 1.0000 - val_loss: 2.2106 - val_acc: 0.8800\n","Epoch 6/10\n","100/100 [==============================] - 0s 694us/sample - loss: 1.3041e-06 - acc: 1.0000 - val_loss: 2.2112 - val_acc: 0.8800\n","Epoch 7/10\n","100/100 [==============================] - 0s 652us/sample - loss: 1.2719e-06 - acc: 1.0000 - val_loss: 2.2119 - val_acc: 0.8800\n","Epoch 8/10\n","100/100 [==============================] - 0s 664us/sample - loss: 1.2349e-06 - acc: 1.0000 - val_loss: 2.2127 - val_acc: 0.8800\n","Epoch 9/10\n","100/100 [==============================] - 0s 702us/sample - loss: 1.1944e-06 - acc: 1.0000 - val_loss: 2.2135 - val_acc: 0.8800\n","Epoch 10/10\n","100/100 [==============================] - 0s 668us/sample - loss: 1.1467e-06 - acc: 1.0000 - val_loss: 2.2145 - val_acc: 0.8800\n","Train on 200 samples, validate on 50 samples\n","Epoch 1/10\n","200/200 [==============================] - 0s 2ms/sample - loss: 5.6110 - acc: 0.8050 - val_loss: 0.4200 - val_acc: 0.9000\n","Epoch 2/10\n","200/200 [==============================] - 0s 668us/sample - loss: 0.7787 - acc: 0.9400 - val_loss: 0.7694 - val_acc: 0.9600\n","Epoch 3/10\n","200/200 [==============================] - 0s 670us/sample - loss: 0.3103 - acc: 0.9550 - val_loss: 0.3188 - val_acc: 0.9600\n","Epoch 4/10\n","200/200 [==============================] - 0s 658us/sample - loss: 0.8244 - acc: 0.9500 - val_loss: 0.4098 - val_acc: 0.9800\n","Epoch 5/10\n","200/200 [==============================] - 0s 672us/sample - loss: 0.3134 - acc: 0.9850 - val_loss: 0.2516 - val_acc: 0.9800\n","Epoch 6/10\n","200/200 [==============================] - 0s 666us/sample - loss: 0.2431 - acc: 0.9850 - val_loss: 0.2053 - val_acc: 0.9800\n","Epoch 7/10\n","200/200 [==============================] - 0s 676us/sample - loss: 2.3412e-04 - acc: 1.0000 - val_loss: 0.1914 - val_acc: 0.9800\n","Epoch 8/10\n","200/200 [==============================] - 0s 662us/sample - loss: 8.1410e-06 - acc: 1.0000 - val_loss: 0.1912 - val_acc: 0.9800\n","Epoch 9/10\n","200/200 [==============================] - 0s 653us/sample - loss: 6.1205e-06 - acc: 1.0000 - val_loss: 0.1909 - val_acc: 0.9800\n","Epoch 10/10\n","200/200 [==============================] - 0s 636us/sample - loss: 4.4321e-06 - acc: 1.0000 - val_loss: 0.1907 - val_acc: 0.9800\n","Train on 200 samples, validate on 50 samples\n","Epoch 1/10\n","200/200 [==============================] - 0s 670us/sample - loss: 3.7142 - acc: 0.8000 - val_loss: 2.1009 - val_acc: 0.8400\n","Epoch 2/10\n","200/200 [==============================] - 0s 660us/sample - loss: 0.2343 - acc: 0.9750 - val_loss: 2.5629 - val_acc: 0.9200\n","Epoch 3/10\n","200/200 [==============================] - 0s 657us/sample - loss: 0.4350 - acc: 0.9800 - val_loss: 2.5669 - val_acc: 0.9200\n","Epoch 4/10\n","200/200 [==============================] - 0s 649us/sample - loss: 0.1661 - acc: 0.9900 - val_loss: 2.4616 - val_acc: 0.9200\n","Epoch 5/10\n","200/200 [==============================] - 0s 660us/sample - loss: 0.0783 - acc: 0.9900 - val_loss: 2.4683 - val_acc: 0.9200\n","Epoch 6/10\n","200/200 [==============================] - 0s 645us/sample - loss: 2.5111e-04 - acc: 1.0000 - val_loss: 2.4431 - val_acc: 0.9200\n","Epoch 7/10\n","200/200 [==============================] - 0s 645us/sample - loss: 3.1193e-06 - acc: 1.0000 - val_loss: 2.4442 - val_acc: 0.9200\n","Epoch 8/10\n","200/200 [==============================] - 0s 648us/sample - loss: 2.4781e-06 - acc: 1.0000 - val_loss: 2.4454 - val_acc: 0.9200\n","Epoch 9/10\n","200/200 [==============================] - 0s 647us/sample - loss: 1.9406e-06 - acc: 1.0000 - val_loss: 2.4464 - val_acc: 0.9200\n","Epoch 10/10\n","200/200 [==============================] - 0s 653us/sample - loss: 1.5943e-06 - acc: 1.0000 - val_loss: 2.4478 - val_acc: 0.9200\n","Train on 200 samples, validate on 50 samples\n","Epoch 1/10\n","200/200 [==============================] - 0s 621us/sample - loss: 6.1960 - acc: 0.7500 - val_loss: 1.3063 - val_acc: 0.8800\n","Epoch 2/10\n","200/200 [==============================] - 0s 651us/sample - loss: 0.9987 - acc: 0.9300 - val_loss: 3.0273 - val_acc: 0.9200\n","Epoch 3/10\n","200/200 [==============================] - 0s 617us/sample - loss: 1.1994 - acc: 0.9350 - val_loss: 0.3757 - val_acc: 0.9600\n","Epoch 4/10\n","200/200 [==============================] - 0s 641us/sample - loss: 3.9526e-04 - acc: 1.0000 - val_loss: 0.4150 - val_acc: 0.9400\n","Epoch 5/10\n","200/200 [==============================] - 0s 637us/sample - loss: 3.3291e-05 - acc: 1.0000 - val_loss: 0.4119 - val_acc: 0.9400\n","Epoch 6/10\n","200/200 [==============================] - 0s 655us/sample - loss: 2.0260e-05 - acc: 1.0000 - val_loss: 0.4099 - val_acc: 0.9400\n","Epoch 7/10\n","200/200 [==============================] - 0s 633us/sample - loss: 1.4149e-05 - acc: 1.0000 - val_loss: 0.4079 - val_acc: 0.9400\n","Epoch 8/10\n","200/200 [==============================] - 0s 631us/sample - loss: 9.9118e-06 - acc: 1.0000 - val_loss: 0.4071 - val_acc: 0.9400\n","Epoch 9/10\n","200/200 [==============================] - 0s 659us/sample - loss: 7.4132e-06 - acc: 1.0000 - val_loss: 0.4079 - val_acc: 0.9400\n","Epoch 10/10\n","200/200 [==============================] - 0s 625us/sample - loss: 5.5173e-06 - acc: 1.0000 - val_loss: 0.4095 - val_acc: 0.9400\n","Train on 200 samples, validate on 50 samples\n","Epoch 1/10\n","200/200 [==============================] - 0s 635us/sample - loss: 4.7075 - acc: 0.7800 - val_loss: 2.5294 - val_acc: 0.9000\n","Epoch 2/10\n","200/200 [==============================] - 0s 629us/sample - loss: 1.4719 - acc: 0.9050 - val_loss: 0.8708 - val_acc: 0.9600\n","Epoch 3/10\n","200/200 [==============================] - 0s 638us/sample - loss: 0.5089 - acc: 0.9750 - val_loss: 0.7325 - val_acc: 0.9600\n","Epoch 4/10\n","200/200 [==============================] - 0s 656us/sample - loss: 0.1456 - acc: 0.9750 - val_loss: 1.1286 - val_acc: 0.9400\n","Epoch 5/10\n","200/200 [==============================] - 0s 615us/sample - loss: 0.1329 - acc: 0.9800 - val_loss: 0.6689 - val_acc: 0.9600\n","Epoch 6/10\n","200/200 [==============================] - 0s 701us/sample - loss: 0.5026 - acc: 0.9550 - val_loss: 0.5249 - val_acc: 0.9800\n","Epoch 7/10\n","200/200 [==============================] - 0s 648us/sample - loss: 0.2558 - acc: 0.9850 - val_loss: 0.7558 - val_acc: 0.9800\n","Epoch 8/10\n","200/200 [==============================] - 0s 621us/sample - loss: 2.4232e-05 - acc: 1.0000 - val_loss: 0.7568 - val_acc: 0.9800\n","Epoch 9/10\n","200/200 [==============================] - 0s 630us/sample - loss: 1.1559e-05 - acc: 1.0000 - val_loss: 0.7574 - val_acc: 0.9800\n","Epoch 10/10\n","200/200 [==============================] - 0s 642us/sample - loss: 6.3463e-06 - acc: 1.0000 - val_loss: 0.7577 - val_acc: 0.9800\n","Train on 200 samples, validate on 50 samples\n","Epoch 1/10\n","200/200 [==============================] - 0s 654us/sample - loss: 3.3251 - acc: 0.7850 - val_loss: 0.8151 - val_acc: 0.9400\n","Epoch 2/10\n","200/200 [==============================] - 0s 667us/sample - loss: 2.8279 - acc: 0.8950 - val_loss: 0.3531 - val_acc: 0.9400\n","Epoch 3/10\n","200/200 [==============================] - 0s 626us/sample - loss: 0.6843 - acc: 0.9850 - val_loss: 0.5229 - val_acc: 0.9000\n","Epoch 4/10\n","200/200 [==============================] - 0s 625us/sample - loss: 0.2856 - acc: 0.9750 - val_loss: 0.2928 - val_acc: 0.9400\n","Epoch 5/10\n","200/200 [==============================] - 0s 629us/sample - loss: 2.4966e-04 - acc: 1.0000 - val_loss: 0.3231 - val_acc: 0.9400\n","Epoch 6/10\n","200/200 [==============================] - 0s 627us/sample - loss: 1.6491e-05 - acc: 1.0000 - val_loss: 0.3230 - val_acc: 0.9400\n","Epoch 7/10\n","200/200 [==============================] - 0s 597us/sample - loss: 1.2413e-05 - acc: 1.0000 - val_loss: 0.3228 - val_acc: 0.9400\n","Epoch 8/10\n","200/200 [==============================] - 0s 593us/sample - loss: 8.6668e-06 - acc: 1.0000 - val_loss: 0.3230 - val_acc: 0.9400\n","Epoch 9/10\n","200/200 [==============================] - 0s 593us/sample - loss: 7.4128e-06 - acc: 1.0000 - val_loss: 0.3233 - val_acc: 0.9400\n","Epoch 10/10\n","200/200 [==============================] - 0s 640us/sample - loss: 5.6479e-06 - acc: 1.0000 - val_loss: 0.3238 - val_acc: 0.9400\n","Train on 400 samples, validate on 100 samples\n","Epoch 1/10\n","400/400 [==============================] - 0s 1ms/sample - loss: 4.3676 - acc: 0.8375 - val_loss: 1.8573 - val_acc: 0.8800\n","Epoch 2/10\n","400/400 [==============================] - 0s 629us/sample - loss: 0.8271 - acc: 0.9575 - val_loss: 1.9450 - val_acc: 0.9100\n","Epoch 3/10\n","400/400 [==============================] - 0s 602us/sample - loss: 0.5980 - acc: 0.9700 - val_loss: 1.1504 - val_acc: 0.9200\n","Epoch 4/10\n","400/400 [==============================] - 0s 610us/sample - loss: 0.5654 - acc: 0.9750 - val_loss: 1.0536 - val_acc: 0.9500\n","Epoch 5/10\n","400/400 [==============================] - 0s 593us/sample - loss: 1.1511 - acc: 0.9700 - val_loss: 1.4109 - val_acc: 0.9500\n","Epoch 6/10\n","400/400 [==============================] - 0s 594us/sample - loss: 0.4874 - acc: 0.9850 - val_loss: 1.4202 - val_acc: 0.9500\n","Epoch 7/10\n","400/400 [==============================] - 0s 632us/sample - loss: 0.0688 - acc: 0.9975 - val_loss: 1.0831 - val_acc: 0.9600\n","Epoch 8/10\n","400/400 [==============================] - 0s 600us/sample - loss: 0.1658 - acc: 0.9975 - val_loss: 1.4772 - val_acc: 0.9400\n","Epoch 9/10\n","400/400 [==============================] - 0s 597us/sample - loss: 0.0835 - acc: 0.9975 - val_loss: 1.1062 - val_acc: 0.9600\n","Epoch 10/10\n","400/400 [==============================] - 0s 605us/sample - loss: 1.0669e-07 - acc: 1.0000 - val_loss: 1.1061 - val_acc: 0.9600\n","Train on 400 samples, validate on 100 samples\n","Epoch 1/10\n","400/400 [==============================] - 0s 612us/sample - loss: 4.3768 - acc: 0.7925 - val_loss: 0.2176 - val_acc: 0.9500\n","Epoch 2/10\n","400/400 [==============================] - 0s 594us/sample - loss: 1.3104 - acc: 0.9350 - val_loss: 0.5309 - val_acc: 0.9800\n","Epoch 3/10\n","400/400 [==============================] - 0s 610us/sample - loss: 1.0237 - acc: 0.9500 - val_loss: 0.1627 - val_acc: 0.9700\n","Epoch 4/10\n","400/400 [==============================] - 0s 601us/sample - loss: 0.3503 - acc: 0.9800 - val_loss: 0.7159 - val_acc: 0.9400\n","Epoch 5/10\n","400/400 [==============================] - 0s 584us/sample - loss: 0.6913 - acc: 0.9875 - val_loss: 0.0687 - val_acc: 0.9800\n","Epoch 6/10\n","400/400 [==============================] - 0s 611us/sample - loss: 1.1821 - acc: 0.9550 - val_loss: 0.1444 - val_acc: 0.9800\n","Epoch 7/10\n","400/400 [==============================] - 0s 615us/sample - loss: 0.0987 - acc: 0.9925 - val_loss: 0.0318 - val_acc: 0.9900\n","Epoch 8/10\n","400/400 [==============================] - 0s 625us/sample - loss: 0.4750 - acc: 0.9850 - val_loss: 0.3650 - val_acc: 0.9800\n","Epoch 9/10\n","400/400 [==============================] - 0s 605us/sample - loss: 0.3734 - acc: 0.9725 - val_loss: 0.2249 - val_acc: 0.9800\n","Epoch 10/10\n","400/400 [==============================] - 0s 609us/sample - loss: 0.1090 - acc: 0.9875 - val_loss: 7.0333e-08 - val_acc: 1.0000\n","Train on 400 samples, validate on 100 samples\n","Epoch 1/10\n","400/400 [==============================] - 0s 609us/sample - loss: 3.8822 - acc: 0.8375 - val_loss: 0.8739 - val_acc: 0.9700\n","Epoch 2/10\n","400/400 [==============================] - 0s 596us/sample - loss: 0.5148 - acc: 0.9625 - val_loss: 2.9807 - val_acc: 0.8500\n","Epoch 3/10\n","400/400 [==============================] - 0s 570us/sample - loss: 0.8756 - acc: 0.9550 - val_loss: 0.4728 - val_acc: 0.9600\n","Epoch 4/10\n","400/400 [==============================] - 0s 570us/sample - loss: 1.0249 - acc: 0.9650 - val_loss: 1.2295 - val_acc: 0.9700\n","Epoch 5/10\n","400/400 [==============================] - 0s 611us/sample - loss: 0.5525 - acc: 0.9750 - val_loss: 1.3028 - val_acc: 0.9800\n","Epoch 6/10\n","400/400 [==============================] - 0s 576us/sample - loss: 0.3319 - acc: 0.9850 - val_loss: 1.3561 - val_acc: 0.9600\n","Epoch 7/10\n","400/400 [==============================] - 0s 578us/sample - loss: 0.4114 - acc: 0.9925 - val_loss: 1.2322 - val_acc: 0.9800\n","Epoch 8/10\n","400/400 [==============================] - 0s 590us/sample - loss: 1.0609e-06 - acc: 1.0000 - val_loss: 1.2317 - val_acc: 0.9800\n","Epoch 9/10\n","400/400 [==============================] - 0s 602us/sample - loss: 8.4099e-07 - acc: 1.0000 - val_loss: 1.2313 - val_acc: 0.9800\n","Epoch 10/10\n","400/400 [==============================] - 0s 573us/sample - loss: 5.7964e-07 - acc: 1.0000 - val_loss: 1.2296 - val_acc: 0.9800\n","Train on 400 samples, validate on 100 samples\n","Epoch 1/10\n","400/400 [==============================] - 0s 574us/sample - loss: 3.4939 - acc: 0.8200 - val_loss: 2.4270 - val_acc: 0.8600\n","Epoch 2/10\n","400/400 [==============================] - 0s 576us/sample - loss: 0.6069 - acc: 0.9575 - val_loss: 0.5823 - val_acc: 0.9300\n","Epoch 3/10\n","400/400 [==============================] - 0s 589us/sample - loss: 0.7115 - acc: 0.9775 - val_loss: 0.6676 - val_acc: 0.9500\n","Epoch 4/10\n","400/400 [==============================] - 0s 581us/sample - loss: 0.6436 - acc: 0.9775 - val_loss: 0.9224 - val_acc: 0.9300\n","Epoch 5/10\n","400/400 [==============================] - 0s 579us/sample - loss: 0.1046 - acc: 0.9875 - val_loss: 0.6717 - val_acc: 0.9500\n","Epoch 6/10\n","400/400 [==============================] - 0s 575us/sample - loss: 0.2286 - acc: 0.9925 - val_loss: 1.5786 - val_acc: 0.9000\n","Epoch 7/10\n","400/400 [==============================] - 0s 583us/sample - loss: 0.2421 - acc: 0.9825 - val_loss: 2.0251 - val_acc: 0.9200\n","Epoch 8/10\n","400/400 [==============================] - 0s 557us/sample - loss: 0.3590 - acc: 0.9675 - val_loss: 0.6041 - val_acc: 0.9700\n","Epoch 9/10\n","400/400 [==============================] - 0s 548us/sample - loss: 0.6171 - acc: 0.9875 - val_loss: 0.6036 - val_acc: 0.9500\n","Epoch 10/10\n","400/400 [==============================] - 0s 576us/sample - loss: 0.0812 - acc: 0.9925 - val_loss: 1.2040 - val_acc: 0.9500\n","Train on 400 samples, validate on 100 samples\n","Epoch 1/10\n","400/400 [==============================] - 0s 577us/sample - loss: 2.9755 - acc: 0.8425 - val_loss: 2.7871 - val_acc: 0.9200\n","Epoch 2/10\n","400/400 [==============================] - 0s 565us/sample - loss: 0.9304 - acc: 0.9525 - val_loss: 4.8592 - val_acc: 0.8700\n","Epoch 3/10\n","400/400 [==============================] - 0s 575us/sample - loss: 0.5006 - acc: 0.9675 - val_loss: 2.2846 - val_acc: 0.9400\n","Epoch 4/10\n","400/400 [==============================] - 0s 565us/sample - loss: 0.1874 - acc: 0.9800 - val_loss: 7.8961 - val_acc: 0.8800\n","Epoch 5/10\n","400/400 [==============================] - 0s 604us/sample - loss: 1.1747 - acc: 0.9700 - val_loss: 4.0913 - val_acc: 0.9400\n","Epoch 6/10\n","400/400 [==============================] - 0s 592us/sample - loss: 3.3382e-04 - acc: 1.0000 - val_loss: 3.7167 - val_acc: 0.9400\n","Epoch 7/10\n","400/400 [==============================] - 0s 582us/sample - loss: 0.1917 - acc: 0.9875 - val_loss: 4.2082 - val_acc: 0.9300\n","Epoch 8/10\n","400/400 [==============================] - 0s 587us/sample - loss: 0.1624 - acc: 0.9950 - val_loss: 4.0049 - val_acc: 0.9300\n","Epoch 9/10\n","400/400 [==============================] - 0s 590us/sample - loss: 0.0732 - acc: 0.9925 - val_loss: 3.7249 - val_acc: 0.9200\n","Epoch 10/10\n","400/400 [==============================] - 0s 583us/sample - loss: 0.1266 - acc: 0.9925 - val_loss: 3.9874 - val_acc: 0.9300\n","Train on 800 samples, validate on 200 samples\n","Epoch 1/10\n","800/800 [==============================] - 1s 903us/sample - loss: 3.5274 - acc: 0.8525 - val_loss: 1.8722 - val_acc: 0.9050\n","Epoch 2/10\n","800/800 [==============================] - 0s 571us/sample - loss: 1.2808 - acc: 0.9287 - val_loss: 0.2542 - val_acc: 0.9800\n","Epoch 3/10\n","800/800 [==============================] - 0s 585us/sample - loss: 0.9614 - acc: 0.9513 - val_loss: 1.2328 - val_acc: 0.9450\n","Epoch 4/10\n","800/800 [==============================] - 0s 578us/sample - loss: 0.6440 - acc: 0.9638 - val_loss: 1.2671 - val_acc: 0.9450\n","Epoch 5/10\n","800/800 [==============================] - 0s 589us/sample - loss: 0.4737 - acc: 0.9750 - val_loss: 1.2640 - val_acc: 0.9500\n","Epoch 6/10\n","800/800 [==============================] - 0s 607us/sample - loss: 0.3921 - acc: 0.9750 - val_loss: 0.8615 - val_acc: 0.9600\n","Epoch 7/10\n","800/800 [==============================] - 0s 578us/sample - loss: 0.5840 - acc: 0.9725 - val_loss: 1.1037 - val_acc: 0.9550\n","Epoch 8/10\n","800/800 [==============================] - 0s 573us/sample - loss: 0.3736 - acc: 0.9762 - val_loss: 1.0814 - val_acc: 0.9700\n","Epoch 9/10\n","800/800 [==============================] - 0s 583us/sample - loss: 0.3411 - acc: 0.9800 - val_loss: 1.2954 - val_acc: 0.9600\n","Epoch 10/10\n","800/800 [==============================] - 0s 573us/sample - loss: 0.1442 - acc: 0.9925 - val_loss: 1.2034 - val_acc: 0.9750\n","Train on 800 samples, validate on 200 samples\n","Epoch 1/10\n","800/800 [==============================] - 0s 568us/sample - loss: 3.7845 - acc: 0.8512 - val_loss: 1.3885 - val_acc: 0.9200\n","Epoch 2/10\n","800/800 [==============================] - 0s 577us/sample - loss: 1.0954 - acc: 0.9500 - val_loss: 0.8521 - val_acc: 0.9050\n","Epoch 3/10\n","800/800 [==============================] - 0s 600us/sample - loss: 0.8462 - acc: 0.9575 - val_loss: 0.8702 - val_acc: 0.9500\n","Epoch 4/10\n","800/800 [==============================] - 0s 580us/sample - loss: 0.4769 - acc: 0.9675 - val_loss: 2.7031 - val_acc: 0.9300\n","Epoch 5/10\n","800/800 [==============================] - 0s 579us/sample - loss: 0.6849 - acc: 0.9650 - val_loss: 1.0053 - val_acc: 0.9600\n","Epoch 6/10\n","800/800 [==============================] - 0s 590us/sample - loss: 0.2423 - acc: 0.9725 - val_loss: 1.4650 - val_acc: 0.9600\n","Epoch 7/10\n","800/800 [==============================] - 0s 582us/sample - loss: 0.3776 - acc: 0.9837 - val_loss: 1.2641 - val_acc: 0.9750\n","Epoch 8/10\n","800/800 [==============================] - 0s 586us/sample - loss: 0.1261 - acc: 0.9900 - val_loss: 1.2592 - val_acc: 0.9600\n","Epoch 9/10\n","800/800 [==============================] - 0s 585us/sample - loss: 0.1749 - acc: 0.9850 - val_loss: 1.6214 - val_acc: 0.9750\n","Epoch 10/10\n","800/800 [==============================] - 0s 570us/sample - loss: 0.2196 - acc: 0.9850 - val_loss: 1.4744 - val_acc: 0.9650\n","Train on 800 samples, validate on 200 samples\n","Epoch 1/10\n","800/800 [==============================] - 0s 583us/sample - loss: 2.9494 - acc: 0.8725 - val_loss: 1.8037 - val_acc: 0.9100\n","Epoch 2/10\n","800/800 [==============================] - 0s 574us/sample - loss: 0.7424 - acc: 0.9500 - val_loss: 1.5518 - val_acc: 0.9400\n","Epoch 3/10\n","800/800 [==============================] - 0s 588us/sample - loss: 0.6596 - acc: 0.9675 - val_loss: 2.4293 - val_acc: 0.9050\n","Epoch 4/10\n","800/800 [==============================] - 0s 588us/sample - loss: 0.8025 - acc: 0.9588 - val_loss: 2.4331 - val_acc: 0.9500\n","Epoch 5/10\n","800/800 [==============================] - 0s 569us/sample - loss: 0.4031 - acc: 0.9737 - val_loss: 2.3484 - val_acc: 0.9400\n","Epoch 6/10\n","800/800 [==============================] - 0s 576us/sample - loss: 0.6046 - acc: 0.9613 - val_loss: 3.3892 - val_acc: 0.9100\n","Epoch 7/10\n","800/800 [==============================] - 0s 576us/sample - loss: 0.1667 - acc: 0.9925 - val_loss: 2.2812 - val_acc: 0.9350\n","Epoch 8/10\n","800/800 [==============================] - 0s 585us/sample - loss: 0.3560 - acc: 0.9825 - val_loss: 1.8684 - val_acc: 0.9350\n","Epoch 9/10\n","800/800 [==============================] - 0s 578us/sample - loss: 0.0440 - acc: 0.9937 - val_loss: 2.2136 - val_acc: 0.9500\n","Epoch 10/10\n","800/800 [==============================] - 0s 586us/sample - loss: 0.0979 - acc: 0.9937 - val_loss: 2.6462 - val_acc: 0.9550\n","Train on 800 samples, validate on 200 samples\n","Epoch 1/10\n","800/800 [==============================] - 0s 590us/sample - loss: 2.9032 - acc: 0.8737 - val_loss: 1.7241 - val_acc: 0.9600\n","Epoch 2/10\n","800/800 [==============================] - 0s 574us/sample - loss: 1.3600 - acc: 0.9525 - val_loss: 1.6534 - val_acc: 0.9150\n","Epoch 3/10\n","800/800 [==============================] - 0s 591us/sample - loss: 1.2635 - acc: 0.9475 - val_loss: 2.2648 - val_acc: 0.9150\n","Epoch 4/10\n","800/800 [==============================] - 0s 578us/sample - loss: 0.6138 - acc: 0.9688 - val_loss: 2.8255 - val_acc: 0.9050\n","Epoch 5/10\n","800/800 [==============================] - 0s 586us/sample - loss: 0.5212 - acc: 0.9787 - val_loss: 1.4151 - val_acc: 0.9400\n","Epoch 6/10\n","800/800 [==============================] - 0s 580us/sample - loss: 0.8387 - acc: 0.9725 - val_loss: 1.4695 - val_acc: 0.9550\n","Epoch 7/10\n","800/800 [==============================] - 0s 589us/sample - loss: 0.2362 - acc: 0.9850 - val_loss: 2.5885 - val_acc: 0.9350\n","Epoch 8/10\n","800/800 [==============================] - 0s 597us/sample - loss: 0.4254 - acc: 0.9812 - val_loss: 2.2798 - val_acc: 0.9450\n","Epoch 9/10\n","800/800 [==============================] - 0s 594us/sample - loss: 0.2655 - acc: 0.9850 - val_loss: 2.4525 - val_acc: 0.9350\n","Epoch 10/10\n","800/800 [==============================] - 0s 582us/sample - loss: 0.0952 - acc: 0.9950 - val_loss: 2.5233 - val_acc: 0.9400\n","Train on 800 samples, validate on 200 samples\n","Epoch 1/10\n","800/800 [==============================] - 0s 586us/sample - loss: 2.6157 - acc: 0.8800 - val_loss: 1.6422 - val_acc: 0.9400\n","Epoch 2/10\n","800/800 [==============================] - 0s 578us/sample - loss: 1.3304 - acc: 0.9350 - val_loss: 1.8942 - val_acc: 0.9400\n","Epoch 3/10\n","800/800 [==============================] - 0s 566us/sample - loss: 1.0567 - acc: 0.9500 - val_loss: 2.1785 - val_acc: 0.9400\n","Epoch 4/10\n","800/800 [==============================] - 1s 744us/sample - loss: 0.7160 - acc: 0.9688 - val_loss: 1.0715 - val_acc: 0.9500\n","Epoch 5/10\n","800/800 [==============================] - 0s 564us/sample - loss: 0.5621 - acc: 0.9700 - val_loss: 3.1989 - val_acc: 0.9300\n","Epoch 6/10\n","800/800 [==============================] - 0s 581us/sample - loss: 0.5343 - acc: 0.9712 - val_loss: 2.1607 - val_acc: 0.9600\n","Epoch 7/10\n","800/800 [==============================] - 0s 579us/sample - loss: 0.4566 - acc: 0.9787 - val_loss: 2.7599 - val_acc: 0.9500\n","Epoch 8/10\n","800/800 [==============================] - 0s 565us/sample - loss: 0.7174 - acc: 0.9688 - val_loss: 3.6120 - val_acc: 0.9500\n","Epoch 9/10\n","800/800 [==============================] - 0s 581us/sample - loss: 0.2084 - acc: 0.9875 - val_loss: 3.3828 - val_acc: 0.9700\n","Epoch 10/10\n","800/800 [==============================] - 0s 587us/sample - loss: 0.2258 - acc: 0.9862 - val_loss: 5.2867 - val_acc: 0.9400\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_8Ml-nsPdaR-","colab_type":"code","outputId":"827b3a4f-910b-4180-d84d-c1b8c838f176","executionInfo":{"status":"ok","timestamp":1571349384216,"user_tz":300,"elapsed":1271,"user":{"displayName":"Brendan Sanderson","photoUrl":"","userId":"05405297474563600402"}},"colab":{"base_uri":"https://localhost:8080/","height":350}},"source":["plt.figure(figsize=(8, 5))\n","plt.plot(set_sizes,testing_errors)\n","plt.title(\"Learning Curve\")\n","plt.xlabel(\"Number of training set sizes\")\n","plt.ylabel(\"Testing Error\")\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAfsAAAFNCAYAAAAHGMa6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 576x360 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"JQFlt35Ds8_R","colab_type":"text"},"source":["The learning curve is clearly a parabola. As one would expect, the error increases as you use less data. Also, the higher the percent change in data, the bigger that lose is. This outcome makes sense as the difference between 900 and 1000 datasets should be much less than the different between 200 and 100 datasets.\n","\n","What is interseting is that the error doesnt really start to dramatically fall off until about 250 datasets. Overall, I would still use as many datasets as possible in order to make sure the results as the best."]},{"cell_type":"markdown","metadata":{"id":"B-3jNrOic-ig","colab_type":"text"},"source":["## Part 2. Cancer Type Classifier for 18 Common Tumor Types (40 points)"]},{"cell_type":"markdown","metadata":{"id":"CS1mwYnYQO3W","colab_type":"text"},"source":["As part 2 does a similar thing to part 1, I reused a lot of the functions, so if you cannot find a definition look above."]},{"cell_type":"markdown","metadata":{"id":"bNl97hG0PvKU","colab_type":"text"},"source":["#### Loading and Proprocessing Data\n"]},{"cell_type":"code","metadata":{"id":"_ZNl3xiXI9pe","colab_type":"code","colab":{}},"source":["#Used for quickly loading only all of the coding data.\n","type_coding = pd.read_csv(\n","    \"/content/drive/My Drive/MLiM-Datasets/HW2/type.coding.csv\")\n","type_labels = type_coding.pop(\"Type\").values\n","\n","def normalize_large_data(df):\n","  scaler = preprocessing.StandardScaler()\n","  columns = df.columns\n","  scaled_df = scaler.fit_transform(df)\n","  return pd.DataFrame(scaled_df, columns=columns)\n","\n","type_coding = normalize_large_data(type_coding)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9JfBoRZx4QEZ","colab_type":"code","outputId":"63e76de1-ea76-4e1b-efb0-d265b0aa4caf","executionInfo":{"status":"ok","timestamp":1571351170968,"user_tz":300,"elapsed":599971,"user":{"displayName":"Brendan Sanderson","photoUrl":"","userId":"05405297474563600402"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["type_coding = pd.read_csv(\n","    \"/content/drive/My Drive/MLiM-Datasets/HW2/type.coding.csv\")\n","print(\"Loaded Coding\")\n","type_all = pd.read_csv(\n","    \"/content/drive/My Drive/MLiM-Datasets/HW2/type.all.csv\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loaded Coding\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ps0c2mzcPxb8","colab_type":"code","colab":{}},"source":["#Savings the labels as a seperate data frame and removes them from the original dataframe.\n","type_labels = type_coding.pop(\"Type\").values\n","_ = type_all.pop(\"Type\").values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o_IG_v9FP2Aq","colab_type":"code","colab":{}},"source":["# Normalize the data\n","def normalize_large_data(df):\n","  scaler = preprocessing.StandardScaler()\n","  columns = df.columns\n","  scaled_df = scaler.fit_transform(df)\n","  return pd.DataFrame(scaled_df, columns=columns)\n","\n","type_coding = normalize_large_data(type_coding)\n","type_all = normalize_large_data(type_all)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bS7x23pUdE8r","colab_type":"text"},"source":["### a. Using SciKit Learn build a machine learning classifier that classifies Cancer Type from the type.coding.csv and type.all.csv files. Compare the coding vs all genes cases.\n"]},{"cell_type":"markdown","metadata":{"id":"3-0Rcm3W53gs","colab_type":"text"},"source":["Linear SVC was taking forever to run (I stopped after 30 min), so I decided to use RandomForest for this part."]},{"cell_type":"code","metadata":{"id":"AbeQ3ijJP9SM","colab_type":"code","colab":{}},"source":["#Run random forest 5 times on \n","cr = five_fold_predict_stats(RandomForestClassifier(random_state=0, n_estimators = 25),type_coding, type_labels, c=18)\n","ar = five_fold_predict_stats(RandomForestClassifier(random_state=0, n_estimators = 25),type_all, type_labels, c=18)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uxiPJ3SRRYwB","colab_type":"code","outputId":"bbdeda50-0a65-42dc-a517-6d12f8250cc3","executionInfo":{"status":"ok","timestamp":1571353127544,"user_tz":300,"elapsed":147019,"user":{"displayName":"Brendan Sanderson","photoUrl":"","userId":"05405297474563600402"}},"colab":{"base_uri":"https://localhost:8080/","height":935}},"source":["print_five_fold_results(\"Protein Coding Genes\", \"Random Forest\", cr)\n","print_five_fold_results(\"All Genes\", \"Random Forest\", ar)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","----------------------------------------------------\n","Results for running Random Forest with the Protein Coding Genes Dataset\n","The confusion matrix is:\n","[[300   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"," [  0 276   0   0   0   0  10   0   0   1   5   0   0   0   0   0   8   0]\n"," [  0   0 283  15   0   0   0   0   0   2   0   0   0   0   0   0   0   0]\n"," [  0   0  11 285   0   0   0   0   0   0   4   0   0   0   0   0   0   0]\n"," [  0   0   0   1 296   0   0   0   0   3   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0 279  16   0   0   0   1   1   0   0   0   2   1   0]\n"," [  0  10   0   0   1  37 244   0   0   1   5   0   0   0   0   1   1   0]\n"," [  0   0   0   0   0   0   0 299   0   0   0   0   0   1   0   0   0   0]\n"," [  0   0   0   0   0   1   0   0 298   0   0   0   1   0   0   0   0   0]\n"," [  0   1   0   0   0   0   0   0   0 299   0   0   0   0   0   0   0   0]\n"," [  0  13   0   0   0   1  10   0   0   5 260   1   0   0   0   1   8   1]\n"," [  0   3   0   0   1   0   0   1   0   2   0 293   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0 300   0   0   0   0   0]\n"," [  0   1   0   2   1   1   0   2   0   1   1   0   0 289   0   1   0   1]\n"," [  0   0   0   0   0   1   0   0   0   0   0   0   0   0 299   0   0   0]\n"," [  0   1   0   0   1   0   0   0   0   1   0   0   0   1   0 294   1   1]\n"," [  0   2   0   0   0   3   2   0   0   0   5   0   0  12   0   0 272   4]\n"," [  0   0   0   0   0   0   0   0   0   0   0   3   0   0   0   0   0 297]]\n","The f1 score is: 0.9561111111111111\n","The percent error is: 0.04388888888888889\n","The average runtime for training is: 7.908648109436035 seconds\n","----------------------------------------------------\n","\n","\n","----------------------------------------------------\n","Results for running Random Forest with the All Genes Dataset\n","The confusion matrix is:\n","[[300   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"," [  0 265   0   0   0   0  12   0   0   2   3   0   0   1   0   1  16   0]\n"," [  0   0 281  14   0   0   0   0   2   0   1   0   0   1   0   1   0   0]\n"," [  0   0  11 285   0   0   0   0   0   0   4   0   0   0   0   0   0   0]\n"," [  0   0   0   0 297   0   0   0   0   3   0   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0 279  15   0   0   0   1   1   0   1   0   1   2   0]\n"," [  0  12   0   0   1  40 239   0   0   0   1   0   1   0   0   2   4   0]\n"," [  0   0   0   0   0   0   0 299   0   0   0   1   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0 298   0   0   0   1   1   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0 299   0   0   0   0   0   1   0   0]\n"," [  0  14   0   0   0   0   8   0   0   6 258   1   0   1   2   0  10   0]\n"," [  0   1   0   0   0   0   0   0   0   0   0 299   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0 300   0   0   0   0   0]\n"," [  0   0   0   1   1   1   0   0   0   1   1   1   0 291   0   0   1   2]\n"," [  0   0   0   0   0   0   0   0   0   1   0   0   0   0 299   0   0   0]\n"," [  0   0   0   0   1   1   0   0   0   1   1   0   0   1   0 293   2   0]\n"," [  0   9   0   0   0   0   1   0   0   0   6   0   0  16   0   2 262   4]\n"," [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   1 298]]\n","The f1 score is: 0.9522222222222222\n","The percent error is: 0.04777777777777778\n","The average runtime for training is: 10.957520008087158 seconds\n","----------------------------------------------------\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7jABrSdx88BJ","colab_type":"text"},"source":["Similar to Part 1, the coding data performed slightly better. The error rate was slightly lower and the runtime was faster. For this reason, I will proceed with the coding data set again."]},{"cell_type":"markdown","metadata":{"id":"u3GDouFVdJJu","colab_type":"text"},"source":["### b. Using model selection methods of your choice, determine which classical ML method performs best."]},{"cell_type":"markdown","metadata":{"id":"z59zpsrz6WhA","colab_type":"text"},"source":["I removed the SVC classifiers as they took too long to run. I used the rest."]},{"cell_type":"code","metadata":{"id":"r7pNAStmq3g7","colab_type":"code","outputId":"f384f08b-6440-4d16-c3d0-98ecc772474e","executionInfo":{"status":"ok","timestamp":1571366555368,"user_tz":300,"elapsed":1244214,"user":{"displayName":"Brendan Sanderson","photoUrl":"","userId":"05405297474563600402"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["names = [\n","         \"AdaBoost\",\n","         \"Nearest Neighbors\", \n","         \"Decision Tree\", \n","         \"Random Forest\"\n","]\n","\n","classifiers = [\n","   # LinearSVC(random_state=0, tol=1e-5),\n","   # SVC(random_state=0, gamma='auto'),\n","    AdaBoostClassifier(random_state=0),\n","    KNeighborsClassifier(3),\n","    DecisionTreeClassifier(random_state=0),\n","    RandomForestClassifier(random_state=0, n_estimators = 25)\n","]\n","\n","classifier_results = []\n","for name, clf in zip(names, classifiers):\n","  results = five_fold_predict_stats(clf, type_coding, type_labels, c=18)\n","  print_five_fold_results(\"Protein Coding Genes\", name, results)\n","  classifier_results.append(results)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","----------------------------------------------------\n","Results for running AdaBoost with the Protein Coding Genes Dataset\n","The confusion matrix is:\n","[[128   0   0   0   0   0   0   0   0   0   0 103   0   0   0   0  69   0]\n"," [  0  60   0  60   0   0  58  57   0   0   0   0   0   0   0   0  65   0]\n"," [  1  59   0  56   0   0  62  55   0   0   0   0   0   0   0   0  67   0]\n"," [  0  60   0  45   0   0  67  68   0   0   0   0   0   0   0   0  60   0]\n"," [  0  60   0   2  50   0  63  55   0   0   0   0   0   0   0   0  70   0]\n"," [  0  60   0  55   0   0  70  55   0   0   0   0   0   0   0   0  60   0]\n"," [  0  60   0  60   0   2  56  63   0   0   0   0   0   0   0   0  59   0]\n"," [  0  60   0  61   0   0  65  50   0   0   0   0   1   0   0   0  63   0]\n"," [  0  58   0   0   0   0  59  56  64   0   0   0   2   0   0   0  61   0]\n"," [  0  60   0  57   0   0  56  74   0   0   0   0   0   0   0   0  53   0]\n"," [  0  60   0  58   0   0  59  62   0   0   0   0   0   0   0   0  61   0]\n"," [  1  60   0  55   0   0  64  60   0   0   0   0   0   0   0   0  60   0]\n"," [  0   0   0  73   0   0   0   0   0   0   0   0 227   0   0   0   0   0]\n"," [  0  60   0  59   1   0  63  69   1   0   0   0   0   0   0   0  47   0]\n"," [  0  60   0  67   0  53  58  60   0   0   0   0   1   0   0   0   1   0]\n"," [  0  60   0  60   0   0  59  65   0   0   0   0   0   0   0   0  56   0]\n"," [  0  60   0  65   0   0  70  63   0   0   0   0   0   0   0   0  42   0]\n"," [  0  60   0  62   0   0  65  52   0   0   0   0   0   0   0   0  61   0]]\n","The f1 score is: 0.1337037037037037\n","The percent error is: 0.8662962962962963\n","The average runtime for training is: 121.42858052253723 seconds\n","----------------------------------------------------\n","\n","\n","----------------------------------------------------\n","Results for running Nearest Neighbors with the Protein Coding Genes Dataset\n","The confusion matrix is:\n","[[300   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"," [  0 262   0   0   0   0  20   0   0   0   2   0   0   0   0   2  14   0]\n"," [  0   1 287  12   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"," [  0   0  16 280   0   0   0   0   0   0   4   0   0   0   0   0   0   0]\n"," [  0   0   0   1 295   1   0   0   0   1   1   1   0   0   0   0   0   0]\n"," [  0   1   0   0   0 261  36   0   0   1   0   1   0   0   0   0   0   0]\n"," [  0  11   0   0   0  32 254   0   1   1   1   0   0   0   0   0   0   0]\n"," [  0   0   0   0   0   0   0 299   0   0   0   0   0   1   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0 299   0   0   0   0   0   0   1   0   0]\n"," [  0   1   0   0   0   2   1   0   0 293   2   0   0   0   0   1   0   0]\n"," [  0  26   0   0   0   2  10   0   1   2 250   0   0   2   0   0   7   0]\n"," [  0   2   0   0   0   0   0   0   0   1   1 292   0   0   0   0   0   4]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0 300   0   0   0   0   0]\n"," [  0   0   0   0   0   3   2   3   0   0   0   0   0 289   0   0   2   1]\n"," [  0   0   0   0   0   1   0   0   0   0   0   0   0   0 299   0   0   0]\n"," [  0   3   0   0   0   0   1   0   0   0   0   0   0   0   0 294   1   1]\n"," [  0   9   0   0   0   0   7   0   0   0   4   0   0  12   0   0 266   2]\n"," [  0   0   0   0   0   0   0   0   0   0   0   3   0   0   0   0   0 297]]\n","The f1 score is: 0.9475925925925927\n","The percent error is: 0.052407407407407416\n","The average runtime for training is: 1.3947683334350587 seconds\n","----------------------------------------------------\n","\n","\n","----------------------------------------------------\n","Results for running Decision Tree with the Protein Coding Genes Dataset\n","The confusion matrix is:\n","[[300   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"," [  0 250   2   0   0   3  14   0   0   4   9   2   0   0   0   4  12   0]\n"," [  1   0 266  22   2   1   0   0   0   0   3   4   0   1   0   0   0   0]\n"," [  0   0  21 266   1   0   0   0   0   1   5   1   0   2   0   0   2   1]\n"," [  0   0   0   2 295   0   0   0   0   1   0   0   0   0   0   0   1   1]\n"," [  0   1   0   0   1 243  39   0   0   1   3   0   0   1   0   4   6   1]\n"," [  0  13   0   0   2  40 226   1   0   1   8   2   0   0   0   3   4   0]\n"," [  0   0   0   1   0   0   3 287   0   0   0   2   1   5   0   1   0   0]\n"," [  0   0   0   0   0   0   0   0 297   0   1   0   2   0   0   0   0   0]\n"," [  0   2   1   0   0   0   2   0   0 290   0   0   0   0   1   1   3   0]\n"," [  0  10   2   6   4   6  12   2   0   0 246   1   0   2   0   3   5   1]\n"," [  1   3   1   2   3   2   0   3   0   0   2 276   0   1   0   3   2   1]\n"," [  0   0   0   0   0   0   0   0   0   0   1   0 299   0   0   0   0   0]\n"," [  0   0   1   2   0   0   0   6   0   0   6   1   0 261   1   4  18   0]\n"," [  0   0   0   0   0   1   0   0   0   0   0   0   1   1 297   0   0   0]\n"," [  0   4   1   1   1   0   2   0   0   0   3   1   0   0   0 284   3   0]\n"," [  0  16   1   1   3   5   3   3   0   0  15   3   0  14   0   1 234   1]\n"," [  0   0   0   0   1   3   0   0   0   0   1   4   0   0   0   0   1 290]]\n","The f1 score is: 0.9087037037037037\n","The percent error is: 0.09129629629629629\n","The average runtime for training is: 24.89840450286865 seconds\n","----------------------------------------------------\n","\n","\n","----------------------------------------------------\n","Results for running Random Forest with the Protein Coding Genes Dataset\n","The confusion matrix is:\n","[[300   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"," [  0 276   0   0   0   0   7   0   0   1   4   0   0   0   0   4   8   0]\n"," [  0   0 283  15   0   0   0   0   0   0   1   1   0   0   0   0   0   0]\n"," [  0   0  16 280   0   0   0   0   0   0   4   0   0   0   0   0   0   0]\n"," [  0   0   0   3 294   0   0   0   0   3   0   0   0   0   0   0   0   0]\n"," [  0   1   0   1   0 284   9   0   0   0   3   0   0   0   0   1   1   0]\n"," [  0  15   0   0   1  34 243   0   0   0   3   0   0   0   1   1   2   0]\n"," [  0   0   0   0   0   0   0 298   0   0   0   0   0   2   0   0   0   0]\n"," [  0   0   0   0   0   0   0   0 298   0   0   0   1   1   0   0   0   0]\n"," [  0   1   0   0   0   0   0   0   1 298   0   0   0   0   0   0   0   0]\n"," [  0  12   0   0   0   0   2   0   1   5 267   0   0   2   1   1   9   0]\n"," [  0   4   0   0   0   0   0   1   0   0   0 294   0   0   0   1   0   0]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0 300   0   0   0   0   0]\n"," [  0   0   1   2   0   0   1   3   0   2   2   1   0 286   0   0   1   1]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   1 299   0   0   0]\n"," [  0   0   0   0   1   1   0   0   0   1   1   0   0   0   0 293   3   0]\n"," [  0   4   0   0   0   2   0   0   0   1   7   0   0  12   0   1 269   4]\n"," [  0   0   0   0   0   0   0   0   0   0   0   5   0   1   0   0   0 294]]\n","The f1 score is: 0.9548148148148148\n","The percent error is: 0.04518518518518518\n","The average runtime for training is: 4.648022413253784 seconds\n","----------------------------------------------------\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0dgU55kennZG","colab_type":"text"},"source":["The random forest and nearest neighbors had the lowest error rates (RandomForest had a slightly better error rate). KNeighbors suprisingly had the lowest runtime. I will use RandomForest going forward as the confusion matrix makes it look more reliable. KNeighbors actually took a while to predict the data, which was interesting as I normally expect the training to be the hard part. I was surprised by how poorly AdaBoost performed given how well it did in part 1."]},{"cell_type":"markdown","metadata":{"id":"0D2zp4FcdLSc","colab_type":"text"},"source":["### c. Using feature selection methods of your choice, determine a < 100 gene signature that can be used to classify tumor type."]},{"cell_type":"markdown","metadata":{"id":"fWQzMdQa_Wx0","colab_type":"text"},"source":["I do the same thing as before where I first reduce the dimensions to 10,000 through selectKBest and then iteratively use ExtraTreesClassifier to determine the worst features and reduce dimension. I use RandomForest to classify the model here as SVCLinear took too long to run."]},{"cell_type":"code","metadata":{"id":"yfPA3XdRAbgN","colab_type":"code","colab":{}},"source":["def get_K_best(df, labels, k):\n","  mini = np.min(np.min(df))\n","  selectKBest = SelectKBest(chi2, k=k)\n","  select_df = selectKBest.fit_transform(df-mini, labels)+mini\n","  column_inds = selectKBest.get_support(indices = True)\n","  new_columns = [df.columns[i] for i in column_inds]\n","  return pd.DataFrame(select_df, columns = new_columns)\n","\n","type_10k_coding = get_K_best(type_coding, type_labels, 10000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YsO9WIi4QoE9","colab_type":"code","colab":{}},"source":["type_select_coding = type_10k_coding.copy()\n","type_select_labels = type_labels\n","\n","model = ExtraTreesClassifier(n_estimators = 50)\n","errors = []\n","times = []\n","\n","#Add once for original model\n","r = five_fold_predict_stats(RandomForestClassifier(n_estimators = 25),type_coding, type_labels, c = 18)\n","features = [len(type_coding.columns)]\n","errors.append(r[2])\n","times.append(r[3])\n","\n","#Add once for 10k model\n","r = five_fold_predict_stats(RandomForestClassifier(n_estimators = 25),type_select_coding, type_labels, c = 18)\n","errors.append(r[2])\n","times.append(r[3])\n","features.append(10000)\n","mi = 1000\n","for i in range(20):\n","  model.fit(type_select_coding, type_select_labels)\n","  feature_rankings = model.feature_importances_\n","  sorted_test = sorted(list(enumerate(feature_rankings)),key= lambda x: x[1])\n","  type_select_coding = type_select_coding.drop(columns=[type_select_coding.columns[x[0]] for x in sorted_test[:int(len(sorted_test)*0.2)]])\n","  if (len(type_select_coding.columns) < 3000):\n","    mi = 2500\n","  if (len(type_select_coding.columns) < 1000):\n","    mi = 5000\n","  r = five_fold_predict_stats(RandomForestClassifier(random_state=0, n_estimators = 25),type_select_coding, type_labels, c = 18)\n","  errors.append(r[2])\n","  times.append(r[3])\n","  features.append(len(type_select_coding.columns))\n","\n","   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bc09prw-RQ_i","colab_type":"code","outputId":"54fcae50-877f-4bf8-9b89-e336cc458666","executionInfo":{"status":"ok","timestamp":1571360200129,"user_tz":300,"elapsed":1304,"user":{"displayName":"Brendan Sanderson","photoUrl":"","userId":"05405297474563600402"}},"colab":{"base_uri":"https://localhost:8080/","height":350}},"source":["plt.figure(figsize=(16, 5))\n","plt.subplot(1,2,1)\n","plt.plot(features,errors)\n","plt.title(\"Testing Error with Different Numbers of Features\")\n","plt.xlabel(\"Number of Features\")\n","plt.ylabel(\"Testing Error\")\n","plt.subplot(1,2,2)\n","plt.plot(features,times)\n","plt.title(\"Average Training Runtime Different Numbers of Features\")\n","plt.xlabel(\"Number of Features\")\n","plt.ylabel(\"Average Runtime\")\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAA8IAAAFNCAYAAADLktaJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n","text/plain":["<Figure size 1152x360 with 2 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"5gkSIJbsgxfd","colab_type":"text"},"source":["Given this graph, reducing features seems to generally slightly decrease the testing error. This seems extremely wierd as in part 1, it increased the testing error. Perhaps the features that are being removed are the features that are affiliated with having a tumor versus not and thus provide misinformation in actually classifying the tumor. The runtime graph is as expected where the runtime decreases as features are removed."]},{"cell_type":"markdown","metadata":{"id":"JZFj5gWjdNBl","colab_type":"text"},"source":["### d. Using Keras, build a deep learning classifier that performs the same classification task, and determine the learning curve (relationship of number of training samples to prediction accuracy) for your network.\n"]},{"cell_type":"markdown","metadata":{"id":"H18I5Z9GaLdA","colab_type":"text"},"source":["\\"]},{"cell_type":"code","metadata":{"id":"I3koiUA-6mr7","colab_type":"code","colab":{}},"source":["#A cnn classification model given an input layer and # of classifications\n","\n","def classification_model_cnn(i,c):\n","  model = models.Sequential()\n","  model.add(layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='valid', input_shape=(i,1)))\n","  model.add(layers.Activation('relu'))\n","  model.add(layers.MaxPooling1D(pool_size=1))\n","  model.add(layers.Conv1D(filters=16, kernel_size=1, strides=1, padding='valid'))\n","  model.add(layers.Activation('relu'))\n","  model.add(layers.MaxPooling1D(pool_size=10))\n","  model.add(layers.Flatten())\n","  model.add(layers.Dense(200))\n","  model.add(layers.Activation('relu'))\n","  model.add(layers.Dropout(0.1))\n","  model.add(layers.Dense(20))\n","  model.add(layers.Activation('relu'))\n","  model.add(layers.Dropout(0.1))\n","  model.add(layers.Dense(c))\n","  model.add(layers.Activation('softmax'))\n","  model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.SGD(), metrics=['accuracy'])\n","  return model\n","\n","#A fully connected classification model given an input layer and # of classifications\n","\n","def classification_model_fc(i,c):\n","  model = models.Sequential()\n","  model.add(layers.InputLayer(i,))\n","  model.add(layers.Dense(512, activation='relu'))\n","  model.add(layers.Dense(256, activation='relu'))\n","  model.add(layers.Dense(c, activation='softmax'))\n","  model.compile(loss=\"categorical_crossentropy\", optimizer='rmsprop', metrics=['accuracy'])\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lPUtBMvo6qZB","colab_type":"code","colab":{}},"source":["def five_fold_nn_samples_categorical(model, X, y, eps, samples = 1000):\n","  train_errors = []\n","  valid_errors = []\n","  test_errors = []\n","  ns = 5\n","  if samples < 10:\n","    ns = 2\n","  kf = KFold(n_splits=ns)\n","  y_cat = utils.to_categorical(y)\n","  X_test = X[-1000:]\n","  Y_test = y[-1000:]\n","  model.save_weights('empty')\n","  for train_index, test_index in kf.split(X[:samples]):\n","      X_train, X_val = X.values[train_index], X.values[test_index]\n","      y_train, y_val = y_cat[train_index], y_cat[test_index]\n","      hist = model.fit(x=X_train, y=y_train, epochs=eps, batch_size=25, shuffle=True, validation_data=(np.array(X_val), y_val))\n","      test_errors.append(np.mean(model.predict(X_test)!= utils.to_categorical(Y_test)))\n","      model.load_weights('empty')\n","  return np.mean(test_errors)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qzhnLcLi6tXd","colab_type":"code","outputId":"8dc60257-0b69-47be-df5c-2b4667c684ba","executionInfo":{"status":"ok","timestamp":1571357622508,"user_tz":300,"elapsed":94403,"user":{"displayName":"Brendan Sanderson","photoUrl":"","userId":"05405297474563600402"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["set_sizes = [3,7,15,30,60,125,250,500,1000, 2500, 4000]\n","testing_errors = []\n","for s in set_sizes:\n","  testing_errors.append(five_fold_nn_samples_categorical(classification_model_fc(len(type_coding.columns),19), type_coding, type_labels, 10, samples=s))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 1 samples, validate on 2 samples\n","Epoch 1/10\n","1/1 [==============================] - 1s 574ms/sample - loss: 2.2392 - acc: 0.0000e+00 - val_loss: 4.4832 - val_acc: 0.5000\n","Epoch 2/10\n","1/1 [==============================] - 0s 17ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 4.4832 - val_acc: 0.5000\n","Epoch 3/10\n","1/1 [==============================] - 0s 16ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 4.4832 - val_acc: 0.5000\n","Epoch 4/10\n","1/1 [==============================] - 0s 17ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 4.4832 - val_acc: 0.5000\n","Epoch 5/10\n","1/1 [==============================] - 0s 18ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 4.4832 - val_acc: 0.5000\n","Epoch 6/10\n","1/1 [==============================] - 0s 16ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 4.4832 - val_acc: 0.5000\n","Epoch 7/10\n","1/1 [==============================] - 0s 17ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 4.4832 - val_acc: 0.5000\n","Epoch 8/10\n","1/1 [==============================] - 0s 17ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 4.4832 - val_acc: 0.5000\n","Epoch 9/10\n","1/1 [==============================] - 0s 16ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 4.4832 - val_acc: 0.5000\n","Epoch 10/10\n","1/1 [==============================] - 0s 17ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 4.4832 - val_acc: 0.5000\n","Train on 3 samples, validate on 4 samples\n","Epoch 1/10\n","3/3 [==============================] - 1s 191ms/sample - loss: 2.9084 - acc: 0.3333 - val_loss: 7.2790 - val_acc: 0.5000\n","Epoch 2/10\n","3/3 [==============================] - 0s 6ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 7.2790 - val_acc: 0.5000\n","Epoch 3/10\n","3/3 [==============================] - 0s 6ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 7.2790 - val_acc: 0.5000\n","Epoch 4/10\n","3/3 [==============================] - 0s 6ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 7.2790 - val_acc: 0.5000\n","Epoch 5/10\n","3/3 [==============================] - 0s 6ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 7.2790 - val_acc: 0.5000\n","Epoch 6/10\n","3/3 [==============================] - 0s 6ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 7.2790 - val_acc: 0.5000\n","Epoch 7/10\n","3/3 [==============================] - 0s 6ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 7.2790 - val_acc: 0.5000\n","Epoch 8/10\n","3/3 [==============================] - 0s 6ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 7.2790 - val_acc: 0.5000\n","Epoch 9/10\n","3/3 [==============================] - 0s 6ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 7.2790 - val_acc: 0.5000\n","Epoch 10/10\n","3/3 [==============================] - 0s 7ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 7.2790 - val_acc: 0.5000\n","Train on 12 samples, validate on 3 samples\n","Epoch 1/10\n","12/12 [==============================] - 1s 53ms/sample - loss: 3.1852 - acc: 0.0000e+00 - val_loss: 8.9507 - val_acc: 0.6667\n","Epoch 2/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 8.9507 - val_acc: 0.6667\n","Epoch 3/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 8.9507 - val_acc: 0.6667\n","Epoch 4/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 8.9507 - val_acc: 0.6667\n","Epoch 5/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 8.9507 - val_acc: 0.6667\n","Epoch 6/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 8.9507 - val_acc: 0.6667\n","Epoch 7/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 8.9507 - val_acc: 0.6667\n","Epoch 8/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 8.9507 - val_acc: 0.6667\n","Epoch 9/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 8.9507 - val_acc: 0.6667\n","Epoch 10/10\n","12/12 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 8.9507 - val_acc: 0.6667\n","Train on 24 samples, validate on 6 samples\n","Epoch 1/10\n","24/24 [==============================] - 1s 26ms/sample - loss: 3.3436 - acc: 0.0833 - val_loss: 9.6453 - val_acc: 0.3333\n","Epoch 2/10\n","24/24 [==============================] - 0s 1ms/sample - loss: 2.4423e-04 - acc: 1.0000 - val_loss: 9.1588 - val_acc: 0.3333\n","Epoch 3/10\n","24/24 [==============================] - 0s 796us/sample - loss: 9.8347e-07 - acc: 1.0000 - val_loss: 9.1511 - val_acc: 0.3333\n","Epoch 4/10\n","24/24 [==============================] - 0s 783us/sample - loss: 8.8910e-07 - acc: 1.0000 - val_loss: 9.1454 - val_acc: 0.3333\n","Epoch 5/10\n","24/24 [==============================] - 0s 788us/sample - loss: 8.1459e-07 - acc: 1.0000 - val_loss: 9.1406 - val_acc: 0.3333\n","Epoch 6/10\n","24/24 [==============================] - 0s 782us/sample - loss: 7.4506e-07 - acc: 1.0000 - val_loss: 9.1364 - val_acc: 0.3333\n","Epoch 7/10\n","24/24 [==============================] - 0s 1ms/sample - loss: 6.8545e-07 - acc: 1.0000 - val_loss: 9.1327 - val_acc: 0.3333\n","Epoch 8/10\n","24/24 [==============================] - 0s 908us/sample - loss: 6.4075e-07 - acc: 1.0000 - val_loss: 9.1288 - val_acc: 0.3333\n","Epoch 9/10\n","24/24 [==============================] - 0s 810us/sample - loss: 5.9604e-07 - acc: 1.0000 - val_loss: 9.1253 - val_acc: 0.3333\n","Epoch 10/10\n","24/24 [==============================] - 0s 835us/sample - loss: 5.6128e-07 - acc: 1.0000 - val_loss: 9.1225 - val_acc: 0.3333\n","Train on 48 samples, validate on 12 samples\n","Epoch 1/10\n","48/48 [==============================] - 1s 14ms/sample - loss: 10.1794 - acc: 0.2292 - val_loss: 8.1280 - val_acc: 0.2500\n","Epoch 2/10\n","48/48 [==============================] - 0s 827us/sample - loss: 3.3624 - acc: 0.8333 - val_loss: 7.0859 - val_acc: 0.2500\n","Epoch 3/10\n","48/48 [==============================] - 0s 814us/sample - loss: 0.5581 - acc: 0.9375 - val_loss: 7.7124 - val_acc: 0.4167\n","Epoch 4/10\n","48/48 [==============================] - 0s 822us/sample - loss: 6.0439e-05 - acc: 1.0000 - val_loss: 7.7096 - val_acc: 0.4167\n","Epoch 5/10\n","48/48 [==============================] - 0s 817us/sample - loss: 3.1195e-05 - acc: 1.0000 - val_loss: 7.7085 - val_acc: 0.4167\n","Epoch 6/10\n","48/48 [==============================] - 0s 818us/sample - loss: 2.0662e-05 - acc: 1.0000 - val_loss: 7.7078 - val_acc: 0.4167\n","Epoch 7/10\n","48/48 [==============================] - 0s 845us/sample - loss: 1.5951e-05 - acc: 1.0000 - val_loss: 7.7075 - val_acc: 0.4167\n","Epoch 8/10\n","48/48 [==============================] - 0s 857us/sample - loss: 1.2689e-05 - acc: 1.0000 - val_loss: 7.7071 - val_acc: 0.4167\n","Epoch 9/10\n","48/48 [==============================] - 0s 810us/sample - loss: 1.0283e-05 - acc: 1.0000 - val_loss: 7.7068 - val_acc: 0.4167\n","Epoch 10/10\n","48/48 [==============================] - 0s 828us/sample - loss: 8.6219e-06 - acc: 1.0000 - val_loss: 7.7063 - val_acc: 0.4167\n","Train on 100 samples, validate on 25 samples\n","Epoch 1/10\n","100/100 [==============================] - 1s 7ms/sample - loss: 7.4308 - acc: 0.3500 - val_loss: 6.1663 - val_acc: 0.6400\n","Epoch 2/10\n","100/100 [==============================] - 0s 699us/sample - loss: 2.1864 - acc: 0.8400 - val_loss: 4.3341 - val_acc: 0.7600\n","Epoch 3/10\n","100/100 [==============================] - 0s 681us/sample - loss: 0.1879 - acc: 0.9800 - val_loss: 3.4330 - val_acc: 0.7200\n","Epoch 4/10\n","100/100 [==============================] - 0s 683us/sample - loss: 9.5431e-05 - acc: 1.0000 - val_loss: 3.4240 - val_acc: 0.7200\n","Epoch 5/10\n","100/100 [==============================] - 0s 670us/sample - loss: 4.0999e-05 - acc: 1.0000 - val_loss: 3.4205 - val_acc: 0.7200\n","Epoch 6/10\n","100/100 [==============================] - 0s 733us/sample - loss: 2.9021e-05 - acc: 1.0000 - val_loss: 3.4175 - val_acc: 0.7200\n","Epoch 7/10\n","100/100 [==============================] - 0s 677us/sample - loss: 2.1694e-05 - acc: 1.0000 - val_loss: 3.4151 - val_acc: 0.7200\n","Epoch 8/10\n","100/100 [==============================] - 0s 681us/sample - loss: 1.7417e-05 - acc: 1.0000 - val_loss: 3.4124 - val_acc: 0.7200\n","Epoch 9/10\n","100/100 [==============================] - 0s 665us/sample - loss: 1.3871e-05 - acc: 1.0000 - val_loss: 3.4103 - val_acc: 0.7200\n","Epoch 10/10\n","100/100 [==============================] - 0s 665us/sample - loss: 1.1603e-05 - acc: 1.0000 - val_loss: 3.4080 - val_acc: 0.7200\n","Train on 200 samples, validate on 50 samples\n","Epoch 1/10\n","200/200 [==============================] - 1s 4ms/sample - loss: 5.6293 - acc: 0.5300 - val_loss: 2.0651 - val_acc: 0.8000\n","Epoch 2/10\n","200/200 [==============================] - 0s 705us/sample - loss: 0.5896 - acc: 0.9300 - val_loss: 1.3912 - val_acc: 0.8800\n","Epoch 3/10\n","200/200 [==============================] - 0s 708us/sample - loss: 0.0992 - acc: 0.9650 - val_loss: 1.4587 - val_acc: 0.8800\n","Epoch 4/10\n","200/200 [==============================] - 0s 682us/sample - loss: 6.7947e-04 - acc: 1.0000 - val_loss: 1.3745 - val_acc: 0.9000\n","Epoch 5/10\n","200/200 [==============================] - 0s 686us/sample - loss: 2.4237e-05 - acc: 1.0000 - val_loss: 1.3714 - val_acc: 0.9000\n","Epoch 6/10\n","200/200 [==============================] - 0s 668us/sample - loss: 2.0223e-05 - acc: 1.0000 - val_loss: 1.3671 - val_acc: 0.9000\n","Epoch 7/10\n","200/200 [==============================] - 0s 656us/sample - loss: 1.6193e-05 - acc: 1.0000 - val_loss: 1.3626 - val_acc: 0.9000\n","Epoch 8/10\n","200/200 [==============================] - 0s 664us/sample - loss: 1.2873e-05 - acc: 1.0000 - val_loss: 1.3569 - val_acc: 0.9000\n","Epoch 9/10\n","200/200 [==============================] - 0s 655us/sample - loss: 9.6632e-06 - acc: 1.0000 - val_loss: 1.3515 - val_acc: 0.9000\n","Epoch 10/10\n","200/200 [==============================] - 0s 648us/sample - loss: 7.4607e-06 - acc: 1.0000 - val_loss: 1.3451 - val_acc: 0.9000\n","Train on 400 samples, validate on 100 samples\n","Epoch 1/10\n","400/400 [==============================] - 1s 2ms/sample - loss: 5.9583 - acc: 0.6450 - val_loss: 2.3424 - val_acc: 0.8500\n","Epoch 2/10\n","400/400 [==============================] - 0s 710us/sample - loss: 0.7384 - acc: 0.9350 - val_loss: 1.7395 - val_acc: 0.8600\n","Epoch 3/10\n","400/400 [==============================] - 0s 629us/sample - loss: 0.4871 - acc: 0.9575 - val_loss: 2.7179 - val_acc: 0.8600\n","Epoch 4/10\n","400/400 [==============================] - 0s 650us/sample - loss: 1.1024 - acc: 0.9425 - val_loss: 3.2328 - val_acc: 0.8800\n","Epoch 5/10\n","400/400 [==============================] - 0s 619us/sample - loss: 0.4357 - acc: 0.9500 - val_loss: 3.0252 - val_acc: 0.8500\n","Epoch 6/10\n","400/400 [==============================] - 0s 632us/sample - loss: 1.2652 - acc: 0.9500 - val_loss: 3.0924 - val_acc: 0.8200\n","Epoch 7/10\n","400/400 [==============================] - 0s 619us/sample - loss: 0.1884 - acc: 0.9850 - val_loss: 2.7958 - val_acc: 0.8500\n","Epoch 8/10\n","400/400 [==============================] - 0s 612us/sample - loss: 0.1445 - acc: 0.9850 - val_loss: 2.9947 - val_acc: 0.8900\n","Epoch 9/10\n","400/400 [==============================] - 0s 611us/sample - loss: 0.2344 - acc: 0.9850 - val_loss: 2.9648 - val_acc: 0.9000\n","Epoch 10/10\n","400/400 [==============================] - 0s 645us/sample - loss: 9.0206e-07 - acc: 1.0000 - val_loss: 2.9646 - val_acc: 0.9000\n","Train on 800 samples, validate on 200 samples\n","Epoch 1/10\n","800/800 [==============================] - 1s 2ms/sample - loss: 3.3439 - acc: 0.7812 - val_loss: 1.6179 - val_acc: 0.8250\n","Epoch 2/10\n","800/800 [==============================] - 1s 636us/sample - loss: 0.6871 - acc: 0.9388 - val_loss: 2.0795 - val_acc: 0.8700\n","Epoch 3/10\n","800/800 [==============================] - 1s 636us/sample - loss: 0.3450 - acc: 0.9725 - val_loss: 1.6821 - val_acc: 0.9050\n","Epoch 4/10\n","800/800 [==============================] - 0s 622us/sample - loss: 0.8004 - acc: 0.9650 - val_loss: 5.2904 - val_acc: 0.8300\n","Epoch 5/10\n","800/800 [==============================] - 0s 603us/sample - loss: 1.2850 - acc: 0.9475 - val_loss: 2.8240 - val_acc: 0.8650\n","Epoch 6/10\n","800/800 [==============================] - 1s 625us/sample - loss: 0.3901 - acc: 0.9712 - val_loss: 3.0652 - val_acc: 0.8450\n","Epoch 7/10\n","800/800 [==============================] - 0s 610us/sample - loss: 0.2792 - acc: 0.9837 - val_loss: 2.5533 - val_acc: 0.8950\n","Epoch 8/10\n","800/800 [==============================] - 1s 625us/sample - loss: 0.2347 - acc: 0.9862 - val_loss: 2.7562 - val_acc: 0.9050\n","Epoch 9/10\n","800/800 [==============================] - 0s 597us/sample - loss: 0.4581 - acc: 0.9787 - val_loss: 4.0911 - val_acc: 0.8650\n","Epoch 10/10\n","800/800 [==============================] - 0s 623us/sample - loss: 0.3399 - acc: 0.9775 - val_loss: 5.6600 - val_acc: 0.8650\n","Train on 2000 samples, validate on 500 samples\n","Epoch 1/10\n","2000/2000 [==============================] - 2s 1ms/sample - loss: 2.4113 - acc: 0.8335 - val_loss: 1.3183 - val_acc: 0.9080\n","Epoch 2/10\n","2000/2000 [==============================] - 1s 589us/sample - loss: 0.9858 - acc: 0.9350 - val_loss: 2.4320 - val_acc: 0.9140\n","Epoch 3/10\n","2000/2000 [==============================] - 1s 580us/sample - loss: 0.8669 - acc: 0.9510 - val_loss: 2.2055 - val_acc: 0.9200\n","Epoch 4/10\n","2000/2000 [==============================] - 1s 588us/sample - loss: 0.5629 - acc: 0.9660 - val_loss: 2.0705 - val_acc: 0.9060\n","Epoch 5/10\n","2000/2000 [==============================] - 1s 584us/sample - loss: 0.4891 - acc: 0.9745 - val_loss: 5.1909 - val_acc: 0.8840\n","Epoch 6/10\n","2000/2000 [==============================] - 1s 589us/sample - loss: 0.6415 - acc: 0.9750 - val_loss: 2.2173 - val_acc: 0.9340\n","Epoch 7/10\n","2000/2000 [==============================] - 1s 590us/sample - loss: 0.3499 - acc: 0.9865 - val_loss: 2.2109 - val_acc: 0.9240\n","Epoch 8/10\n","2000/2000 [==============================] - 1s 590us/sample - loss: 0.3650 - acc: 0.9840 - val_loss: 2.1025 - val_acc: 0.9360\n","Epoch 9/10\n","2000/2000 [==============================] - 1s 584us/sample - loss: 0.1493 - acc: 0.9910 - val_loss: 1.4527 - val_acc: 0.9500\n","Epoch 10/10\n","2000/2000 [==============================] - 1s 601us/sample - loss: 0.2210 - acc: 0.9880 - val_loss: 1.7879 - val_acc: 0.9460\n","Train on 3200 samples, validate on 800 samples\n","Epoch 1/10\n","3200/3200 [==============================] - 3s 850us/sample - loss: 2.0760 - acc: 0.8597 - val_loss: 2.2564 - val_acc: 0.8950\n","Epoch 2/10\n","3200/3200 [==============================] - 2s 587us/sample - loss: 0.9778 - acc: 0.9391 - val_loss: 1.7597 - val_acc: 0.9337\n","Epoch 3/10\n","3200/3200 [==============================] - 2s 589us/sample - loss: 0.5405 - acc: 0.9613 - val_loss: 1.7521 - val_acc: 0.9350\n","Epoch 4/10\n","3200/3200 [==============================] - 2s 581us/sample - loss: 0.5019 - acc: 0.9697 - val_loss: 1.6224 - val_acc: 0.9325\n","Epoch 5/10\n","3200/3200 [==============================] - 2s 586us/sample - loss: 0.4116 - acc: 0.9769 - val_loss: 1.6977 - val_acc: 0.9438\n","Epoch 6/10\n","3200/3200 [==============================] - 2s 589us/sample - loss: 0.3566 - acc: 0.9800 - val_loss: 1.9137 - val_acc: 0.9400\n","Epoch 7/10\n","3200/3200 [==============================] - 2s 589us/sample - loss: 0.4378 - acc: 0.9853 - val_loss: 2.5534 - val_acc: 0.9438\n","Epoch 8/10\n","3200/3200 [==============================] - 2s 585us/sample - loss: 0.3293 - acc: 0.9891 - val_loss: 1.9920 - val_acc: 0.9425\n","Epoch 9/10\n","3200/3200 [==============================] - 2s 592us/sample - loss: 0.1382 - acc: 0.9922 - val_loss: 1.5890 - val_acc: 0.9563\n","Epoch 10/10\n","3200/3200 [==============================] - 2s 594us/sample - loss: 0.0795 - acc: 0.9953 - val_loss: 2.5000 - val_acc: 0.9425\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DkCnG5-q6wqU","colab_type":"code","outputId":"86ad9554-740a-408b-fcb0-1cae5ab3cc34","executionInfo":{"status":"ok","timestamp":1571357751217,"user_tz":300,"elapsed":995,"user":{"displayName":"Brendan Sanderson","photoUrl":"","userId":"05405297474563600402"}},"colab":{"base_uri":"https://localhost:8080/","height":350}},"source":["plt.figure(figsize=(8, 5))\n","plt.plot(set_sizes,testing_errors)\n","plt.title(\"Learning Curve\")\n","plt.xlabel(\"Training set sizes\")\n","plt.ylabel(\"Testing Error\")\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAfUAAAFNCAYAAAAZ0fYJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmYXGWZ/vHv03t6TTq9ZU8IWTto\nwLDKDiYhKMHREVBkRhkZUWf0BzMjI4iAMAPOuIuD4I6jIC4QFUgQUJZhC3v2hISQrTt70tk6vTy/\nP87p7upOL5Wkq08t9+e66uqqU6eqntOnk7ve97znvObuiIiISOrLiroAERER6R8KdRERkTShUBcR\nEUkTCnUREZE0oVAXERFJEwp1ERGRNKFQF8lgZvaImf1d1HWISP9QqItEwMzeNrPzo67D3S9w958l\n4r3NrNTMvmVm75jZHjN7K3xckYjPExGFukjaMrOcCD87D3gcqAVmA6XAqcA24KQjeL/ItkUklSjU\nRZKMmb3fzF4zs51m9n9m9q6Y564LW7wNZrbEzD4Y89zfm9mzZvZNM9sG3BQue8bM/tvMdpjZGjO7\nIOY1fzGzf4h5fW/rjjOzp8LP/rOZ3Wlmv+hhM64ARgMfdPcl7t7q7pvd/avu/nD4fm5mx8a8/0/N\n7Nbw/tlmtt7MvmhmdcBPzGypmb0/Zv0cM9tiZieEj08Jf187zex1Mzv7aPaDSCpSqIskETM7Hvgx\n8I/AUOAHwDwzyw9XeQs4AygDbgZ+YWbDYt7iZGA1UA3cFrNsOVABfA34kZlZDyX0tu4vgRfDum4C\nPt7LppwPPOrue/re6h7VAOXAGOAq4FfAZTHPzwK2uvsrZjYC+BNwa/iafwF+a2aVR/H5IilHoS6S\nXK4CfuDuL7h7S3i8uxE4BcDdH3D3jWHL935gJZ27sze6+3fdvdnd94fL1rr7Pe7eAvwMGEYQ+t3p\ndl0zGw2cCNzo7gfd/RlgXi/bMRTYdES/gQ6twFfcvTHcll8CF5lZYfj8RwmCHuBy4GF3fzj83TwG\nLATmHGUNIilFoS6SXMYA14ZdyDvNbCcwChgOYGZXxHTN7wSmEbSq26zr5j3r2u64+77wbnEPn9/T\nusOB7THLevqsNtsIvhAcjS3ufiCmnlXAUuADYbBfRBD0EPze/rbL7+30fqhBJKVo8IlIclkH3Obu\nt3V9wszGAPcA5wHPuXuLmb0GxHalJ2raxU1AuZkVxgT7qF7W/zNwq5kVufveHtbZBxTGPK4B1sc8\n7m5b2rrgs4AlYdBD8Hu7190/1cd2iKQ1tdRFopNrZgUxtxyC0P60mZ1sgSIzu9DMSoAigqDbAmBm\nnyBoqSecu68l6M6+yczyzOxU4AO9vORegqD9rZlNNrMsMxtqZl8ys7Yu8deAj5pZtpnNBs6Ko5T7\ngJnA1XS00gF+QdCCnxW+X0E42G7kYW6qSEpTqItE52Fgf8ztJndfCHwK+B6wA1gF/D2Auy8Bvg48\nB9QDxwHPDmC9H6PjtLRbgfsJjvcfwt0bCQbLLQMeA3YTDLKrAF4IV/s8wReDneF7P9hXAe6+iWD7\nTws/v235OmAu8CWCLz3rgH9F/8dJhjH3RPXWiUg6M7P7gWXu/pWoaxGRgL7FikhczOxEMxsfdqXP\nJmgZ99m6FpGBo4FyIhKvGuB3BKerrQeudvdXoy1JRGKp+11ERCRNqPtdREQkTSQs1M3sx2a22cwW\n9fC8mdl3zGyVmb3Rdv1mEREROTKJPKb+U4LTcn7ew/MXABPC28nA/4Q/e1VRUeFjx47tnwpFRESS\n3Msvv7zV3eOaxyBhoe7uT5nZ2F5WmQv83IOD+s+b2WAzGxaeh9qjsWPHsnDhwn6sVEREJHmZ2dp4\n143ymPoIOl87en24TERERI5ASgyUM7OrzGyhmS3csmVL1OWIiIgkpShDfQOdJ4QYGS47hLvf7e4z\n3H1GZaWmRxYREelOlKE+D7giHAV/CrCrr+PpIiIi0rOEDZQzs18BZwMVZrYe+AqQC+DudxFMZjGH\nYMKKfcAnElWLiIhIJkjk6PfL+njegc8m6vNFREQyTUoMlBMREZG+KdRFRETShEJdREQkTWR8qL+2\nbic79x2MugwREZGjltGh7u5cfOezfPiu56IuRURE5KhldKgfbGkFYNXmPRFXIiIicvQyOtT3H2yJ\nugQREZF+k9mh3tQR6sFp8yIiIqkro0N9X0xLffteDZYTEZHUltGhHtv9vnb7vggrEREROXqZHeox\n3e/rd+yPsBIREZGjl9mhHtNS37anMcJKREREjl5Gh7qOqYuISDrJ6FDf39Tcfn+bQl1ERFJcwqZe\nTQVNLU52llGYl832PQp1ERFJbRndUv/IjFG89R9zmDqslG17dUxdRERSW0aHepuK4nx1v4uISMpT\nqAPlRXkaKCciIilPoU4Q6jv3NdEcTvAiIiKSihTqwNDiPAC2a151ERFJYQp1gpY66Fx1ERFJbQp1\nYGhRPoBOaxMRkZSmUKej+10j4EVEJJUp1Onoftf130VEJJUp1IEhhXmY9X1Mfc3WvXz2f1/hiWX1\nA1SZiIhI/DL6MrFtsrOMIYV5PXa/t7Q6P3l2Df81fzmNza08vXILj37hTIYPHjTAlYqIiPRMLfVQ\nTxegWb1lDx/5wXPc+qelnDGhggc+fSrNrc6/PPA6ra0eQaUiIiLdU6iHyovy2BYz+r2l1fnh06u5\n4NtPs7K+gW985N3cc8UMThxbzo3vn8r/vbWNHz+7JsKKRUREOlP3e6iyOJ/HltTz0XueZ9qIMl5e\nu4OX1+7g/ClV/McHj6OqtKB93UtOHMWfl27ma/OXc/qECibXlEZYuYiISEAt9dBnzhnPh2eMZE9j\nMz999m1Wbd7DNy8JWuexgQ5gZtzxoeMoLcjlC/e9RmNzS0RVi4iIdDD31DouPGPGDF+4cGFCP6Op\npRV3yMvp/TvPE8vq+eRPF3LVmcfwpTlTElqTiIhkJjN72d1nxLOuWurdyM3O6jPQAc6dXM3HTh7N\nPU+v5rm3tg1AZSIiIj1TqB+l6y+cwtihRVz769fYtb8p6nJERCSDKdSPUmFeDt+8ZDr1DY3c+NCi\nqMsREZEMplDvB9NHDeafz53AQ69tZN7rG6MuR0REMpRCvZ989pzxHD96MDf8/k027twfdTkiIpKB\nFOr9JCc7i29+ZLquNiciIpFRqPejsRVFutqciIhERqHezy45cRTnT6nma48uZ1nd7qjLERGRDKJQ\n72dmxu0fOo7SQTm62pyIiAwohXoCVBTn87UPv4tldQ18fcGKqMsREZEMoVBPkHMnV/NRXW1OREQG\nkEI9gW6IudrcnsbmqMsREZE0p1BPoMK8HG774DQ27jrAY0vqoi5HRETSnEI9wU4ZN5SqknwWLK6P\nuhQREUlzCvUEy8oy3je1mr8s38KBJo2EFxGRxFGoD4CZtTXsb2rhmZVboy5FRETSmEJ9AJx6zFBK\nCnKYv1jH1UVEJHEU6gMgLyeLcydX8eel9TS3tEZdjoiIpCmF+gCZObWGHfuaWLh2R9SliIhImlKo\nD5CzJlWSl5OlUfAiIpIwCQ11M5ttZsvNbJWZXdfN86PN7Ekze9XM3jCzOYmsJ0rF+TmcfmwF8xfX\n4a5pWUVEpP8lLNTNLBu4E7gAmApcZmZTu6x2A/Brdz8euBT4fqLqSQazaqvZsHM/izdq9jYREel/\niWypnwSscvfV7n4QuA+Y22UdB0rD+2XAxgTWE7nzplSTZbBgibrgRUSk/yUy1EcA62Ierw+XxboJ\nuNzM1gMPA/+UwHoiV1Gcz4wx5SzQqW0iIpIAUQ+Uuwz4qbuPBOYA95rZITWZ2VVmttDMFm7ZsmXA\ni+xPM2urWVbXwNpte6MuRURE0kwiQ30DMCrm8chwWawrgV8DuPtzQAFQ0fWN3P1ud5/h7jMqKysT\nVO7AmDm1BkCj4EVEpN8lMtRfAiaY2TgzyyMYCDevyzrvAOcBmNkUglBP7aZ4H0YPLWRyTQkLNGub\niIj0s4SFurs3A58D5gNLCUa5LzazW8zsonC1a4FPmdnrwK+Av/cMON9rVm0NC9fuYEtDY9SliIhI\nGslJ5Ju7+8MEA+Bil90Yc38J8N5E1pCMZtZW8+3HV/L40nouPWl01OWIiEiaiHqgXEaaOqyUkUMG\naYIXERHpVwr1CJgZs2preHbVNvY0NkddjoiIpAmFekRmTq3mYEsrf1m+OepSREQkTSjUIzJjbDnl\nRXk6tU1ERPqNQj0i2VnG+VOqeHLZZg42a451ERE5egr1CM2qraGhsZn/e2tr1KWIiEgaUKhH6L3H\nVlCYl60JXkREpF8o1CNUkJvN2ZMqeWxJPa2taX/NHRERSTCFesRmTq1hS0Mjr67bGXUpIiKS4hTq\nETtnchU5WabpWEVE5Kgp1CNWNiiXU8cPZf7iOjLgsvciIpJACvUkMLO2hre37WPl5j1RlyIiIilM\noZ4E3jelGkBd8CIiclQU6kmgpqyA6aMGM19XlxMRkaOgUE8SM2ureXPDLjbu3B91KSIikqIU6kli\nVm0NoC54ERE5cgr1JDG+sphjq4p1dTkRETliCvUkMnNqNS+s2c6OvQejLkVERFKQQj2JzKqtoaXV\neWKZ5lgXEZHDp1BPIseNKKOmtID5Oq4uIiJHQKGeRLKyjJm11Ty1cgv7D7ZEXY6IiKQYhXqSmTm1\nhgNNrTy1ckvUpYiISIpRqCeZk48pp7QgR13wIiJy2BTqSSY3O4vzplTz+NLNNLe0Rl2OiIikEIV6\nEppVW82u/U28uGZ71KWIiEgKUagnoTMnVpKfk6UL0YiIyGFRqCehwrwczphQyQLNsS4iIodBoZ6k\nZtZWs3HXARZt2B11KSIikiIU6knq/CnVZBkaBS8iInFTqCep8qI8ThxbzoIlCnUREYmPQj2Jzaqt\nYUX9HtZs3Rt1KSIikgIU6klsZm01oDnWRUQkPgr1JDZySCG1w0t1XF1EROKiUE9ys2preOWdnWze\nfSDqUkREJMkp1JNcWxf8Y0t1IRoREemdQj3JTaouYczQQuYvVqiLiEjveg11M8s2s9sHqhg5lJkx\nc2o1z721ld0HmqIuR0REklivoe7uLcA5A1SL9GBWbQ1NLc6TyzZHXYqIiCSxnDjWednMfgc8ALSf\nMO3u8xJWlXRy/OghVBTnsWBJPXOnj4i6HBERSVLxhHoJQZjPiVnmgEJ9gGRnGe+bWs281zZyoKmF\ngtzsqEsSEZEk1Geou/vHB6IQ6d3MqTX86sV1PPfWNs6ZXBV1OSIikoT6HP1uZsPN7AEz2xTe7jez\n4QNRnHQ47dihFOVl60I0IiLSo3hOafsJsAAYG94eC5fJAMrPyebsyVX8eWk9La2aY11ERA4VT6hX\nu/s97t4Y3n4IVCe6MDnUrNoatu45yCvv7Ii6FBERSULxhPp2M7vUOlwCbE90YXKosydVkpttmuBF\nRES6FU+ofxK4AtgKbAE+Hi6TAVZakMtp4yuYv7ged3XBi4hIZ31eUQ64yN3nuPtQd69w9/e7+9sD\nU550Nau2hne272N5fUPUpYiISJKJ54pylw9QLRKH86dWYQbzF+la8CIi0lk83e/PmNm3zOxUM3tX\n2y3hlUm3qkoKOGH0EJ3aJiIih4jninInhj/fE7PMgTP7vxyJx8yp1fznI8tYt30fo8oLoy5HRESS\nRDzH1L/l7md0uSnQIzSztgaABUvUBS8iIh3iOab+pSN9czObbWbLzWyVmV3XwzofMbMlZrbYzH55\npJ+VScZVFDGxulintomISCfxHFNfYGZfMLNhZlbaduvrRWEr/07gAmAqcJmZTe2yzgTg34H3unst\n8IXD34TMNKu2hpfe3s62PY1RlyIiIkkinlC/HLgWeBFYBCwOf/blJGCVu69294PAfcDcLut8CrjT\n3XcAuLsmDI/TzKk1tDo8rjnWRUQk1Geou/uomNvotp9xvPcIYF3M4/XhslgTgYlm9qyZPW9ms+Mv\nPbNNG1HK8LICdcGLiEi7HkPdzK6Nuf83XZ77aj99fg4wATgbuAy4x8wGd1PLVWa20MwWbtmypZ8+\nOrWZGTNra3hq5Vb2NjZHXY6IiCSB3lrqH4u5f0OX5y6M4703AKNiHo8Ml8VaD8xz9yZ3XwOsIAj5\nTtz9bnef4e4zKisr4/jozDCztpqDza08tUJfdEREpPdQtx7ud/e4Oy8BE8xsnJnlAZcC87qs8yBB\nKx0zqyDojl8dx3sLcNLYcgYX5urUNhERAXoPde/hfnePD32xezPwOWA+sBT4tbsvNrNbzOyicLX5\nwDYzWwI8Cfyru2+Lu/oMl5OdxXmTq3l8aT1NLa1RlyMiIhHr7Ypy7zaz7QSt8pLwPuHj4nje3N0f\nBh7usuzGmPsOXBPe5AjMqq3mt6+s54XV2zl9QkXU5YiISIR6C/W8AatCjtgZEyopyM1i/uI6hbqI\nSIbrsfvd3Vt6uw1kkdKzQXnZnDWxkgVL6mht1RzrIiKZLJ6Lz0iSmzm1hvrdjbyxYVfUpYiISIQU\n6mngvClVZGcZD77a9YxBERHJJAr1NDC4MI8PnTCCXzy/lhX1DVGXIyIiEekz1M1sh5lt73JbY2YP\nmNnYxJco8fji7MkU5efw5QcXEZxUICIimSaelvqdwJeB8eHtBuABggvH/CRxpcnhGFqczxdnT+aF\nNdt58DV1w4uIZKJ4Qv0D7n6nu+8Ib98HZrr7/wLlCa5PDsOlJ47i3aMGc9uflrFrf1PU5YiIyACL\nJ9T3x07oEt5vm8RblzFLIllZxq1zp7F9byPfWLA86nJERGSAxTuf+qfCY+nbCOZA/7iZFQJfSGh1\nctiOG1nG5aeM4d7n17JIp7iJiGSUeOZTX+XuF7h7ubsPDe+vcPd97v7XgShSDs+1MydRXpTHDQ8u\n0gVpREQySDyj3yvM7N/M7PtmdnfbbSCKkyNTNiiXL82ZwmvrdnL/wnVRlyMiIgMknu73h4Bq4Bng\n8ZibJLEPHj+Ck8aVc8ejy9i+92DU5YiIyACIJ9SL3P1ad/+lu9/fdkt4ZXJUzIxbL57GngPN3PHI\nsqjLERGRARBPqD9iZjMTXon0u4nVJXzy9HHcv3AdL6/dEXU5IiKSYPGE+qeBR81sTzgCfkfM3OqS\n5D5/3gRqSgu44cFFNLfoDEQRkXQWT6hXALlAGVAZPq5MZFHSf4ryc7jxA1NZumk39z6/NupyREQk\ngXoMdTObEN6t7eEmKeKCaTWcObGSry9YwebdB6IuR0REEqS3lvp14c87u7l9L8F1ST8yM26+qJaD\nza3c9vDSqMsREZEEyenpCXe/Mrx7rrt3upC4meUmtCrpd+Mqivj02eP5zuMruWTGKE47tiLqkkRE\npJ/Fc0z9hTiXSZL7zNnjGVU+iC8/tIiDzRo0JyKSbno7pl5lZu8GBpnZcWb2rvB2OlA4cCVKfynI\nzeaWi6bx1pa9/PCZ1VGXIyIi/azH7nfgQuCTwEiC4+gWLm8gmF9dUtA5k6uYObWa7z6+irnTRzBi\n8KCoSxIRkX7SY0vd3X/i7mcAV7r7me5+Rnib4+4PDGCN0s9u/MBUAG75w+KIKxERkf4UzzH1KjMr\nBTCzu8zsRTM7L8F1SQKNHFLIP513LPMX1/Pkss1RlyMiIv0knlC/yt13h5eKHUYwn/rXEluWJNo/\nnH4M4yuL+Mq8xRxoaom6HBER6QfxhHrbhNxzgJ+7++txvk6SWF5OFl+9eBrvbN/H9//yVtTliIhI\nP4gnnF83s4eB9xNM7lJMR9BLCjttfAUXvXs4d/31LdZs3Rt1OSIicpTiCfVPADcBJ7n7PqAAuLLX\nV0jKuOHCKeRlZ/GVeYtx13c1EZFU1meou3sLcAxwdbhoUDyvk9RQVVrANe+byFMrtvDoorqoyxER\nkaPQZzib2feAc4DLw0V7gbsSWZQMrCtOHcOUYaXc/Icl7G1sjrocERE5QvG0uE9z938EDgC4+3Yg\nL6FVyYDKyc7i1ounUbf7AN95fGXU5YiIyBGKJ9SbzCyLcHCcmQ0FdOHwNPOeMUO4ZMYofvTMGlbU\nN0RdjoiIHIHerv3edgnZO4HfApVmdjPwDHDHANQmA+yLF0ymuCCHGx5cpEFzIiIpqLeW+osA7v5z\n4Abgv4EdwN+6+30DUJsMsPKiPL44ezIvrtnO71/dEHU5IiJymHqb0KVtAhfcfTGgC4VngEtmjOL+\nl9bxHw8v5bwp1ZQNyo26JBERiVNvoV5pZtf09KS7fyMB9UjEsrKMWy+exkXfe4ZvLFjOzXOnRV2S\niIjEqbfu92ygGCjp4SZpatqIMj5+yhjufX4tizbsirocERGJU28t9U3ufsuAVSJJ5ZqZk/jTm3Vc\n/+Aifn/1aWRlWd8vEhGRSPXWUtf/4hmsbFAu1184mdfX7eS+l9ZFXY6IiMSht1DXnOkZ7uLpIzh5\nXDl3PLqMbXsaoy5HRET60GOoh1eOkwxmZnz14mnsbWzmjkeXRV2OiIj0QROzSK8mVpdw5enj+PXC\n9by8Vt/zRESSmUJd+vTP501gWFkB1/9+Ec0tukKwiEiyUqhLn4ryc7jx/VNZVtfAz59bG3U5IiLS\nA4W6xGX2tBrOmljJNx5bQf3uA1GXIyIi3VCoS1zMjJsvquVgSyu3/Wlp1OWIiEg3FOoSt7EVRVx9\n1njmvb6RZ1dtjbocERHpQqEuh+Xqs8czuryQLz+0iIPNGjQnIpJMFOpyWApys7l5bi2rt+zlnqdX\nR12OiIjEUKjLYTtnUhWzaqv57hMrWb9jX9TliIhIKKGhbmazzWy5ma0ys+t6We9DZuZmNiOR9Uj/\nufEDtRjGLX9YEnUpIiISSliom1k2cCdwATAVuMzMpnazXgnweeCFRNUi/W/E4EH883kTWLCknieW\n1UddjoiIkNiW+knAKndf7e4HgfuAud2s91XgDkAnP6eYK08fx7FVxXxl3mIONLVEXY6ISMZLZKiP\nAGLn7FwfLmtnZicAo9z9TwmsQxIkLyeLW+bWsm77fr7/5KqoyxERyXiRDZQzsyzgG8C1cax7lZkt\nNLOFW7ZsSXxxErfTxlcwd/pw7vrratZs3Rt1OSIiGS2Rob4BGBXzeGS4rE0JMA34i5m9DZwCzOtu\nsJy73+3uM9x9RmVlZQJLliNx/Zwp5OdkceNDi3D3qMsREclYiQz1l4AJZjbOzPKAS4F5bU+6+y53\nr3D3se4+FngeuMjdFyawJkmAqtICrpk5kadXbuWRRXVRlyMikrESFuru3gx8DpgPLAV+7e6LzewW\nM7soUZ8r0fj4KWOYOqyUW/6whD2NzVGXIyKSkRJ6TN3dH3b3ie4+3t1vC5fd6O7zuln3bLXSU1dO\ndhZfvXgadbsP8J3HV0ZdjohIRtIV5aTfvGfMEC49cRQ/emYNy+saoi5HRCTjKNSlX/3b7MmUFOTw\n5Qc1aE5EZKAp1KVflRflcd3sybz49nZ+98qGvl8gIiL9RqEu/e4jM0Zx/OjB/OcjS9m1vynqckRE\nMoZCXfpdVpbx1bnT2L73IF9fsDzqckREMoZCXRJi2ogyrjh1LPc+v5Y31++KuhwRkYygUJeEuWbm\nRIYW5XPDg2/S0qpBcyIiiaZQl4QpLcjlhgun8Pr6Xdz30jtRlyMikvYU6pJQc6cP55Rjyvnao8vZ\ntqcx6nJERNKaQl0SyiwYNLe3sZnbH1kWdTkiImlNoS4JN6G6hCvPGMcDL69n4dvboy5HRCRtKdRl\nQPzzuRMYXlbADQ8uormlNepyRETSkkJdBkRRfg43fmAqy+oa+Nlza6MuR0QkLSnUZcDMqq3h7EmV\nfPOxFdTvPhB1OSIiaUehLgPGzLj5oloOtrRy65+WRl2OiEjaUajLgBoztIjPnD2eP7y+kWdXbY26\nHBGRtKJQlwH36bPGM2ZoIV9+aBGNzS1RlyMikjYU6jLgCnKzuemiWlZv2csPn14TdTkiImlDoS6R\nOGdSFbNra/juEytZt31f1OWIiKQFhbpE5sYPTMUwbvnjkqhLERFJCwp1iczwwYP4/PkTeGxJPY8v\nrY+6HBGRlKdQl0h98r3jOLaqmK/MW8z+gxo0JyJyNBTqEqm8nCy+Onca63fs5/t/WRV1OSIiKU2h\nLpE7dfxQLp4+nB/8dTWrt+yJuhwRkZSlUJek8KULp5Cfk8VX5i3G3aMuR0QkJSnUJSlUlRRw7cyJ\nPL1yKw+/WRd1OSIiKUmhLknj8lPGUDu8lFv+uJgX12xn1/6mqEsSEUkpOVEXINImJzuLWy+exiV3\nP89HfvAcAMPKCphUU8KkmhIm15QwqbqU8VVF5OdkR1ytiEjyUahLUjl+9BCe+eI5LN6wm2V1DSyv\nC34+u2orTS3BsfacLGNcRVFH0NeUMrmmhBGDB5GVZRFvgYhIdBTqknSqSgqomlzAOZOr2pc1tbSy\nZuve9qBfXtfAa+t28sc3NrWvU5SXzcT2Fn1H2A8pyotiM0REBpyl2kjjGTNm+MKFC6MuQ5LEnsZm\nltc1hLewdV/fwM59Hcfjq0ryD2nVH1tVTEGuuvBFJPmZ2cvuPiOeddVSl5RWnJ/De8YM4T1jhrQv\nc3c2NzR26r5fXtfAz55by8HmVgCyDMZWFLUfp28L/dHlherCF5GUpVCXtGNmVJcWUF1awFkTK9uX\nN7e08va2fZ1a9Ys37uaRRXW0dVgNys1mYnVxODivNGzdl1BRnB/R1oiIxE/d75Lx9h1sZkX9nk6t\n+uV1DWzbe7B9nYrivCDoqzuCfmJ1CYPy1IUvIoml7neRw1CYl8P0UYOZPmpwp+VbGhpZXtfAsnBg\n3vL6Bn754loONAVd+GYwprzwkFb92KFFZKsLX0QioFAX6UFlST6VJfmcPqGifVlLq/PO9n2HtOof\nW1JPa9jplZ+TxYTq4k6t+sk1JVSW5GOmsBeRxFH3u0g/ONDUwsr6PZ1a9cvqGtjS0Ni+zpDC3DDg\nS9svqDOpuoSifH23FpGeqftdZIAV5GZz3MgyjhtZ1mn59r0HO4K+Lgj6Xy9cx76YueNHlQ86pFU/\nrqKInGxdxVlEDo9CXSSByovyOG18BaeN7+jCb2111u/Y3x72y+qDwH9y+WZawj78vOwsxlcVtwd9\nW9jXlBaoC19EeqRQFxlgWVlQ8DUPAAAQBUlEQVTG6KGFjB5ayMzamvbljc0trNq8p1Or/rm3tvH7\nVze0r1NakNOp+35yTQkTa0ooLciNYlNEJMko1EWSRH5ONrXDy6gd3rkLf9e+pqBVHx6nX17XwIOv\nbqChsbl9nRGDB3We+KamhGMqisnLURe+SCZRqIskubLCXE4+ZignHzO0fZm7s2Hn/vYWfVvr/qkV\nW2hu7Zj4Znxl8SFhP2LwIHXhi6QphbpICjIzRg4pZOSQQs6bUt2+/GBzK6u37ukU9i+v3cG81ze2\nr1OSn8PETtPZBiPyywrVhS+S6nRKm0gG2H2giRVdWvXL6naz+0BHF35NaUGnFv2kcOIbzV0vEi2d\n0iYinZQW5DJjbDkzxpa3L3N36nYf6BL0weC8gy3BVfOyY+eury5pP89+5BDNXS+SjBTqIhnKzBhW\nNohhZYM4Z1Lnuevfbp+7Pgj6N9bv5E9d5q6fUF3S5ZS7Uso1d71IpNT9LiJx2dPYzIr6zt33y+sa\n2BEzd31lSX77cfq2oJ9QrbnrRY6Gut9FpN8V5+dwwughnDC689z1W9rnrg+P2dfv5t7n19IYO3f9\n0KIuo/BLGV1eqIlvRPqZQl1EjpiZUVVaQFVpAWfGzF3f0uq8vW1vzCj83SzdtJtHF3fMXV+Qm8XE\n6s6t+knhxDcicmTU/S4iA2bfwWZW1u9pn/SmLfS37umY+GZoUd4hrfqJ1cUU5qkNIplJ3e8ikpQK\n83J496jBvLvL3PXb9jR2Ord+WX0D9724jv1NwcQ3ZjC6vDA8pz4I+mDu+kJNfCMSI6GhbmazgW8D\n2cAP3f32Ls9fA/wD0AxsAT7p7msTWZOIJJ+hxfmcdmw+px3beeKbdTv2HXJu/Z+Xdsxdn5eTxYSq\n4k6t+sk1JVRp7nrJUAnrfjezbGAF8D5gPfAScJm7L4lZ5xzgBXffZ2ZXA2e7+yW9va+630Uy24Gm\nmIlv6juO2dfv7ujCH1yYe0irflJNCcWau15SULJ0v58ErHL31WFR9wFzgfZQd/cnY9Z/Hrg8gfWI\nSBooyM1m2ogypo3oPPHNjr0HOx2nX1a3m9+8vJ69MXPXjxwyiMk1JYwdWsSwwYMYXlbQ/rOiOF8X\n1JGUl8hQHwGsi3m8Hji5l/WvBB5JYD0iksaGFOVxyjFDOSVm4pvW1o6Jb2Jb9c+s2sqBptZOr8/N\nNqpLCxheNojhgzvCfljZIIYNDpYPLsxVt74ktaToizKzy4EZwFk9PH8VcBXA6NGjB7AyEUllWVnG\nqPJCRpUXcv7Ujolv3J2d+5rYuGs/m3YeYNOu/WzcdYBNO/ezcecBFq7dQf2bm2hq6Xx4siA3i+Fh\nyA8rC0J/+OBBnVr96uKXKCXyr28DMCrm8chwWSdmdj5wPXCWuzd2fR7A3e8G7obgmHr/lyoimcTM\nGFKUx5CivEPmr2/T2ups3dPYEfbhz027DrBx136eWbmV+oYDdB2WVFKQc0jwDxsctP6Hlw2ipqxA\nV9iThElkqL8ETDCzcQRhfinw0dgVzOx44AfAbHffnMBaREQOS1ZWx4V1pnc5Ba9NU0sr9bsPBEEf\nBn77F4Bd+3lz/S627T14yOuGFuUdEvrD2lr9ZQVUlxaQq1P15AgkLNTdvdnMPgfMJzil7cfuvtjM\nbgEWuvs84L+AYuCB8DjVO+5+UaJqEhHpT7nZWe3z2vfkQFNL57CPCf13tu3j+dXbaIiZAheCS+tW\nlRS0H8sfFjOgTwP7pDe6opyISMT2NDa3h/3Gnfs7Bf+mnUF3f48D+7qEvQb2pZ9kOaVNRETiUJyf\nw4TqEiZUl3T7fE8D+4IvAD0P7BuUmx228sOu/q5fADSwL+1ob4qIJLn+GNj39MotbG5ojGtgXzCi\nXwP7UpFCXUQkDUQxsG9EeDpfdUm+rsGfJBTqIiIZQgP70p9CXURE2hXkZjOuoohxFUU9rtM2sG9D\nN639pZt28/iy+m4H9tWUdTl3XwP7+p1CXUREDsuRDOzbsLNjNH9fA/vaztfv2trXwL6+6bcjIiL9\n6mgH9m3YuZ+n4hjYN7yb1n6mD+xTqIuIyIAb6IF9safzpfPAPoW6iIgkpSgG9rWdzldRlJoD+xTq\nIiKSsjSwrzOFuoiIpLXDHdi3cVcwBW9cA/t6OI3vzAkVkQS+Ql1ERDLa0Qzsa/sCEDuwr7Qghzdu\nmjXAWxFQqIuIiPThcAb27dzXNMDVdVCoi4iI9IOOgX3R1ZCeY/pFREQykEJdREQkTSjURURE0oRC\nXUREJE0o1EVERNKEQl1ERCRNKNRFRETShEJdREQkTSjURURE0oRCXUREJE2Yu/e9VhIxsy3A2n58\nywpgaz++X5S0LcknXbYDtC3JKl22JV22A/p/W8a4e2U8K6ZcqPc3M1vo7jOirqM/aFuST7psB2hb\nklW6bEu6bAdEuy3qfhcREUkTCnUREZE0oVCHu6MuoB9pW5JPumwHaFuSVbpsS7psB0S4LRl/TF1E\nRCRdqKUuIiKSJjI61M1stpktN7NVZnZd1PX0xczeNrM3zew1M1sYLis3s8fMbGX4c0i43MzsO+G2\nvWFmJ0Rc+4/NbLOZLYpZdti1m9nfheuvNLO/S6JtucnMNoT75jUzmxPz3L+H27LczGbFLI/078/M\nRpnZk2a2xMwWm9nnw+Upt1962ZZU3C8FZvaimb0ebsvN4fJxZvZCWNf9ZpYXLs8PH68Knx/b1zYm\nwbb81MzWxOyX6eHypP0bC2vINrNXzeyP4ePk2yfunpE3IBt4CzgGyANeB6ZGXVcfNb8NVHRZ9jXg\nuvD+dcAd4f05wCOAAacAL0Rc+5nACcCiI60dKAdWhz+HhPeHJMm23AT8SzfrTg3/tvKBceHfXHYy\n/P0Bw4ATwvslwIqw3pTbL71sSyruFwOKw/u5wAvh7/vXwKXh8ruAq8P7nwHuCu9fCtzf2zYmybb8\nFPhwN+sn7d9YWMc1wC+BP4aPk26fZHJL/SRglbuvdveDwH3A3IhrOhJzgZ+F938GXByz/OceeB4Y\nbGbDoigQwN2fArZ3WXy4tc8CHnP37e6+A3gMmJ346jvrYVt6Mhe4z90b3X0NsIrgby/yvz933+Tu\nr4T3G4ClwAhScL/0si09Seb94u6+J3yYG94cOBf4Tbi8635p21+/Ac4zM6PnbRwwvWxLT5L2b8zM\nRgIXAj8MHxtJuE8yOdRHAOtiHq+n9/8EkoEDC8zsZTO7KlxW7e6bwvt1QHV4PxW273BrT/Zt+lzY\nZfjjti5rUmRbwu7B4wlaUim9X7psC6Tgfgm7eV8DNhME2FvATndv7qau9prD53cBQ0nSbXH3tv1y\nW7hfvmlm+eGyZN4v3wL+DWgNHw8lCfdJJod6Kjrd3U8ALgA+a2Znxj7pQf9OSp7OkMq1h/4HGA9M\nBzYBX4+2nPiZWTHwW+AL7r479rlU2y/dbEtK7hd3b3H36cBIgpbc5IhLOmJdt8XMpgH/TrBNJxJ0\nqX8xwhL7ZGbvBza7+8tR19KXTA71DcComMcjw2VJy903hD83A78n+Mde39atHv7cHK6eCtt3uLUn\n7Ta5e334n1crcA8dXWpJvS1mlksQgv/r7r8LF6fkfuluW1J1v7Rx953Ak8CpBF3ROd3U1V5z+HwZ\nsI3k3ZbZ4eESd/dG4Cck/355L3CRmb1NcEjmXODbJOE+yeRQfwmYEI5ezCMYzDAv4pp6ZGZFZlbS\ndh+YCSwiqLltJOjfAQ+F9+cBV4SjSU8BdsV0qSaLw619PjDTzIaE3agzw2WR6zJe4YME+waCbbk0\nHA07DpgAvEgS/P2Fx/h+BCx192/EPJVy+6WnbUnR/VJpZoPD+4OA9xGMEXgS+HC4Wtf90ra/Pgw8\nEfaw9LSNA6aHbVkW86XRCI5Dx+6XpPsbc/d/d/eR7j6W4G/iCXf/GMm4T452pF0q3whGWq4gOF51\nfdT19FHrMQSjJl8HFrfVS3Cc5nFgJfBnoDxcbsCd4ba9CcyIuP5fEXR/NhEcR7rySGoHPkkwuGQV\n8Ikk2pZ7w1rfIPiHOyxm/evDbVkOXJAsf3/A6QRd628Ar4W3Oam4X3rZllTcL+8CXg1rXgTcGC4/\nhiAAVgEPAPnh8oLw8arw+WP62sYk2JYnwv2yCPgFHSPkk/ZvLKaOs+kY/Z50+0RXlBMREUkTmdz9\nLiIiklYU6iIiImlCoS4iIpImFOoiIiJpQqEuIiKSJhTqIhEys6HWMVNVnXWeUSwvzvf4iZlN6mOd\nz5rZx/qn6viZ2bnh+cbxrj/KzO5PZE0i6UyntIkkCTO7Cdjj7v/dZbkR/Ftt7faFSczMbgW2uvu3\noq5FJBOopS6ShMzsWAvmBv9fgosNDTOzu81soQXzUt8Ys+4zZjbdzHLMbKeZ3W7B/NXPmVlVuM6t\nZvaFmPVvt2Ce6+Vmdlq4vMjMfht+7m/Cz5reTW3/Fa7zhpndES6rNrPfha950cxOMbPxwD8A/xr2\nPJzW5X3ODet8zcxeCT//WAsm/2jrgWjrtdhqZteHy68LP+ONtt+DmZWY2SPh+y0ysw8jkoFy+l5F\nRCIyGbjC3RdCEGbuvt2Ca0k/aWa/cfclXV5TBvzV3a8zs28QXIXr9m7e29z9JDO7CLiRYBrLfwLq\n3P1DZvZu4JVDXmRWTXDFtVp397ZLgALfAb7m7s9bMEvaH919mpn9kJ5b6v8KXOXuL1gwEcuB2Cfd\n/RPhZ44DHgZ+ZmZzgNHAyQRXH3s4/LIwCnjb3S8IX1PW7W9UJM2ppS6SvN5qC/TQZWb2CkHYTgGm\ndvOa/e7+SHj/ZWBsD+/9u27WOZ1gsgrcve1yxF1tJ5h68h4z+yCwN1x+PnBX2Mp+EBgSXuu7N88C\n3zazfwJK3b2l6wpmVkhwuc3PuPt6gmt+X0Bw6dFXgGOBiQSXIZ0d9kC819139fHZImlJLXWR5NUW\nmJjZBODzwEnuvtPMfkFwfemuDsbcb6Hnf+ONcaxzCHdvMrMZBBNz/C1wNUHQWlhb7OcTDAfo8b1u\nNbN5wIXA82Z2HodO83o3cJ+7P9n2lsCt7v6jru8X1jUHuN3MHnH3/4h3u0TShVrqIqmhFGgAdocz\nXM1KwGc8C3wEwMyOo5ueAAtmCix19z8C/w84Pnzqz8BnY9ZrOxbfAJR092FmNt7d33D3/yRodU/q\n8vzngdwuAwfnA1daMFMhZjbSzCrMbATBIMN7CeZMP+GwtlwkTailLpIaXgGWAMuAtQQB3N++C/zc\nzJaEn7UE6NqNXQb8zszyCRoF14TLPwv8j5l9guD/lSfDZQ8BD5jZ3wCfdff/i3mvfzGzMwi6898A\nFhAcL29/HtjXNnAO+J67/9DMJhO07CH40vBRgi8gt5tZK0FvxaeP7lchkpp0SpuIABAOwMtx9wNh\nd/8CYIK7N0dcmojESS11EWlTDDwehrsB/6hAF0ktaqmLiIikCQ2UExERSRMKdRERkTShUBcREUkT\nCnUREZE0oVAXERFJEwp1ERGRNPH/AVZRw/x/cKbsAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 576x360 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"k2rE6GbVN-it","colab_type":"text"},"source":["Here, removing samples strictly increases the training error. This is extremely similar to what we say before. It also makes sense as we definitely need more samples in order to train the neural network. The change error is is greater if the % change in datasets is greater. Overall, this is definitely the behavior that I would expect."]},{"cell_type":"markdown","metadata":{"id":"8EvwD7rldQmR","colab_type":"text"},"source":["## Part 3. Cancer Gene Expression Autoencoder (40 points)"]},{"cell_type":"markdown","metadata":{"id":"6fcH_s0fOzUw","colab_type":"text"},"source":["#### Loading the Data and Preprocessing"]},{"cell_type":"code","metadata":{"id":"wwFfy4bF4UI5","colab_type":"code","colab":{}},"source":["rnaseq = pd.read_csv(\n","    \"/content/drive/My Drive/MLiM-Datasets/HW2/combined_rnaseq_data_lincs1000_combat.tsv\",\n","    keep_default_na=False,\n","    delimiter='\\t',\n","    na_values=[])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UY3nIio345yA","colab_type":"code","colab":{}},"source":["_ = rnaseq.pop(\"Sample\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dbVohvSK6WRo","colab_type":"code","colab":{}},"source":["def normalize_large_data(df):\n","  scaler = preprocessing.StandardScaler()\n","  columns = df.columns\n","  scaled_df = scaler.fit_transform(df)\n","  return pd.DataFrame(scaled_df, columns=columns)\n","\n","rnaseq = normalize_large_data(rnaseq)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UUEIXfvJdSz7","colab_type":"text"},"source":["### a. Using Keras, build an autoencoder that takes the gene expression values as input, encodes to 50 dimensions and then decodes back to the original width of the input."]},{"cell_type":"code","metadata":{"id":"qL0bgNVreazJ","colab_type":"code","colab":{}},"source":["# A model for building an autoencoder with parameters input size and number of classes.\n","def autoencoder_model(i, b):\n","  model = models.Sequential()\n","  model.add(layers.InputLayer(i,))\n","  model.add(layers.Dense(512, activation='tanh'))\n","  model.add(layers.Dense(256, activation='tanh'))\n","  model.add(layers.Dense(128, activation='tanh'))\n","  model.add(layers.Dense(b, activation='tanh'))\n","  model.add(layers.Dense(128, activation='tanh'))\n","  model.add(layers.Dense(256, activation='tanh'))\n","  model.add(layers.Dense(512, activation='tanh'))\n","  model.add(layers.Dense(i, activation='linear'))\n","  model.compile(loss='mean_squared_error', optimizer='adam')\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CCizV9jYgYDR","colab_type":"code","colab":{}},"source":["# Defines a function to run a bottleneck test. It uses 5-fold and uses mean squared error to determine loss\n","def test_bottleneck(X, autoencoder):\n","  losses = []\n","  X_test = X[int(rnaseq.shape[0]*0.8):]\n","  kf = KFold(n_splits=5)\n","  autoencoder.save_weights('empty')\n","  for train_index, test_index in kf.split(X[:int(rnaseq.shape[0]*0.8)]):\n","    X_train, X_val = X.values[train_index], X.values[test_index]\n","    hist = autoencoder.fit(X_train, X_train,\n","                  epochs=40,\n","                  batch_size=128,\n","                  shuffle=True,\n","                  validation_data=(X_val, X_val))\n","    losses.append(autoencoder.evaluate(X_test,X_test))\n","    #Resets the model.\n","    autoencoder.load_weights('empty')\n","  return np.mean(losses)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"49HwyKRlvFOj","outputId":"d51ccd73-9c71-4199-d950-6b6ada51f9e5","executionInfo":{"status":"ok","timestamp":1571349632092,"user_tz":300,"elapsed":129703,"user":{"displayName":"Brendan Sanderson","photoUrl":"","userId":"05405297474563600402"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["loss = test_bottleneck(rnaseq, autoencoder_model(len(rnaseq.columns), 50))\n","print()\n","print(\"The testing error for a bottlneck of 50 is:\", loss)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 146us/sample - loss: 0.4710 - val_loss: 0.7140\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.3153 - val_loss: 0.6804\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2876 - val_loss: 0.6681\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2678 - val_loss: 0.6555\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2545 - val_loss: 0.6450\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2446 - val_loss: 0.6302\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2379 - val_loss: 0.6168\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2290 - val_loss: 0.6075\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2230 - val_loss: 0.5971\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.2172 - val_loss: 0.5912\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 88us/sample - loss: 0.2116 - val_loss: 0.5819\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2072 - val_loss: 0.5752\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2022 - val_loss: 0.5712\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1986 - val_loss: 0.5664\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1950 - val_loss: 0.5629\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.1910 - val_loss: 0.5553\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1873 - val_loss: 0.5510\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.1837 - val_loss: 0.5479\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.1824 - val_loss: 0.5467\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.1798 - val_loss: 0.5429\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.1765 - val_loss: 0.5362\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1740 - val_loss: 0.5333\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1713 - val_loss: 0.5326\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1705 - val_loss: 0.5295\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1681 - val_loss: 0.5300\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1671 - val_loss: 0.5303\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1660 - val_loss: 0.5284\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1641 - val_loss: 0.5284\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.1636 - val_loss: 0.5259\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1622 - val_loss: 0.5248\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.1628 - val_loss: 0.5266\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1601 - val_loss: 0.5226\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1576 - val_loss: 0.5239\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.1564 - val_loss: 0.5218\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1554 - val_loss: 0.5215\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1551 - val_loss: 0.5237\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.1562 - val_loss: 0.5218\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1535 - val_loss: 0.5230\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.1530 - val_loss: 0.5190\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1527 - val_loss: 0.5202\n","3040/3040 [==============================] - 0s 83us/sample - loss: 0.4493\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.5254 - val_loss: 0.5295\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3749 - val_loss: 0.5350\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3430 - val_loss: 0.4874\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.3233 - val_loss: 0.4769\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3081 - val_loss: 0.4698\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2981 - val_loss: 0.4566\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2895 - val_loss: 0.4417\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2828 - val_loss: 0.4362\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2773 - val_loss: 0.4284\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2712 - val_loss: 0.4351\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2631 - val_loss: 0.4286\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2573 - val_loss: 0.4228\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.2535 - val_loss: 0.4199\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2493 - val_loss: 0.4185\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2446 - val_loss: 0.4192\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.2416 - val_loss: 0.4107\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 61us/sample - loss: 0.2380 - val_loss: 0.4164\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2337 - val_loss: 0.4201\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2310 - val_loss: 0.4121\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2297 - val_loss: 0.4195\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 61us/sample - loss: 0.2264 - val_loss: 0.4075\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2242 - val_loss: 0.4085\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2206 - val_loss: 0.4233\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2181 - val_loss: 0.4163\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2158 - val_loss: 0.4140\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.2140 - val_loss: 0.4067\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2122 - val_loss: 0.4107\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2104 - val_loss: 0.4085\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2081 - val_loss: 0.4070\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2084 - val_loss: 0.4071\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.2063 - val_loss: 0.4126\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2042 - val_loss: 0.4176\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2031 - val_loss: 0.4117\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2016 - val_loss: 0.4116\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1999 - val_loss: 0.4163\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1997 - val_loss: 0.4111\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1991 - val_loss: 0.4052\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1973 - val_loss: 0.4108\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.1964 - val_loss: 0.4105\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.1940 - val_loss: 0.4092\n","3040/3040 [==============================] - 0s 86us/sample - loss: 0.3713\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.5299 - val_loss: 0.3949\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3790 - val_loss: 0.3627\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3480 - val_loss: 0.3512\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3256 - val_loss: 0.3335\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 61us/sample - loss: 0.3111 - val_loss: 0.3299\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3016 - val_loss: 0.3169\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2927 - val_loss: 0.3186\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2840 - val_loss: 0.3044\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2772 - val_loss: 0.3016\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 61us/sample - loss: 0.2702 - val_loss: 0.2924\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2657 - val_loss: 0.2917\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2598 - val_loss: 0.2856\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2545 - val_loss: 0.2821\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2502 - val_loss: 0.2828\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 61us/sample - loss: 0.2458 - val_loss: 0.2773\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2422 - val_loss: 0.2770\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2379 - val_loss: 0.2710\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2351 - val_loss: 0.2739\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2320 - val_loss: 0.2674\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2297 - val_loss: 0.2691\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2266 - val_loss: 0.2617\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2236 - val_loss: 0.2607\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2219 - val_loss: 0.2630\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2188 - val_loss: 0.2606\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2163 - val_loss: 0.2602\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2133 - val_loss: 0.2569\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2128 - val_loss: 0.2563\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2098 - val_loss: 0.2521\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2092 - val_loss: 0.2601\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2083 - val_loss: 0.2528\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2067 - val_loss: 0.2542\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2035 - val_loss: 0.2514\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2022 - val_loss: 0.2499\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2005 - val_loss: 0.2509\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.1994 - val_loss: 0.2493\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1984 - val_loss: 0.2502\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1968 - val_loss: 0.2513\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1950 - val_loss: 0.2506\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1949 - val_loss: 0.2499\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1952 - val_loss: 0.2502\n","3040/3040 [==============================] - 0s 87us/sample - loss: 0.3825\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.5111 - val_loss: 0.5367\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3746 - val_loss: 0.5087\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3425 - val_loss: 0.4868\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3242 - val_loss: 0.4695\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3108 - val_loss: 0.4601\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3007 - val_loss: 0.4461\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2903 - val_loss: 0.4377\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2814 - val_loss: 0.4346\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2748 - val_loss: 0.4169\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2694 - val_loss: 0.4087\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2641 - val_loss: 0.4109\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2576 - val_loss: 0.3924\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2520 - val_loss: 0.4009\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2483 - val_loss: 0.4023\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2450 - val_loss: 0.3843\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2407 - val_loss: 0.3821\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2360 - val_loss: 0.3756\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2321 - val_loss: 0.3734\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2285 - val_loss: 0.3689\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2263 - val_loss: 0.3700\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2239 - val_loss: 0.3680\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2215 - val_loss: 0.3628\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2204 - val_loss: 0.3606\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 62us/sample - loss: 0.2167 - val_loss: 0.3668\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2152 - val_loss: 0.3570\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2134 - val_loss: 0.3579\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2109 - val_loss: 0.3537\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2090 - val_loss: 0.3521\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2077 - val_loss: 0.3522\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2062 - val_loss: 0.3583\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2046 - val_loss: 0.3516\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2045 - val_loss: 0.3518\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2015 - val_loss: 0.3488\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 62us/sample - loss: 0.1996 - val_loss: 0.3485\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.1993 - val_loss: 0.3490\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.1998 - val_loss: 0.3439\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.1966 - val_loss: 0.3475\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 61us/sample - loss: 0.1955 - val_loss: 0.3470\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.1959 - val_loss: 0.3436\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 62us/sample - loss: 0.1931 - val_loss: 0.3498\n","3040/3040 [==============================] - 0s 84us/sample - loss: 0.3705\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.5144 - val_loss: 0.5087\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3729 - val_loss: 0.4928\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3423 - val_loss: 0.4732\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.3253 - val_loss: 0.4550\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3118 - val_loss: 0.4482\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3003 - val_loss: 0.4363\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2930 - val_loss: 0.4485\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 61us/sample - loss: 0.2843 - val_loss: 0.4195\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2770 - val_loss: 0.4277\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2698 - val_loss: 0.4202\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2648 - val_loss: 0.4216\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2599 - val_loss: 0.4225\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2560 - val_loss: 0.4172\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2518 - val_loss: 0.4096\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2476 - val_loss: 0.4041\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2463 - val_loss: 0.3995\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2427 - val_loss: 0.4091\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2377 - val_loss: 0.3964\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2337 - val_loss: 0.4091\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2309 - val_loss: 0.3988\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2284 - val_loss: 0.3967\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2263 - val_loss: 0.3926\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2224 - val_loss: 0.4010\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2205 - val_loss: 0.3992\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2196 - val_loss: 0.4056\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2161 - val_loss: 0.4061\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2151 - val_loss: 0.3979\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 62us/sample - loss: 0.2121 - val_loss: 0.4038\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2107 - val_loss: 0.3996\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2094 - val_loss: 0.4121\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 61us/sample - loss: 0.2079 - val_loss: 0.4050\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2082 - val_loss: 0.4018\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 61us/sample - loss: 0.2056 - val_loss: 0.3971\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2034 - val_loss: 0.4016\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2040 - val_loss: 0.4031\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 62us/sample - loss: 0.2020 - val_loss: 0.4043\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.1992 - val_loss: 0.4027\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.1980 - val_loss: 0.4026\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.1979 - val_loss: 0.3968\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1960 - val_loss: 0.4123\n","3040/3040 [==============================] - 0s 84us/sample - loss: 0.3830\n","\n","The testing error for a bottlneck of 50 is: 0.3912997350253557\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YyR_2EZndUgZ","colab_type":"text"},"source":["### b. Experiment with changing the width of the bottleneck layer (10, 20, 30, 40, 60, 80) explain what is happening to the loss value when you change the width."]},{"cell_type":"code","metadata":{"id":"fstita2Bk2_E","colab_type":"code","outputId":"8564c4c7-ca8c-408b-dc1c-58720d5ffa03","executionInfo":{"status":"ok","timestamp":1571350442764,"user_tz":300,"elapsed":793944,"user":{"displayName":"Brendan Sanderson","photoUrl":"","userId":"05405297474563600402"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["bs = [10, 20, 30, 40, 60, 80]\n","testing_error = []\n","for b in bs:\n","  testing_error.append(test_bottleneck(rnaseq, autoencoder_model(len(rnaseq.columns), b)))\n","  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 154us/sample - loss: 0.5602 - val_loss: 0.7880\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.4298 - val_loss: 0.7646\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3972 - val_loss: 0.7513\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3753 - val_loss: 0.7537\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3506 - val_loss: 0.7401\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3275 - val_loss: 0.7341\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3132 - val_loss: 0.7469\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3016 - val_loss: 0.7363\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2929 - val_loss: 0.7328\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2843 - val_loss: 0.7344\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2798 - val_loss: 0.7288\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.2728 - val_loss: 0.7335\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2659 - val_loss: 0.7314\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2628 - val_loss: 0.7279\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2589 - val_loss: 0.7334\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2553 - val_loss: 0.7291\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2519 - val_loss: 0.7307\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2491 - val_loss: 0.7298\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2469 - val_loss: 0.7316\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2446 - val_loss: 0.7331\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2423 - val_loss: 0.7288\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2410 - val_loss: 0.7310\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2393 - val_loss: 0.7295\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2372 - val_loss: 0.7318\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2359 - val_loss: 0.7316\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.2354 - val_loss: 0.7354\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2331 - val_loss: 0.7315\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2312 - val_loss: 0.7315\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2301 - val_loss: 0.7360\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2301 - val_loss: 0.7381\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2288 - val_loss: 0.7370\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2270 - val_loss: 0.7362\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2266 - val_loss: 0.7400\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2262 - val_loss: 0.7367\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2241 - val_loss: 0.7404\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2234 - val_loss: 0.7364\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2235 - val_loss: 0.7393\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2221 - val_loss: 0.7431\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2219 - val_loss: 0.7420\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2216 - val_loss: 0.7414\n","3040/3040 [==============================] - 0s 86us/sample - loss: 0.6510\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.6050 - val_loss: 0.6214\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.4889 - val_loss: 0.5976\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.4678 - val_loss: 0.5929\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.4546 - val_loss: 0.5870\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.4361 - val_loss: 0.5997\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.4165 - val_loss: 0.5772\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.4023 - val_loss: 0.5814\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3890 - val_loss: 0.5868\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3803 - val_loss: 0.5940\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3722 - val_loss: 0.5988\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3643 - val_loss: 0.5945\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3576 - val_loss: 0.5970\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3504 - val_loss: 0.5869\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3466 - val_loss: 0.6002\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3416 - val_loss: 0.6003\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3358 - val_loss: 0.5818\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3316 - val_loss: 0.5959\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3275 - val_loss: 0.5846\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3244 - val_loss: 0.5980\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3210 - val_loss: 0.6089\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3180 - val_loss: 0.5819\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3149 - val_loss: 0.5847\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3122 - val_loss: 0.5916\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3104 - val_loss: 0.6028\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3086 - val_loss: 0.5967\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3061 - val_loss: 0.6021\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3041 - val_loss: 0.6013\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3034 - val_loss: 0.5973\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3020 - val_loss: 0.5947\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3005 - val_loss: 0.6169\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2987 - val_loss: 0.5907\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2967 - val_loss: 0.5953\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2952 - val_loss: 0.6032\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2934 - val_loss: 0.5922\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2932 - val_loss: 0.5960\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2919 - val_loss: 0.5885\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2906 - val_loss: 0.5893\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2897 - val_loss: 0.6057\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2886 - val_loss: 0.5963\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2874 - val_loss: 0.5996\n","3040/3040 [==============================] - 0s 83us/sample - loss: 0.5459\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.6286 - val_loss: 0.4944\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.5031 - val_loss: 0.4812\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.4764 - val_loss: 0.4723\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.4625 - val_loss: 0.4649\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.4447 - val_loss: 0.4571\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.4259 - val_loss: 0.4454\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.4093 - val_loss: 0.4327\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3960 - val_loss: 0.4251\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3852 - val_loss: 0.4137\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3746 - val_loss: 0.4109\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3672 - val_loss: 0.4117\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3593 - val_loss: 0.4050\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3514 - val_loss: 0.3981\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3475 - val_loss: 0.3977\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3425 - val_loss: 0.3937\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3368 - val_loss: 0.3942\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3323 - val_loss: 0.3985\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3278 - val_loss: 0.3933\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3250 - val_loss: 0.3873\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3209 - val_loss: 0.3884\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3177 - val_loss: 0.3880\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3155 - val_loss: 0.3892\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3121 - val_loss: 0.3861\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3088 - val_loss: 0.3895\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.3070 - val_loss: 0.3885\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.3053 - val_loss: 0.3847\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.3045 - val_loss: 0.3826\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.3031 - val_loss: 0.3818\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3013 - val_loss: 0.3811\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2976 - val_loss: 0.3810\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2969 - val_loss: 0.3843\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2940 - val_loss: 0.3772\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2937 - val_loss: 0.3805\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2931 - val_loss: 0.3791\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2914 - val_loss: 0.3784\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2899 - val_loss: 0.3757\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2885 - val_loss: 0.3814\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2880 - val_loss: 0.3814\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2866 - val_loss: 0.3805\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2847 - val_loss: 0.3778\n","3040/3040 [==============================] - 0s 91us/sample - loss: 0.5612\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.5897 - val_loss: 0.6199\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.4748 - val_loss: 0.6092\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.4553 - val_loss: 0.5982\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.4447 - val_loss: 0.5902\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.4336 - val_loss: 0.5776\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.4168 - val_loss: 0.5729\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.4030 - val_loss: 0.5824\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3933 - val_loss: 0.5635\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3835 - val_loss: 0.5583\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3761 - val_loss: 0.5522\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3685 - val_loss: 0.5642\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3634 - val_loss: 0.5578\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3575 - val_loss: 0.5456\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3535 - val_loss: 0.5489\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3478 - val_loss: 0.5524\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3418 - val_loss: 0.5448\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3380 - val_loss: 0.5419\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3345 - val_loss: 0.5423\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3316 - val_loss: 0.5377\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3281 - val_loss: 0.5407\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 62us/sample - loss: 0.3250 - val_loss: 0.5388\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3218 - val_loss: 0.5413\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3187 - val_loss: 0.5311\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3170 - val_loss: 0.5270\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3136 - val_loss: 0.5315\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3130 - val_loss: 0.5407\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3097 - val_loss: 0.5351\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3075 - val_loss: 0.5409\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3052 - val_loss: 0.5274\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.3033 - val_loss: 0.5336\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3025 - val_loss: 0.5317\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3017 - val_loss: 0.5262\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3011 - val_loss: 0.5393\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2993 - val_loss: 0.5336\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2966 - val_loss: 0.5411\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2961 - val_loss: 0.5305\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2950 - val_loss: 0.5319\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2941 - val_loss: 0.5262\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2931 - val_loss: 0.5281\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2922 - val_loss: 0.5318\n","3040/3040 [==============================] - 0s 85us/sample - loss: 0.5450\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.5877 - val_loss: 0.6015\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.4717 - val_loss: 0.6122\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.4540 - val_loss: 0.5844\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.4378 - val_loss: 0.5932\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.4264 - val_loss: 0.5946\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.4159 - val_loss: 0.5839\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.4069 - val_loss: 0.5674\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3955 - val_loss: 0.5779\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3881 - val_loss: 0.5659\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3810 - val_loss: 0.5649\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3755 - val_loss: 0.5657\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3703 - val_loss: 0.5742\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3649 - val_loss: 0.5655\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3597 - val_loss: 0.5583\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3551 - val_loss: 0.5577\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3513 - val_loss: 0.5635\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3472 - val_loss: 0.5517\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3434 - val_loss: 0.5545\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3398 - val_loss: 0.5686\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3368 - val_loss: 0.5762\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3337 - val_loss: 0.5590\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3303 - val_loss: 0.5737\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3278 - val_loss: 0.5669\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3252 - val_loss: 0.5760\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.3227 - val_loss: 0.5729\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3207 - val_loss: 0.5565\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.3192 - val_loss: 0.5654\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3157 - val_loss: 0.5821\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3140 - val_loss: 0.5767\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3127 - val_loss: 0.5940\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3118 - val_loss: 0.5678\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3092 - val_loss: 0.5987\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3084 - val_loss: 0.5737\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3089 - val_loss: 0.5700\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3054 - val_loss: 0.5607\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3040 - val_loss: 0.5664\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3024 - val_loss: 0.5620\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 62us/sample - loss: 0.3010 - val_loss: 0.5858\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 62us/sample - loss: 0.3000 - val_loss: 0.5685\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2993 - val_loss: 0.5705\n","3040/3040 [==============================] - 0s 86us/sample - loss: 0.5613\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.5148 - val_loss: 0.7465\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3600 - val_loss: 0.7190\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3212 - val_loss: 0.7187\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3069 - val_loss: 0.7062\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2966 - val_loss: 0.7027\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2884 - val_loss: 0.6952\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2775 - val_loss: 0.6929\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2690 - val_loss: 0.6957\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2615 - val_loss: 0.6870\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2560 - val_loss: 0.6899\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2484 - val_loss: 0.6896\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.2412 - val_loss: 0.6811\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.2368 - val_loss: 0.6801\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.2330 - val_loss: 0.6722\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2288 - val_loss: 0.6697\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2256 - val_loss: 0.6693\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2219 - val_loss: 0.6707\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2209 - val_loss: 0.6656\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2167 - val_loss: 0.6670\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2138 - val_loss: 0.6637\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2120 - val_loss: 0.6619\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2120 - val_loss: 0.6601\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2087 - val_loss: 0.6590\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2061 - val_loss: 0.6569\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2057 - val_loss: 0.6600\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2041 - val_loss: 0.6562\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.2028 - val_loss: 0.6561\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2017 - val_loss: 0.6540\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1999 - val_loss: 0.6546\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1975 - val_loss: 0.6550\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1972 - val_loss: 0.6517\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1969 - val_loss: 0.6545\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1950 - val_loss: 0.6524\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.1955 - val_loss: 0.6520\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.1935 - val_loss: 0.6546\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1934 - val_loss: 0.6560\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1917 - val_loss: 0.6567\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1909 - val_loss: 0.6549\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1900 - val_loss: 0.6537\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.1894 - val_loss: 0.6573\n","3040/3040 [==============================] - 0s 91us/sample - loss: 0.5785\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.5631 - val_loss: 0.5839\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.4278 - val_loss: 0.5559\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3976 - val_loss: 0.5778\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3793 - val_loss: 0.5520\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.3690 - val_loss: 0.5650\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3602 - val_loss: 0.5451\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3521 - val_loss: 0.5724\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3443 - val_loss: 0.5458\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3365 - val_loss: 0.5619\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.3263 - val_loss: 0.5404\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3190 - val_loss: 0.5458\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3122 - val_loss: 0.5461\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3058 - val_loss: 0.5470\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3004 - val_loss: 0.5473\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2973 - val_loss: 0.5397\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2926 - val_loss: 0.5408\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2884 - val_loss: 0.5417\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2847 - val_loss: 0.5395\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2812 - val_loss: 0.5286\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2774 - val_loss: 0.5357\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2743 - val_loss: 0.5341\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2721 - val_loss: 0.5242\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2698 - val_loss: 0.5513\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2671 - val_loss: 0.5236\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2654 - val_loss: 0.5351\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2628 - val_loss: 0.5278\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.2601 - val_loss: 0.5302\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2585 - val_loss: 0.5457\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2563 - val_loss: 0.5445\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2547 - val_loss: 0.5441\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2530 - val_loss: 0.5259\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2519 - val_loss: 0.5457\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2507 - val_loss: 0.5409\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2491 - val_loss: 0.5277\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2485 - val_loss: 0.5528\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2477 - val_loss: 0.5332\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2448 - val_loss: 0.5349\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2437 - val_loss: 0.5366\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2420 - val_loss: 0.5395\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2408 - val_loss: 0.5432\n","3040/3040 [==============================] - 0s 88us/sample - loss: 0.4752\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.5710 - val_loss: 0.4505\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.4357 - val_loss: 0.4207\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.4025 - val_loss: 0.3999\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3847 - val_loss: 0.3917\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3730 - val_loss: 0.3835\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3623 - val_loss: 0.3737\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3515 - val_loss: 0.3710\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3416 - val_loss: 0.3636\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3327 - val_loss: 0.3588\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3234 - val_loss: 0.3549\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3168 - val_loss: 0.3504\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3099 - val_loss: 0.3506\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3034 - val_loss: 0.3448\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2976 - val_loss: 0.3448\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2928 - val_loss: 0.3370\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.2883 - val_loss: 0.3371\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2845 - val_loss: 0.3334\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2818 - val_loss: 0.3315\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2797 - val_loss: 0.3353\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2766 - val_loss: 0.3293\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2731 - val_loss: 0.3297\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2699 - val_loss: 0.3266\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2671 - val_loss: 0.3247\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2647 - val_loss: 0.3235\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2628 - val_loss: 0.3241\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2607 - val_loss: 0.3206\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2584 - val_loss: 0.3237\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2561 - val_loss: 0.3230\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2550 - val_loss: 0.3249\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2545 - val_loss: 0.3199\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2529 - val_loss: 0.3212\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2513 - val_loss: 0.3272\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2525 - val_loss: 0.3219\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2491 - val_loss: 0.3160\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2459 - val_loss: 0.3182\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2441 - val_loss: 0.3188\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2430 - val_loss: 0.3203\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2419 - val_loss: 0.3207\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2407 - val_loss: 0.3239\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2396 - val_loss: 0.3186\n","3040/3040 [==============================] - 0s 84us/sample - loss: 0.4834\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.5501 - val_loss: 0.5905\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.4186 - val_loss: 0.5674\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3901 - val_loss: 0.5412\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3730 - val_loss: 0.5398\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 62us/sample - loss: 0.3631 - val_loss: 0.5398\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3557 - val_loss: 0.5247\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 62us/sample - loss: 0.3489 - val_loss: 0.5192\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3429 - val_loss: 0.5117\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3345 - val_loss: 0.5232\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3269 - val_loss: 0.5251\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3198 - val_loss: 0.5176\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.3138 - val_loss: 0.5114\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3081 - val_loss: 0.5168\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3025 - val_loss: 0.5042\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2988 - val_loss: 0.5122\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2941 - val_loss: 0.4948\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2889 - val_loss: 0.4939\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2853 - val_loss: 0.4899\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2810 - val_loss: 0.4863\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2786 - val_loss: 0.4862\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2753 - val_loss: 0.4862\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2727 - val_loss: 0.4719\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2701 - val_loss: 0.4708\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2670 - val_loss: 0.4665\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2653 - val_loss: 0.4674\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2640 - val_loss: 0.4686\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2620 - val_loss: 0.4668\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2591 - val_loss: 0.4652\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2570 - val_loss: 0.4645\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2557 - val_loss: 0.4575\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2539 - val_loss: 0.4610\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2522 - val_loss: 0.4596\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2513 - val_loss: 0.4645\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2489 - val_loss: 0.4560\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2476 - val_loss: 0.4572\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2467 - val_loss: 0.4573\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2444 - val_loss: 0.4614\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2436 - val_loss: 0.4600\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2428 - val_loss: 0.4601\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2417 - val_loss: 0.4534\n","3040/3040 [==============================] - 0s 89us/sample - loss: 0.4651\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.5485 - val_loss: 0.5737\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.4211 - val_loss: 0.5499\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3935 - val_loss: 0.5356\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3771 - val_loss: 0.5314\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3665 - val_loss: 0.5163\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3570 - val_loss: 0.5053\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3503 - val_loss: 0.5120\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3422 - val_loss: 0.5071\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3348 - val_loss: 0.5148\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3282 - val_loss: 0.5050\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.3220 - val_loss: 0.5120\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3157 - val_loss: 0.5032\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.3088 - val_loss: 0.5022\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3043 - val_loss: 0.5177\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2991 - val_loss: 0.4946\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2947 - val_loss: 0.4916\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2905 - val_loss: 0.4969\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 62us/sample - loss: 0.2866 - val_loss: 0.5004\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2831 - val_loss: 0.5010\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2802 - val_loss: 0.4929\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2770 - val_loss: 0.4982\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2748 - val_loss: 0.5010\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2742 - val_loss: 0.4954\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2709 - val_loss: 0.4941\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2681 - val_loss: 0.5010\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2662 - val_loss: 0.4902\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2640 - val_loss: 0.4927\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2633 - val_loss: 0.4970\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2605 - val_loss: 0.4856\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2594 - val_loss: 0.5007\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2575 - val_loss: 0.4956\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2565 - val_loss: 0.5052\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2558 - val_loss: 0.4953\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2544 - val_loss: 0.4941\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2516 - val_loss: 0.5017\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2505 - val_loss: 0.4932\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2493 - val_loss: 0.4992\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2480 - val_loss: 0.5054\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2463 - val_loss: 0.4873\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2468 - val_loss: 0.5036\n","3040/3040 [==============================] - 0s 87us/sample - loss: 0.4791\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 2s 206us/sample - loss: 0.4928 - val_loss: 0.7381\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3349 - val_loss: 0.7082\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3033 - val_loss: 0.6928\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2847 - val_loss: 0.6760\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2732 - val_loss: 0.6689\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2651 - val_loss: 0.6604\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2554 - val_loss: 0.6530\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2483 - val_loss: 0.6472\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2420 - val_loss: 0.6446\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2349 - val_loss: 0.6368\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2294 - val_loss: 0.6331\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2240 - val_loss: 0.6285\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2187 - val_loss: 0.6268\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2146 - val_loss: 0.6211\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2117 - val_loss: 0.6186\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2087 - val_loss: 0.6171\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2053 - val_loss: 0.6130\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2019 - val_loss: 0.6081\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1999 - val_loss: 0.6067\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1976 - val_loss: 0.6086\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1962 - val_loss: 0.6031\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1944 - val_loss: 0.6020\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1921 - val_loss: 0.6000\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1893 - val_loss: 0.5981\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1872 - val_loss: 0.5983\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1854 - val_loss: 0.5974\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1843 - val_loss: 0.5990\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1829 - val_loss: 0.5954\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1822 - val_loss: 0.5961\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1807 - val_loss: 0.5982\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1802 - val_loss: 0.5960\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1790 - val_loss: 0.5940\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1792 - val_loss: 0.5949\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1777 - val_loss: 0.5957\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1763 - val_loss: 0.5950\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1745 - val_loss: 0.5957\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1736 - val_loss: 0.5953\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1727 - val_loss: 0.5943\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1725 - val_loss: 0.5966\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1709 - val_loss: 0.5954\n","3040/3040 [==============================] - 0s 89us/sample - loss: 0.5204\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.5497 - val_loss: 0.5814\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3984 - val_loss: 0.5333\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.3709 - val_loss: 0.5347\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3513 - val_loss: 0.5143\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3392 - val_loss: 0.5045\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3295 - val_loss: 0.5071\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3194 - val_loss: 0.4822\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3127 - val_loss: 0.4866\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3050 - val_loss: 0.4720\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2974 - val_loss: 0.4819\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2912 - val_loss: 0.4757\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2843 - val_loss: 0.4659\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2791 - val_loss: 0.4710\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2744 - val_loss: 0.4579\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2698 - val_loss: 0.4817\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2658 - val_loss: 0.4663\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2605 - val_loss: 0.4559\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2583 - val_loss: 0.4580\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2551 - val_loss: 0.4608\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2513 - val_loss: 0.4556\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2488 - val_loss: 0.4596\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2471 - val_loss: 0.4559\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2436 - val_loss: 0.4787\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2419 - val_loss: 0.4633\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2391 - val_loss: 0.4620\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2371 - val_loss: 0.4716\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2350 - val_loss: 0.4613\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2339 - val_loss: 0.4562\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2334 - val_loss: 0.4647\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2318 - val_loss: 0.4648\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2293 - val_loss: 0.4655\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2275 - val_loss: 0.4718\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2264 - val_loss: 0.4593\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2255 - val_loss: 0.4651\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.2231 - val_loss: 0.4623\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2218 - val_loss: 0.4713\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2209 - val_loss: 0.4686\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2200 - val_loss: 0.4734\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2191 - val_loss: 0.4708\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2172 - val_loss: 0.4788\n","3040/3040 [==============================] - 0s 88us/sample - loss: 0.4197\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.5613 - val_loss: 0.4385\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.4079 - val_loss: 0.3852\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3743 - val_loss: 0.3708\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3561 - val_loss: 0.3624\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3441 - val_loss: 0.3516\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3329 - val_loss: 0.3405\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3244 - val_loss: 0.3416\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.3158 - val_loss: 0.3304\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3085 - val_loss: 0.3293\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3031 - val_loss: 0.3231\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2950 - val_loss: 0.3224\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2900 - val_loss: 0.3147\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2845 - val_loss: 0.3157\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2791 - val_loss: 0.3116\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2726 - val_loss: 0.3046\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2678 - val_loss: 0.3030\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2641 - val_loss: 0.3006\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2612 - val_loss: 0.2980\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2579 - val_loss: 0.2964\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2552 - val_loss: 0.2953\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2516 - val_loss: 0.2942\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2495 - val_loss: 0.2918\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2472 - val_loss: 0.2896\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2440 - val_loss: 0.2878\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2414 - val_loss: 0.2880\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2404 - val_loss: 0.2889\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2374 - val_loss: 0.2884\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2352 - val_loss: 0.2868\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2332 - val_loss: 0.2861\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2312 - val_loss: 0.2852\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2304 - val_loss: 0.2861\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2285 - val_loss: 0.2848\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2267 - val_loss: 0.2866\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2252 - val_loss: 0.2845\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2237 - val_loss: 0.2852\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2227 - val_loss: 0.2848\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2227 - val_loss: 0.2853\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2217 - val_loss: 0.2851\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2188 - val_loss: 0.2833\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2179 - val_loss: 0.2851\n","3040/3040 [==============================] - 0s 94us/sample - loss: 0.4378\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.5354 - val_loss: 0.5614\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3980 - val_loss: 0.5326\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3662 - val_loss: 0.5193\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3493 - val_loss: 0.4979\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3368 - val_loss: 0.4912\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3282 - val_loss: 0.4835\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3197 - val_loss: 0.4862\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3128 - val_loss: 0.4676\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3049 - val_loss: 0.4611\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2990 - val_loss: 0.4561\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2924 - val_loss: 0.4527\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2860 - val_loss: 0.4488\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2809 - val_loss: 0.4350\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2751 - val_loss: 0.4362\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2710 - val_loss: 0.4319\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2674 - val_loss: 0.4263\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2623 - val_loss: 0.4290\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2603 - val_loss: 0.4150\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2553 - val_loss: 0.4147\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2528 - val_loss: 0.4073\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2494 - val_loss: 0.4164\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2472 - val_loss: 0.4069\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2464 - val_loss: 0.4100\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2431 - val_loss: 0.4074\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2400 - val_loss: 0.4039\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2374 - val_loss: 0.4024\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2356 - val_loss: 0.4004\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2339 - val_loss: 0.4057\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2316 - val_loss: 0.3999\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2302 - val_loss: 0.3984\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2294 - val_loss: 0.3986\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 62us/sample - loss: 0.2291 - val_loss: 0.3971\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2269 - val_loss: 0.3984\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2256 - val_loss: 0.4006\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2250 - val_loss: 0.3993\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2223 - val_loss: 0.4055\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2206 - val_loss: 0.3981\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2198 - val_loss: 0.3990\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2180 - val_loss: 0.3974\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2173 - val_loss: 0.3976\n","3040/3040 [==============================] - 0s 89us/sample - loss: 0.4277\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.5482 - val_loss: 0.5459\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.4031 - val_loss: 0.5335\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3715 - val_loss: 0.5201\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3530 - val_loss: 0.5136\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3409 - val_loss: 0.5082\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3302 - val_loss: 0.4883\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3223 - val_loss: 0.4798\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3144 - val_loss: 0.4748\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3081 - val_loss: 0.4777\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.3010 - val_loss: 0.4707\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2965 - val_loss: 0.4793\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2901 - val_loss: 0.4738\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 70us/sample - loss: 0.2849 - val_loss: 0.4819\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2801 - val_loss: 0.4780\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2750 - val_loss: 0.4725\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2711 - val_loss: 0.4761\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2677 - val_loss: 0.4722\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 70us/sample - loss: 0.2643 - val_loss: 0.4734\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2606 - val_loss: 0.4748\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2575 - val_loss: 0.4607\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2552 - val_loss: 0.4677\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2517 - val_loss: 0.4631\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2491 - val_loss: 0.4752\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2472 - val_loss: 0.4745\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2446 - val_loss: 0.4737\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2422 - val_loss: 0.4688\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2406 - val_loss: 0.4640\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2391 - val_loss: 0.4720\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2374 - val_loss: 0.4659\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2360 - val_loss: 0.4612\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2347 - val_loss: 0.4758\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2334 - val_loss: 0.4704\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2313 - val_loss: 0.4709\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2306 - val_loss: 0.4601\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2288 - val_loss: 0.4820\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2274 - val_loss: 0.4761\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2271 - val_loss: 0.4707\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2249 - val_loss: 0.4692\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2235 - val_loss: 0.4667\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2230 - val_loss: 0.4716\n","3040/3040 [==============================] - 0s 91us/sample - loss: 0.4380\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 2s 185us/sample - loss: 0.4656 - val_loss: 0.7226\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3204 - val_loss: 0.6870\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2905 - val_loss: 0.6670\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2740 - val_loss: 0.6569\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2604 - val_loss: 0.6462\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2514 - val_loss: 0.6349\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2431 - val_loss: 0.6253\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2360 - val_loss: 0.6159\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2314 - val_loss: 0.6083\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2240 - val_loss: 0.5996\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2178 - val_loss: 0.5969\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2129 - val_loss: 0.5907\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2089 - val_loss: 0.5821\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2043 - val_loss: 0.5851\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2012 - val_loss: 0.5760\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1966 - val_loss: 0.5734\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1938 - val_loss: 0.5695\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1909 - val_loss: 0.5713\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1884 - val_loss: 0.5637\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1868 - val_loss: 0.5623\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1850 - val_loss: 0.5624\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1820 - val_loss: 0.5623\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1803 - val_loss: 0.5579\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1778 - val_loss: 0.5592\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1784 - val_loss: 0.5579\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1758 - val_loss: 0.5573\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1737 - val_loss: 0.5527\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1719 - val_loss: 0.5533\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1708 - val_loss: 0.5527\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1708 - val_loss: 0.5558\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1684 - val_loss: 0.5518\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1667 - val_loss: 0.5526\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1664 - val_loss: 0.5513\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1651 - val_loss: 0.5514\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1644 - val_loss: 0.5515\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1637 - val_loss: 0.5510\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1622 - val_loss: 0.5490\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1616 - val_loss: 0.5505\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1608 - val_loss: 0.5493\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1593 - val_loss: 0.5494\n","3040/3040 [==============================] - 0s 89us/sample - loss: 0.4749\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.5251 - val_loss: 0.5697\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3828 - val_loss: 0.5242\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3510 - val_loss: 0.4999\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3343 - val_loss: 0.5141\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3221 - val_loss: 0.4918\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3104 - val_loss: 0.5024\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3017 - val_loss: 0.4926\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2954 - val_loss: 0.4748\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2879 - val_loss: 0.4673\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2832 - val_loss: 0.4681\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2757 - val_loss: 0.4708\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2700 - val_loss: 0.4636\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2657 - val_loss: 0.4800\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2609 - val_loss: 0.4575\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2569 - val_loss: 0.4481\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2522 - val_loss: 0.4572\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2479 - val_loss: 0.4587\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2440 - val_loss: 0.4504\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2407 - val_loss: 0.4495\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2397 - val_loss: 0.4640\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2360 - val_loss: 0.4545\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2331 - val_loss: 0.4480\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2306 - val_loss: 0.4359\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2282 - val_loss: 0.4349\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2266 - val_loss: 0.4391\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2253 - val_loss: 0.4431\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2233 - val_loss: 0.4421\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2208 - val_loss: 0.4365\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2204 - val_loss: 0.4298\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2176 - val_loss: 0.4378\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2155 - val_loss: 0.4357\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2137 - val_loss: 0.4351\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2125 - val_loss: 0.4315\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2128 - val_loss: 0.4336\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2103 - val_loss: 0.4414\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2087 - val_loss: 0.4338\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2079 - val_loss: 0.4453\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2066 - val_loss: 0.4407\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2060 - val_loss: 0.4400\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2047 - val_loss: 0.4453\n","3040/3040 [==============================] - 0s 88us/sample - loss: 0.3938\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.5386 - val_loss: 0.3969\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3867 - val_loss: 0.3681\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3580 - val_loss: 0.3606\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3403 - val_loss: 0.3508\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3271 - val_loss: 0.3399\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.3154 - val_loss: 0.3324\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3074 - val_loss: 0.3278\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2994 - val_loss: 0.3220\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2918 - val_loss: 0.3153\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2845 - val_loss: 0.3090\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.2781 - val_loss: 0.3051\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2736 - val_loss: 0.2989\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2672 - val_loss: 0.2989\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2624 - val_loss: 0.2903\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 62us/sample - loss: 0.2581 - val_loss: 0.2893\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2539 - val_loss: 0.2866\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2502 - val_loss: 0.2841\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2455 - val_loss: 0.2868\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2424 - val_loss: 0.2814\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2403 - val_loss: 0.2802\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2362 - val_loss: 0.2803\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2338 - val_loss: 0.2796\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2316 - val_loss: 0.2752\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2303 - val_loss: 0.2711\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2274 - val_loss: 0.2719\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2253 - val_loss: 0.2697\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2219 - val_loss: 0.2690\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2198 - val_loss: 0.2669\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2182 - val_loss: 0.2691\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2185 - val_loss: 0.2687\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2163 - val_loss: 0.2664\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2139 - val_loss: 0.2650\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2123 - val_loss: 0.2649\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2121 - val_loss: 0.2762\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2120 - val_loss: 0.2674\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2099 - val_loss: 0.2638\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2073 - val_loss: 0.2636\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2057 - val_loss: 0.2652\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2061 - val_loss: 0.2636\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.2043 - val_loss: 0.2634\n","3040/3040 [==============================] - 0s 90us/sample - loss: 0.4083\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.5136 - val_loss: 0.5402\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.3803 - val_loss: 0.5156\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 71us/sample - loss: 0.3505 - val_loss: 0.5110\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.3322 - val_loss: 0.4736\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3191 - val_loss: 0.4730\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3104 - val_loss: 0.4567\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.3014 - val_loss: 0.4604\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2936 - val_loss: 0.4459\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2868 - val_loss: 0.4389\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2811 - val_loss: 0.4343\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2756 - val_loss: 0.4287\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2705 - val_loss: 0.4223\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2650 - val_loss: 0.4195\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2605 - val_loss: 0.4140\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2562 - val_loss: 0.4075\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2514 - val_loss: 0.4053\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2477 - val_loss: 0.4027\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2449 - val_loss: 0.3998\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2425 - val_loss: 0.3929\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2379 - val_loss: 0.3939\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2361 - val_loss: 0.3901\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2324 - val_loss: 0.3924\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2302 - val_loss: 0.3844\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2294 - val_loss: 0.3897\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2268 - val_loss: 0.3831\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2241 - val_loss: 0.3860\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2236 - val_loss: 0.3892\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2210 - val_loss: 0.3805\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2185 - val_loss: 0.3835\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2181 - val_loss: 0.3852\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2158 - val_loss: 0.3763\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2140 - val_loss: 0.3751\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2130 - val_loss: 0.3770\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2109 - val_loss: 0.3749\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2092 - val_loss: 0.3780\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2088 - val_loss: 0.3759\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2065 - val_loss: 0.3740\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2053 - val_loss: 0.3708\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2042 - val_loss: 0.3785\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2063 - val_loss: 0.3823\n","3040/3040 [==============================] - 0s 100us/sample - loss: 0.3956\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.5167 - val_loss: 0.5406\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3793 - val_loss: 0.4843\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.3502 - val_loss: 0.4938\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.3353 - val_loss: 0.4782\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3201 - val_loss: 0.4731\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3092 - val_loss: 0.4579\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3016 - val_loss: 0.4584\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2939 - val_loss: 0.4665\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2886 - val_loss: 0.4514\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2832 - val_loss: 0.4496\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2780 - val_loss: 0.4542\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2732 - val_loss: 0.4479\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2682 - val_loss: 0.4429\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2639 - val_loss: 0.4507\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2603 - val_loss: 0.4459\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2558 - val_loss: 0.4384\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2530 - val_loss: 0.4383\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2498 - val_loss: 0.4534\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2466 - val_loss: 0.4403\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2427 - val_loss: 0.4322\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2416 - val_loss: 0.4388\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 62us/sample - loss: 0.2382 - val_loss: 0.4441\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2356 - val_loss: 0.4274\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2328 - val_loss: 0.4320\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2302 - val_loss: 0.4195\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2283 - val_loss: 0.4256\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2273 - val_loss: 0.4287\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2275 - val_loss: 0.4323\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2238 - val_loss: 0.4242\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2207 - val_loss: 0.4348\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2202 - val_loss: 0.4209\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2193 - val_loss: 0.4150\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2172 - val_loss: 0.4274\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2154 - val_loss: 0.4218\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2157 - val_loss: 0.4277\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2146 - val_loss: 0.4301\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2118 - val_loss: 0.4303\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2101 - val_loss: 0.4343\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2093 - val_loss: 0.4428\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2088 - val_loss: 0.4294\n","3040/3040 [==============================] - 0s 89us/sample - loss: 0.4070\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 2s 190us/sample - loss: 0.4658 - val_loss: 0.7114\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.3114 - val_loss: 0.6737\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2825 - val_loss: 0.6556\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2628 - val_loss: 0.6361\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2493 - val_loss: 0.6258\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2409 - val_loss: 0.6155\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2309 - val_loss: 0.6003\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2238 - val_loss: 0.5927\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2184 - val_loss: 0.5802\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2123 - val_loss: 0.5714\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2063 - val_loss: 0.5660\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2009 - val_loss: 0.5582\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1977 - val_loss: 0.5530\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1946 - val_loss: 0.5445\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1894 - val_loss: 0.5420\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1880 - val_loss: 0.5372\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.1824 - val_loss: 0.5333\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1799 - val_loss: 0.5268\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1764 - val_loss: 0.5229\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1755 - val_loss: 0.5258\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1728 - val_loss: 0.5159\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1694 - val_loss: 0.5150\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 70us/sample - loss: 0.1676 - val_loss: 0.5117\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1653 - val_loss: 0.5102\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1632 - val_loss: 0.5058\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1622 - val_loss: 0.5055\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1598 - val_loss: 0.5035\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1601 - val_loss: 0.5060\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.1621 - val_loss: 0.5031\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1569 - val_loss: 0.5015\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1545 - val_loss: 0.4964\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1536 - val_loss: 0.5011\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1532 - val_loss: 0.4986\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1528 - val_loss: 0.5003\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1509 - val_loss: 0.4979\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1494 - val_loss: 0.5002\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 70us/sample - loss: 0.1502 - val_loss: 0.4959\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1465 - val_loss: 0.4952\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1460 - val_loss: 0.4949\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1455 - val_loss: 0.4944\n","3040/3040 [==============================] - 0s 97us/sample - loss: 0.4247\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.5172 - val_loss: 0.5190\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3661 - val_loss: 0.4827\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3344 - val_loss: 0.4823\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3153 - val_loss: 0.4650\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3014 - val_loss: 0.4459\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2907 - val_loss: 0.4405\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2818 - val_loss: 0.4360\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2734 - val_loss: 0.4321\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2674 - val_loss: 0.4196\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2617 - val_loss: 0.4139\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2530 - val_loss: 0.4252\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2489 - val_loss: 0.4039\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2444 - val_loss: 0.4051\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2403 - val_loss: 0.3956\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2360 - val_loss: 0.3930\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2328 - val_loss: 0.3924\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2284 - val_loss: 0.3872\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2258 - val_loss: 0.3876\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2222 - val_loss: 0.3844\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2197 - val_loss: 0.3793\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2165 - val_loss: 0.3915\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2125 - val_loss: 0.3782\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2119 - val_loss: 0.3786\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2093 - val_loss: 0.3830\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2066 - val_loss: 0.3778\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2051 - val_loss: 0.3788\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2021 - val_loss: 0.3862\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2000 - val_loss: 0.3792\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1993 - val_loss: 0.3769\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1978 - val_loss: 0.3770\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1960 - val_loss: 0.3806\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1992 - val_loss: 0.3824\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1943 - val_loss: 0.3828\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1927 - val_loss: 0.3854\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1915 - val_loss: 0.3793\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1899 - val_loss: 0.3775\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1883 - val_loss: 0.3842\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1872 - val_loss: 0.3814\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1857 - val_loss: 0.3797\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1843 - val_loss: 0.3864\n","3040/3040 [==============================] - 0s 90us/sample - loss: 0.3488\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.5298 - val_loss: 0.3892\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.3733 - val_loss: 0.3619\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3398 - val_loss: 0.3432\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3198 - val_loss: 0.3264\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3056 - val_loss: 0.3144\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2945 - val_loss: 0.3089\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.2848 - val_loss: 0.3032\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 70us/sample - loss: 0.2781 - val_loss: 0.2956\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2697 - val_loss: 0.2881\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2640 - val_loss: 0.2832\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2589 - val_loss: 0.2841\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2553 - val_loss: 0.2771\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2477 - val_loss: 0.2756\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2439 - val_loss: 0.2707\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2409 - val_loss: 0.2679\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2348 - val_loss: 0.2604\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2322 - val_loss: 0.2601\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2280 - val_loss: 0.2601\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2239 - val_loss: 0.2546\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2217 - val_loss: 0.2534\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2183 - val_loss: 0.2513\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2157 - val_loss: 0.2478\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2135 - val_loss: 0.2477\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2109 - val_loss: 0.2497\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2108 - val_loss: 0.2447\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2069 - val_loss: 0.2426\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2044 - val_loss: 0.2455\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2035 - val_loss: 0.2405\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2013 - val_loss: 0.2423\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1995 - val_loss: 0.2368\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1975 - val_loss: 0.2361\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1950 - val_loss: 0.2371\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1940 - val_loss: 0.2347\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1925 - val_loss: 0.2357\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1919 - val_loss: 0.2350\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1916 - val_loss: 0.2328\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1905 - val_loss: 0.2334\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1905 - val_loss: 0.2344\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1863 - val_loss: 0.2400\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1871 - val_loss: 0.2350\n","3040/3040 [==============================] - 0s 97us/sample - loss: 0.3630\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 83us/sample - loss: 0.5101 - val_loss: 0.5465\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3664 - val_loss: 0.5114\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3352 - val_loss: 0.4755\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.3175 - val_loss: 0.4655\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3034 - val_loss: 0.4415\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2926 - val_loss: 0.4379\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2830 - val_loss: 0.4221\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2758 - val_loss: 0.4150\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2683 - val_loss: 0.4033\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2612 - val_loss: 0.4041\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2562 - val_loss: 0.3925\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2501 - val_loss: 0.3808\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2452 - val_loss: 0.3801\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2406 - val_loss: 0.3732\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2355 - val_loss: 0.3709\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2320 - val_loss: 0.3636\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2275 - val_loss: 0.3627\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2235 - val_loss: 0.3572\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2213 - val_loss: 0.3521\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2191 - val_loss: 0.3465\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2157 - val_loss: 0.3451\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2126 - val_loss: 0.3481\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2116 - val_loss: 0.3427\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2091 - val_loss: 0.3384\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2071 - val_loss: 0.3384\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2056 - val_loss: 0.3351\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2039 - val_loss: 0.3331\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1995 - val_loss: 0.3393\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.1987 - val_loss: 0.3374\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.1973 - val_loss: 0.3440\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.1966 - val_loss: 0.3298\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1942 - val_loss: 0.3347\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1914 - val_loss: 0.3305\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.1912 - val_loss: 0.3404\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.1905 - val_loss: 0.3347\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.1885 - val_loss: 0.3290\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.1897 - val_loss: 0.3320\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.1873 - val_loss: 0.3246\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.1856 - val_loss: 0.3299\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.1843 - val_loss: 0.3277\n","3040/3040 [==============================] - 0s 87us/sample - loss: 0.3484\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.5108 - val_loss: 0.4998\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3692 - val_loss: 0.4738\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3356 - val_loss: 0.4621\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.3174 - val_loss: 0.4614\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3042 - val_loss: 0.4427\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2923 - val_loss: 0.4286\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2829 - val_loss: 0.4198\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2746 - val_loss: 0.4209\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2677 - val_loss: 0.4118\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2629 - val_loss: 0.4113\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2590 - val_loss: 0.4070\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2512 - val_loss: 0.3997\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2460 - val_loss: 0.3894\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2417 - val_loss: 0.4039\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2388 - val_loss: 0.3909\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2340 - val_loss: 0.3807\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2310 - val_loss: 0.3788\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2266 - val_loss: 0.3830\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2252 - val_loss: 0.3841\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2213 - val_loss: 0.3740\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2193 - val_loss: 0.3806\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2163 - val_loss: 0.3804\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2135 - val_loss: 0.3850\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2103 - val_loss: 0.3756\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2089 - val_loss: 0.3860\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2071 - val_loss: 0.3879\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2058 - val_loss: 0.3714\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2041 - val_loss: 0.3744\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.2017 - val_loss: 0.3700\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2019 - val_loss: 0.3744\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.1979 - val_loss: 0.3770\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1968 - val_loss: 0.3724\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.1971 - val_loss: 0.3756\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.1956 - val_loss: 0.3705\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.1938 - val_loss: 0.3704\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1936 - val_loss: 0.3697\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 63us/sample - loss: 0.1908 - val_loss: 0.3653\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.1898 - val_loss: 0.3741\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.1891 - val_loss: 0.3811\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1882 - val_loss: 0.3676\n","3040/3040 [==============================] - 0s 91us/sample - loss: 0.3585\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 2s 197us/sample - loss: 0.4534 - val_loss: 0.7022\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.3069 - val_loss: 0.6692\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2747 - val_loss: 0.6465\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2555 - val_loss: 0.6289\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2425 - val_loss: 0.6097\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2315 - val_loss: 0.5962\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2226 - val_loss: 0.5830\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2160 - val_loss: 0.5726\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2096 - val_loss: 0.5653\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2049 - val_loss: 0.5566\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1997 - val_loss: 0.5532\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1951 - val_loss: 0.5433\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1920 - val_loss: 0.5365\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1865 - val_loss: 0.5312\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1833 - val_loss: 0.5269\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1788 - val_loss: 0.5200\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1761 - val_loss: 0.5158\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1727 - val_loss: 0.5124\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1694 - val_loss: 0.5086\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1682 - val_loss: 0.5039\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1651 - val_loss: 0.4995\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1627 - val_loss: 0.4914\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1598 - val_loss: 0.4928\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1576 - val_loss: 0.4865\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1554 - val_loss: 0.4869\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1567 - val_loss: 0.4866\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1535 - val_loss: 0.4817\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1507 - val_loss: 0.4787\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1494 - val_loss: 0.4766\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1474 - val_loss: 0.4752\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1481 - val_loss: 0.4750\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.1474 - val_loss: 0.4732\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.1449 - val_loss: 0.4707\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.1424 - val_loss: 0.4687\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.1431 - val_loss: 0.4699\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 70us/sample - loss: 0.1413 - val_loss: 0.4691\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1447 - val_loss: 0.4683\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1393 - val_loss: 0.4641\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1380 - val_loss: 0.4691\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1367 - val_loss: 0.4637\n","3040/3040 [==============================] - 0s 97us/sample - loss: 0.4010\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.5002 - val_loss: 0.5344\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3580 - val_loss: 0.4745\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3234 - val_loss: 0.4619\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3032 - val_loss: 0.4473\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2886 - val_loss: 0.4341\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2770 - val_loss: 0.4214\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2687 - val_loss: 0.4251\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2617 - val_loss: 0.4131\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2543 - val_loss: 0.4004\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2469 - val_loss: 0.4015\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2421 - val_loss: 0.4062\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2380 - val_loss: 0.3847\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2315 - val_loss: 0.3805\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2274 - val_loss: 0.3847\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 63us/sample - loss: 0.2238 - val_loss: 0.3734\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2212 - val_loss: 0.3738\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2176 - val_loss: 0.3798\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2156 - val_loss: 0.3712\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2094 - val_loss: 0.3690\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2068 - val_loss: 0.3747\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2045 - val_loss: 0.3662\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2012 - val_loss: 0.3705\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1993 - val_loss: 0.3607\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1974 - val_loss: 0.3634\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1998 - val_loss: 0.3607\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1948 - val_loss: 0.3699\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1901 - val_loss: 0.3581\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1883 - val_loss: 0.3609\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1873 - val_loss: 0.3625\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1874 - val_loss: 0.3619\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1845 - val_loss: 0.3589\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1854 - val_loss: 0.3621\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1825 - val_loss: 0.3613\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1805 - val_loss: 0.3600\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1773 - val_loss: 0.3561\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1772 - val_loss: 0.3540\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1767 - val_loss: 0.3546\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1735 - val_loss: 0.3523\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1738 - val_loss: 0.3552\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1735 - val_loss: 0.3595\n","3040/3040 [==============================] - 0s 90us/sample - loss: 0.3221\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.5187 - val_loss: 0.3835\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.3606 - val_loss: 0.3492\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.3263 - val_loss: 0.3314\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3080 - val_loss: 0.3178\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.2923 - val_loss: 0.3097\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2823 - val_loss: 0.2974\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2722 - val_loss: 0.2886\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2640 - val_loss: 0.2819\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2571 - val_loss: 0.2740\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2508 - val_loss: 0.2713\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2459 - val_loss: 0.2674\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2401 - val_loss: 0.2634\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2360 - val_loss: 0.2608\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2317 - val_loss: 0.2562\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2267 - val_loss: 0.2521\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2238 - val_loss: 0.2478\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2194 - val_loss: 0.2474\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2184 - val_loss: 0.2454\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2127 - val_loss: 0.2382\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2098 - val_loss: 0.2434\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.2088 - val_loss: 0.2393\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2050 - val_loss: 0.2339\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2019 - val_loss: 0.2325\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1988 - val_loss: 0.2304\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1976 - val_loss: 0.2292\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1950 - val_loss: 0.2274\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 64us/sample - loss: 0.1952 - val_loss: 0.2292\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1922 - val_loss: 0.2268\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1912 - val_loss: 0.2241\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1874 - val_loss: 0.2217\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1868 - val_loss: 0.2233\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1853 - val_loss: 0.2250\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1849 - val_loss: 0.2197\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1824 - val_loss: 0.2200\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1812 - val_loss: 0.2200\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1790 - val_loss: 0.2182\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1774 - val_loss: 0.2176\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1770 - val_loss: 0.2166\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1766 - val_loss: 0.2176\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1764 - val_loss: 0.2174\n","3040/3040 [==============================] - 0s 93us/sample - loss: 0.3381\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.4983 - val_loss: 0.5200\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3569 - val_loss: 0.4810\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.3232 - val_loss: 0.4592\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3047 - val_loss: 0.4410\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2892 - val_loss: 0.4260\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2777 - val_loss: 0.4140\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2679 - val_loss: 0.3994\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2596 - val_loss: 0.3876\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2547 - val_loss: 0.3773\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2474 - val_loss: 0.3676\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2418 - val_loss: 0.3614\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2372 - val_loss: 0.3644\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2328 - val_loss: 0.3589\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2280 - val_loss: 0.3458\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2251 - val_loss: 0.3498\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2204 - val_loss: 0.3384\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2162 - val_loss: 0.3382\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2131 - val_loss: 0.3402\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2103 - val_loss: 0.3334\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2058 - val_loss: 0.3335\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2059 - val_loss: 0.3424\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2038 - val_loss: 0.3289\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2003 - val_loss: 0.3244\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.1974 - val_loss: 0.3225\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.1928 - val_loss: 0.3267\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1934 - val_loss: 0.3238\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.1912 - val_loss: 0.3125\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.1880 - val_loss: 0.3180\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1870 - val_loss: 0.3124\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1849 - val_loss: 0.3107\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1844 - val_loss: 0.3114\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.1819 - val_loss: 0.3108\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.1809 - val_loss: 0.3066\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1775 - val_loss: 0.3033\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1790 - val_loss: 0.3022\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.1782 - val_loss: 0.3029\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.1778 - val_loss: 0.3068\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.1751 - val_loss: 0.3010\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1730 - val_loss: 0.3009\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1727 - val_loss: 0.3017\n","3040/3040 [==============================] - 0s 95us/sample - loss: 0.3245\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.5009 - val_loss: 0.5015\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.3549 - val_loss: 0.4465\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.3243 - val_loss: 0.4406\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.3048 - val_loss: 0.4180\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2904 - val_loss: 0.4104\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2792 - val_loss: 0.4024\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2707 - val_loss: 0.4047\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2619 - val_loss: 0.3828\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.2556 - val_loss: 0.3802\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2489 - val_loss: 0.3799\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2433 - val_loss: 0.3738\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2389 - val_loss: 0.3760\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 64us/sample - loss: 0.2336 - val_loss: 0.3641\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2295 - val_loss: 0.3683\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2269 - val_loss: 0.3695\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2220 - val_loss: 0.3610\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2182 - val_loss: 0.3569\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2155 - val_loss: 0.3646\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2126 - val_loss: 0.3675\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2102 - val_loss: 0.3643\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.2076 - val_loss: 0.3527\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.2037 - val_loss: 0.3481\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2018 - val_loss: 0.3497\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1993 - val_loss: 0.3446\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.1978 - val_loss: 0.3485\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.1960 - val_loss: 0.3420\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.1940 - val_loss: 0.3414\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.1909 - val_loss: 0.3428\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.1895 - val_loss: 0.3416\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.1884 - val_loss: 0.3434\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 70us/sample - loss: 0.1871 - val_loss: 0.3368\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.1852 - val_loss: 0.3423\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.1858 - val_loss: 0.3449\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1819 - val_loss: 0.3482\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1795 - val_loss: 0.3465\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.1786 - val_loss: 0.3440\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 65us/sample - loss: 0.1785 - val_loss: 0.3440\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1768 - val_loss: 0.3467\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.1759 - val_loss: 0.3334\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1746 - val_loss: 0.3391\n","3040/3040 [==============================] - 0s 93us/sample - loss: 0.3354\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tZK5JF-Ck7oE","colab_type":"code","outputId":"d4e05641-aee2-4f73-ba00-560cbfe7f51c","executionInfo":{"status":"ok","timestamp":1571350449115,"user_tz":300,"elapsed":1203,"user":{"displayName":"Brendan Sanderson","photoUrl":"","userId":"05405297474563600402"}},"colab":{"base_uri":"https://localhost:8080/","height":350}},"source":["plt.figure(figsize=(8, 5))\n","plt.plot(bs,testing_error)\n","plt.title(\"Keras Autoencoder error with different bottlenecks\")\n","plt.xlabel(\"Size of bottleneck\")\n","plt.ylabel(\"Testing loss\")\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAfsAAAFNCAYAAAAHGMa6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJXsREZGY+/8rZe7hlBP3oQAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 576x360 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"dunIMY9W1x_9","colab_type":"text"},"source":["The testing loss increases as the size of the bottleneck shrinks. This behavior makes sense given that the smaller the bottleneck, the less values the model compresses the data into. Evidently, if you are compressing data into a layer of size 10, there is no way that you can contain as much information as if you compress the data into a layer of size 50. Thus, the smaller the bottleneck, the larger the loss."]},{"cell_type":"markdown","metadata":{"id":"LexV2KA7dWHK","colab_type":"text"},"source":["### c. Experiment with changing the number of layers in the encoder and decoder. What happens if the depth is too small (i.e. 1 ?) what happens when the depth is too large (~20)?"]},{"cell_type":"markdown","metadata":{"id":"u323dxb28K95","colab_type":"text"},"source":["I am going to vary the number of layers between 1 and 19 in intervals of 3. I add layers in different sizes between the bottle neck and input size. I try to add mores than I believe would be more effective in size. FOrinstance, the first one I add is 256. I make sure to sort the layer sizes before I add them to the model."]},{"cell_type":"code","metadata":{"id":"wBoBw-2dr46H","colab_type":"code","colab":{}},"source":["def auto_encoding_model_layers(i,b,ls = 2): \n","  ds = sorted([256, 128, 512, 640, 64, 384, 196, 320, 448, 96, 576, 706, 80, 160, 112, 228, 288, 72][:ls-1])\n","  model = models.Sequential()\n","  model.add(layers.InputLayer(i,))\n","  for d in reversed(ds):\n","    model.add(layers.Dense(d, activation='tanh'))\n","  model.add(layers.Dense(b, activation='tanh'))\n","  for d in ds:\n","    model.add(layers.Dense(d, activation='tanh'))\n","  model.add(layers.Dense(i, activation='linear'))\n","  model.compile(loss='mean_squared_error', optimizer='adam')\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"If4LL8TqtXNe","colab_type":"code","outputId":"8179a122-d4f3-4351-8e96-e79c69d56fb6","executionInfo":{"status":"ok","timestamp":1571352706641,"user_tz":300,"elapsed":1336747,"user":{"displayName":"Brendan Sanderson","photoUrl":"","userId":"05405297474563600402"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["testing_errors = []\n","ls = list(range(1,19,3))\n","for l in ls:\n","  testing_errors.append(test_bottleneck(rnaseq, auto_encoding_model_layers(len(rnaseq.columns), 50, ls=l)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 2s 189us/sample - loss: 0.6348 - val_loss: 0.7688\n","Epoch 2/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.4095 - val_loss: 0.7013\n","Epoch 3/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.3393 - val_loss: 0.6717\n","Epoch 4/40\n","9726/9726 [==============================] - 0s 40us/sample - loss: 0.3058 - val_loss: 0.6501\n","Epoch 5/40\n","9726/9726 [==============================] - 0s 44us/sample - loss: 0.2849 - val_loss: 0.6351\n","Epoch 6/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2702 - val_loss: 0.6223\n","Epoch 7/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2590 - val_loss: 0.6130\n","Epoch 8/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2514 - val_loss: 0.6035\n","Epoch 9/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2436 - val_loss: 0.5963\n","Epoch 10/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.2377 - val_loss: 0.5920\n","Epoch 11/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2335 - val_loss: 0.5862\n","Epoch 12/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2292 - val_loss: 0.5819\n","Epoch 13/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.2263 - val_loss: 0.5768\n","Epoch 14/40\n","9726/9726 [==============================] - 0s 51us/sample - loss: 0.2233 - val_loss: 0.5739\n","Epoch 15/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.2205 - val_loss: 0.5698\n","Epoch 16/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2224 - val_loss: 0.5683\n","Epoch 17/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.2161 - val_loss: 0.5646\n","Epoch 18/40\n","9726/9726 [==============================] - 0s 40us/sample - loss: 0.2148 - val_loss: 0.5628\n","Epoch 19/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2126 - val_loss: 0.5619\n","Epoch 20/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.2125 - val_loss: 0.5562\n","Epoch 21/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2104 - val_loss: 0.5574\n","Epoch 22/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.2097 - val_loss: 0.5519\n","Epoch 23/40\n","9726/9726 [==============================] - 0s 40us/sample - loss: 0.2085 - val_loss: 0.5495\n","Epoch 24/40\n","9726/9726 [==============================] - 0s 44us/sample - loss: 0.2067 - val_loss: 0.5479\n","Epoch 25/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.2070 - val_loss: 0.5488\n","Epoch 26/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2055 - val_loss: 0.5444\n","Epoch 27/40\n","9726/9726 [==============================] - 0s 44us/sample - loss: 0.2054 - val_loss: 0.5430\n","Epoch 28/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2032 - val_loss: 0.5406\n","Epoch 29/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2026 - val_loss: 0.5388\n","Epoch 30/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2019 - val_loss: 0.5368\n","Epoch 31/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2028 - val_loss: 0.5365\n","Epoch 32/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.2005 - val_loss: 0.5348\n","Epoch 33/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2007 - val_loss: 0.5328\n","Epoch 34/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.2013 - val_loss: 0.5320\n","Epoch 35/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2016 - val_loss: 0.5305\n","Epoch 36/40\n","9726/9726 [==============================] - 0s 40us/sample - loss: 0.1985 - val_loss: 0.5298\n","Epoch 37/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.1973 - val_loss: 0.5276\n","Epoch 38/40\n","9726/9726 [==============================] - 0s 40us/sample - loss: 0.1982 - val_loss: 0.5280\n","Epoch 39/40\n","9726/9726 [==============================] - 0s 47us/sample - loss: 0.2040 - val_loss: 0.5251\n","Epoch 40/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.1962 - val_loss: 0.5249\n","3040/3040 [==============================] - 0s 78us/sample - loss: 0.4524\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.5655 - val_loss: 0.4967\n","Epoch 2/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.3880 - val_loss: 0.4704\n","Epoch 3/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.3533 - val_loss: 0.4541\n","Epoch 4/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.3356 - val_loss: 0.4449\n","Epoch 5/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.3240 - val_loss: 0.4360\n","Epoch 6/40\n","9726/9726 [==============================] - 0s 40us/sample - loss: 0.3156 - val_loss: 0.4293\n","Epoch 7/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.3087 - val_loss: 0.4241\n","Epoch 8/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.3036 - val_loss: 0.4205\n","Epoch 9/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2987 - val_loss: 0.4161\n","Epoch 10/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2946 - val_loss: 0.4142\n","Epoch 11/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2910 - val_loss: 0.4100\n","Epoch 12/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2878 - val_loss: 0.4094\n","Epoch 13/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.2854 - val_loss: 0.4045\n","Epoch 14/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2831 - val_loss: 0.4020\n","Epoch 15/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2800 - val_loss: 0.3988\n","Epoch 16/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2770 - val_loss: 0.3948\n","Epoch 17/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2749 - val_loss: 0.3921\n","Epoch 18/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2740 - val_loss: 0.3903\n","Epoch 19/40\n","9726/9726 [==============================] - 0s 44us/sample - loss: 0.2717 - val_loss: 0.3880\n","Epoch 20/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.2699 - val_loss: 0.3848\n","Epoch 21/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2672 - val_loss: 0.3824\n","Epoch 22/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.2662 - val_loss: 0.3801\n","Epoch 23/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2665 - val_loss: 0.3783\n","Epoch 24/40\n","9726/9726 [==============================] - 0s 44us/sample - loss: 0.2635 - val_loss: 0.3766\n","Epoch 25/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2644 - val_loss: 0.3768\n","Epoch 26/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2617 - val_loss: 0.3727\n","Epoch 27/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.2604 - val_loss: 0.3712\n","Epoch 28/40\n","9726/9726 [==============================] - 0s 44us/sample - loss: 0.2585 - val_loss: 0.3698\n","Epoch 29/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2577 - val_loss: 0.3695\n","Epoch 30/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2567 - val_loss: 0.3659\n","Epoch 31/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2559 - val_loss: 0.3649\n","Epoch 32/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2555 - val_loss: 0.3920\n","Epoch 33/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2613 - val_loss: 0.3643\n","Epoch 34/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2541 - val_loss: 0.3641\n","Epoch 35/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.2520 - val_loss: 0.3608\n","Epoch 36/40\n","9726/9726 [==============================] - 0s 40us/sample - loss: 0.2522 - val_loss: 0.3608\n","Epoch 37/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2532 - val_loss: 0.3620\n","Epoch 38/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.2507 - val_loss: 0.3598\n","Epoch 39/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2519 - val_loss: 0.3748\n","Epoch 40/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2545 - val_loss: 0.3575\n","3040/3040 [==============================] - 0s 80us/sample - loss: 0.3995\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.5763 - val_loss: 0.3979\n","Epoch 2/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.3918 - val_loss: 0.3552\n","Epoch 3/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.3568 - val_loss: 0.3383\n","Epoch 4/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.3386 - val_loss: 0.3264\n","Epoch 5/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.3267 - val_loss: 0.3183\n","Epoch 6/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.3183 - val_loss: 0.3117\n","Epoch 7/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.3116 - val_loss: 0.3067\n","Epoch 8/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.3058 - val_loss: 0.3036\n","Epoch 9/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.3008 - val_loss: 0.2989\n","Epoch 10/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.2966 - val_loss: 0.2958\n","Epoch 11/40\n","9726/9726 [==============================] - 0s 40us/sample - loss: 0.2928 - val_loss: 0.2920\n","Epoch 12/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.2896 - val_loss: 0.2886\n","Epoch 13/40\n","9726/9726 [==============================] - 0s 40us/sample - loss: 0.2869 - val_loss: 0.2865\n","Epoch 14/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2838 - val_loss: 0.2838\n","Epoch 15/40\n","9726/9726 [==============================] - 0s 40us/sample - loss: 0.2813 - val_loss: 0.2807\n","Epoch 16/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2790 - val_loss: 0.2802\n","Epoch 17/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2766 - val_loss: 0.2757\n","Epoch 18/40\n","9726/9726 [==============================] - 0s 40us/sample - loss: 0.2742 - val_loss: 0.2738\n","Epoch 19/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2748 - val_loss: 0.2721\n","Epoch 20/40\n","9726/9726 [==============================] - 0s 46us/sample - loss: 0.2721 - val_loss: 0.2691\n","Epoch 21/40\n","9726/9726 [==============================] - 0s 40us/sample - loss: 0.2682 - val_loss: 0.2690\n","Epoch 22/40\n","9726/9726 [==============================] - 0s 45us/sample - loss: 0.2671 - val_loss: 0.2674\n","Epoch 23/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2671 - val_loss: 0.2652\n","Epoch 24/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2658 - val_loss: 0.2647\n","Epoch 25/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2629 - val_loss: 0.2616\n","Epoch 26/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2619 - val_loss: 0.2629\n","Epoch 27/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2617 - val_loss: 0.2594\n","Epoch 28/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2668 - val_loss: 0.2643\n","Epoch 29/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2615 - val_loss: 0.2568\n","Epoch 30/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2579 - val_loss: 0.2560\n","Epoch 31/40\n","9726/9726 [==============================] - 0s 39us/sample - loss: 0.2569 - val_loss: 0.2608\n","Epoch 32/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2576 - val_loss: 0.2587\n","Epoch 33/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2579 - val_loss: 0.2530\n","Epoch 34/40\n","9726/9726 [==============================] - 0s 40us/sample - loss: 0.2550 - val_loss: 0.2538\n","Epoch 35/40\n","9726/9726 [==============================] - 0s 42us/sample - loss: 0.2542 - val_loss: 0.2540\n","Epoch 36/40\n","9726/9726 [==============================] - 0s 40us/sample - loss: 0.2540 - val_loss: 0.2525\n","Epoch 37/40\n","9726/9726 [==============================] - 0s 40us/sample - loss: 0.2532 - val_loss: 0.2498\n","Epoch 38/40\n","9726/9726 [==============================] - 0s 43us/sample - loss: 0.2535 - val_loss: 0.2503\n","Epoch 39/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2542 - val_loss: 0.2534\n","Epoch 40/40\n","9726/9726 [==============================] - 0s 41us/sample - loss: 0.2522 - val_loss: 0.2476\n","3040/3040 [==============================] - 0s 78us/sample - loss: 0.4053\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.5523 - val_loss: 0.5254\n","Epoch 2/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.3820 - val_loss: 0.4831\n","Epoch 3/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.3492 - val_loss: 0.4632\n","Epoch 4/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.3319 - val_loss: 0.4492\n","Epoch 5/40\n","9727/9727 [==============================] - 0s 43us/sample - loss: 0.3205 - val_loss: 0.4395\n","Epoch 6/40\n","9727/9727 [==============================] - 0s 43us/sample - loss: 0.3124 - val_loss: 0.4326\n","Epoch 7/40\n","9727/9727 [==============================] - 0s 44us/sample - loss: 0.3058 - val_loss: 0.4251\n","Epoch 8/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.3004 - val_loss: 0.4213\n","Epoch 9/40\n","9727/9727 [==============================] - 0s 43us/sample - loss: 0.2958 - val_loss: 0.4172\n","Epoch 10/40\n","9727/9727 [==============================] - 0s 39us/sample - loss: 0.2918 - val_loss: 0.4121\n","Epoch 11/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2882 - val_loss: 0.4072\n","Epoch 12/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.2856 - val_loss: 0.4067\n","Epoch 13/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2822 - val_loss: 0.4012\n","Epoch 14/40\n","9727/9727 [==============================] - 0s 43us/sample - loss: 0.2790 - val_loss: 0.3976\n","Epoch 15/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2772 - val_loss: 0.4017\n","Epoch 16/40\n","9727/9727 [==============================] - 0s 40us/sample - loss: 0.2754 - val_loss: 0.3907\n","Epoch 17/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.2721 - val_loss: 0.3909\n","Epoch 18/40\n","9727/9727 [==============================] - 0s 40us/sample - loss: 0.2701 - val_loss: 0.3856\n","Epoch 19/40\n","9727/9727 [==============================] - 0s 43us/sample - loss: 0.2722 - val_loss: 0.3895\n","Epoch 20/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2677 - val_loss: 0.3819\n","Epoch 21/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.2658 - val_loss: 0.3793\n","Epoch 22/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2635 - val_loss: 0.3770\n","Epoch 23/40\n","9727/9727 [==============================] - 0s 40us/sample - loss: 0.2636 - val_loss: 0.3769\n","Epoch 24/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.2618 - val_loss: 0.3789\n","Epoch 25/40\n","9727/9727 [==============================] - 0s 43us/sample - loss: 0.2598 - val_loss: 0.3718\n","Epoch 26/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2610 - val_loss: 0.3716\n","Epoch 27/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2580 - val_loss: 0.3682\n","Epoch 28/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.2577 - val_loss: 0.3667\n","Epoch 29/40\n","9727/9727 [==============================] - 0s 40us/sample - loss: 0.2564 - val_loss: 0.3684\n","Epoch 30/40\n","9727/9727 [==============================] - 0s 44us/sample - loss: 0.2550 - val_loss: 0.3661\n","Epoch 31/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.2536 - val_loss: 0.3631\n","Epoch 32/40\n","9727/9727 [==============================] - 0s 43us/sample - loss: 0.2558 - val_loss: 0.3624\n","Epoch 33/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2533 - val_loss: 0.3612\n","Epoch 34/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2515 - val_loss: 0.3580\n","Epoch 35/40\n","9727/9727 [==============================] - 0s 43us/sample - loss: 0.2538 - val_loss: 0.3589\n","Epoch 36/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.2506 - val_loss: 0.3566\n","Epoch 37/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2509 - val_loss: 0.3559\n","Epoch 38/40\n","9727/9727 [==============================] - 0s 40us/sample - loss: 0.2488 - val_loss: 0.3526\n","Epoch 39/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2482 - val_loss: 0.3525\n","Epoch 40/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2485 - val_loss: 0.3520\n","3040/3040 [==============================] - 0s 79us/sample - loss: 0.3998\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.5610 - val_loss: 0.5051\n","Epoch 2/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.3852 - val_loss: 0.4638\n","Epoch 3/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.3504 - val_loss: 0.4445\n","Epoch 4/40\n","9727/9727 [==============================] - 0s 43us/sample - loss: 0.3330 - val_loss: 0.4332\n","Epoch 5/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.3215 - val_loss: 0.4215\n","Epoch 6/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.3130 - val_loss: 0.4146\n","Epoch 7/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.3066 - val_loss: 0.4080\n","Epoch 8/40\n","9727/9727 [==============================] - 0s 40us/sample - loss: 0.3008 - val_loss: 0.4047\n","Epoch 9/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2963 - val_loss: 0.3977\n","Epoch 10/40\n","9727/9727 [==============================] - 0s 43us/sample - loss: 0.2923 - val_loss: 0.3953\n","Epoch 11/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.2885 - val_loss: 0.3897\n","Epoch 12/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.2850 - val_loss: 0.3883\n","Epoch 13/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2825 - val_loss: 0.3830\n","Epoch 14/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2790 - val_loss: 0.3778\n","Epoch 15/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2766 - val_loss: 0.3743\n","Epoch 16/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2742 - val_loss: 0.3711\n","Epoch 17/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2725 - val_loss: 0.3688\n","Epoch 18/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2701 - val_loss: 0.3648\n","Epoch 19/40\n","9727/9727 [==============================] - 0s 44us/sample - loss: 0.2683 - val_loss: 0.3631\n","Epoch 20/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2673 - val_loss: 0.3607\n","Epoch 21/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.2642 - val_loss: 0.3569\n","Epoch 22/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2632 - val_loss: 0.3534\n","Epoch 23/40\n","9727/9727 [==============================] - 0s 40us/sample - loss: 0.2622 - val_loss: 0.3526\n","Epoch 24/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.2612 - val_loss: 0.3515\n","Epoch 25/40\n","9727/9727 [==============================] - 0s 43us/sample - loss: 0.2603 - val_loss: 0.3499\n","Epoch 26/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.2602 - val_loss: 0.3492\n","Epoch 27/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2579 - val_loss: 0.3458\n","Epoch 28/40\n","9727/9727 [==============================] - 0s 40us/sample - loss: 0.2559 - val_loss: 0.3433\n","Epoch 29/40\n","9727/9727 [==============================] - 0s 43us/sample - loss: 0.2560 - val_loss: 0.3419\n","Epoch 30/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.2553 - val_loss: 0.3448\n","Epoch 31/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2551 - val_loss: 0.3409\n","Epoch 32/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.2529 - val_loss: 0.3378\n","Epoch 33/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.2528 - val_loss: 0.3405\n","Epoch 34/40\n","9727/9727 [==============================] - 0s 42us/sample - loss: 0.2540 - val_loss: 0.3419\n","Epoch 35/40\n","9727/9727 [==============================] - 0s 40us/sample - loss: 0.2524 - val_loss: 0.3341\n","Epoch 36/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.2501 - val_loss: 0.3337\n","Epoch 37/40\n","9727/9727 [==============================] - 0s 43us/sample - loss: 0.2495 - val_loss: 0.3328\n","Epoch 38/40\n","9727/9727 [==============================] - 0s 40us/sample - loss: 0.2503 - val_loss: 0.3333\n","Epoch 39/40\n","9727/9727 [==============================] - 0s 44us/sample - loss: 0.2484 - val_loss: 0.3323\n","Epoch 40/40\n","9727/9727 [==============================] - 0s 41us/sample - loss: 0.2532 - val_loss: 0.3379\n","3040/3040 [==============================] - 0s 85us/sample - loss: 0.4053\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 2s 241us/sample - loss: 0.4702 - val_loss: 0.7118\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.3115 - val_loss: 0.6763\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2834 - val_loss: 0.6563\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2657 - val_loss: 0.6435\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.2529 - val_loss: 0.6255\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2440 - val_loss: 0.6185\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2336 - val_loss: 0.6040\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2275 - val_loss: 0.6016\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2215 - val_loss: 0.5884\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 70us/sample - loss: 0.2161 - val_loss: 0.5814\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.2106 - val_loss: 0.5777\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2049 - val_loss: 0.5681\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2003 - val_loss: 0.5621\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1965 - val_loss: 0.5571\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1932 - val_loss: 0.5596\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 75us/sample - loss: 0.1923 - val_loss: 0.5497\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1882 - val_loss: 0.5487\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1847 - val_loss: 0.5432\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1819 - val_loss: 0.5432\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1789 - val_loss: 0.5392\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1764 - val_loss: 0.5361\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.1739 - val_loss: 0.5346\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1734 - val_loss: 0.5336\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1705 - val_loss: 0.5315\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1694 - val_loss: 0.5286\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1682 - val_loss: 0.5299\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.1672 - val_loss: 0.5271\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1641 - val_loss: 0.5248\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1640 - val_loss: 0.5249\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.1610 - val_loss: 0.5250\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1612 - val_loss: 0.5277\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1607 - val_loss: 0.5277\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1604 - val_loss: 0.5253\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1577 - val_loss: 0.5255\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.1560 - val_loss: 0.5221\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 70us/sample - loss: 0.1549 - val_loss: 0.5206\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1535 - val_loss: 0.5212\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.1540 - val_loss: 0.5213\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1530 - val_loss: 0.5225\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1539 - val_loss: 0.5230\n","3040/3040 [==============================] - 0s 101us/sample - loss: 0.4462\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 70us/sample - loss: 0.5164 - val_loss: 0.5553\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3711 - val_loss: 0.5187\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.3393 - val_loss: 0.4829\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.3217 - val_loss: 0.4687\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3071 - val_loss: 0.4669\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2965 - val_loss: 0.4537\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2874 - val_loss: 0.4393\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2789 - val_loss: 0.4427\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2733 - val_loss: 0.4464\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2711 - val_loss: 0.4349\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2612 - val_loss: 0.4307\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2559 - val_loss: 0.4279\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2515 - val_loss: 0.4207\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2473 - val_loss: 0.4144\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2442 - val_loss: 0.4247\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 71us/sample - loss: 0.2399 - val_loss: 0.4262\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.2348 - val_loss: 0.4054\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.2307 - val_loss: 0.4059\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 70us/sample - loss: 0.2296 - val_loss: 0.4172\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2271 - val_loss: 0.4182\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2232 - val_loss: 0.4037\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2196 - val_loss: 0.3984\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2181 - val_loss: 0.4173\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2199 - val_loss: 0.4077\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 70us/sample - loss: 0.2144 - val_loss: 0.4070\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.2124 - val_loss: 0.4081\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2090 - val_loss: 0.4073\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2073 - val_loss: 0.4107\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2066 - val_loss: 0.4030\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2049 - val_loss: 0.4027\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2043 - val_loss: 0.4076\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 70us/sample - loss: 0.2012 - val_loss: 0.4032\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.2009 - val_loss: 0.4011\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.1996 - val_loss: 0.4027\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1979 - val_loss: 0.4065\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1974 - val_loss: 0.4120\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1966 - val_loss: 0.4063\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1945 - val_loss: 0.4094\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.1934 - val_loss: 0.4147\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 72us/sample - loss: 0.1935 - val_loss: 0.4091\n","3040/3040 [==============================] - 0s 94us/sample - loss: 0.3732\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.5236 - val_loss: 0.3856\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.3741 - val_loss: 0.3652\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.3447 - val_loss: 0.3455\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3253 - val_loss: 0.3389\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.3120 - val_loss: 0.3256\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.3003 - val_loss: 0.3207\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2923 - val_loss: 0.3094\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2858 - val_loss: 0.3079\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2782 - val_loss: 0.2998\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2715 - val_loss: 0.2945\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.2659 - val_loss: 0.2860\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2602 - val_loss: 0.2840\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2547 - val_loss: 0.2815\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.2537 - val_loss: 0.2776\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.2455 - val_loss: 0.2737\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2414 - val_loss: 0.2695\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2378 - val_loss: 0.2689\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2357 - val_loss: 0.2678\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2323 - val_loss: 0.2627\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2287 - val_loss: 0.2648\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2257 - val_loss: 0.2625\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 71us/sample - loss: 0.2222 - val_loss: 0.2612\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2186 - val_loss: 0.2598\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.2178 - val_loss: 0.2561\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2152 - val_loss: 0.2565\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2131 - val_loss: 0.2550\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 70us/sample - loss: 0.2113 - val_loss: 0.2526\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.2113 - val_loss: 0.2546\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2082 - val_loss: 0.2542\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.2074 - val_loss: 0.2522\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2057 - val_loss: 0.2510\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2039 - val_loss: 0.2498\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.2016 - val_loss: 0.2501\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 66us/sample - loss: 0.2001 - val_loss: 0.2495\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 68us/sample - loss: 0.1992 - val_loss: 0.2481\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 69us/sample - loss: 0.1974 - val_loss: 0.2493\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1970 - val_loss: 0.2489\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 65us/sample - loss: 0.1947 - val_loss: 0.2501\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1949 - val_loss: 0.2503\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 67us/sample - loss: 0.1954 - val_loss: 0.2471\n","3040/3040 [==============================] - 0s 97us/sample - loss: 0.3773\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.5093 - val_loss: 0.5343\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.3701 - val_loss: 0.5036\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.3427 - val_loss: 0.4848\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.3237 - val_loss: 0.4580\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.3105 - val_loss: 0.4555\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2980 - val_loss: 0.4434\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.2885 - val_loss: 0.4360\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2817 - val_loss: 0.4255\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.2753 - val_loss: 0.4231\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2690 - val_loss: 0.4125\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.2623 - val_loss: 0.4087\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2575 - val_loss: 0.3976\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2531 - val_loss: 0.3921\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2477 - val_loss: 0.3972\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2448 - val_loss: 0.3876\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 70us/sample - loss: 0.2409 - val_loss: 0.3873\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.2357 - val_loss: 0.3804\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.2321 - val_loss: 0.3762\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 70us/sample - loss: 0.2302 - val_loss: 0.3772\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 73us/sample - loss: 0.2266 - val_loss: 0.3767\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.2256 - val_loss: 0.3729\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 73us/sample - loss: 0.2211 - val_loss: 0.3678\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2183 - val_loss: 0.3701\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2160 - val_loss: 0.3643\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2141 - val_loss: 0.3634\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2113 - val_loss: 0.3606\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 71us/sample - loss: 0.2105 - val_loss: 0.3606\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 71us/sample - loss: 0.2096 - val_loss: 0.3556\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2080 - val_loss: 0.3622\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2059 - val_loss: 0.3563\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2037 - val_loss: 0.3548\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.2016 - val_loss: 0.3512\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2017 - val_loss: 0.3581\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2004 - val_loss: 0.3529\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1976 - val_loss: 0.3558\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.1965 - val_loss: 0.3518\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.1958 - val_loss: 0.3522\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.1948 - val_loss: 0.3505\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.1937 - val_loss: 0.3516\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.1930 - val_loss: 0.3575\n","3040/3040 [==============================] - 0s 99us/sample - loss: 0.3766\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.5123 - val_loss: 0.5078\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.3714 - val_loss: 0.4914\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 71us/sample - loss: 0.3412 - val_loss: 0.4695\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.3227 - val_loss: 0.4585\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.3089 - val_loss: 0.4428\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2988 - val_loss: 0.4362\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2910 - val_loss: 0.4302\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 70us/sample - loss: 0.2832 - val_loss: 0.4240\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2761 - val_loss: 0.4202\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2705 - val_loss: 0.4239\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2639 - val_loss: 0.4152\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.2587 - val_loss: 0.4087\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2541 - val_loss: 0.3996\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2497 - val_loss: 0.3996\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2454 - val_loss: 0.4089\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2422 - val_loss: 0.3989\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2379 - val_loss: 0.3955\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2353 - val_loss: 0.4104\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 70us/sample - loss: 0.2320 - val_loss: 0.4049\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2292 - val_loss: 0.4173\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2261 - val_loss: 0.3998\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2244 - val_loss: 0.4078\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2217 - val_loss: 0.3895\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2203 - val_loss: 0.4053\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2170 - val_loss: 0.3975\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2149 - val_loss: 0.4027\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2125 - val_loss: 0.4010\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 66us/sample - loss: 0.2111 - val_loss: 0.3978\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2099 - val_loss: 0.4030\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2086 - val_loss: 0.4012\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2076 - val_loss: 0.3978\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.2049 - val_loss: 0.3987\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2061 - val_loss: 0.4056\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 70us/sample - loss: 0.2038 - val_loss: 0.4010\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 68us/sample - loss: 0.2009 - val_loss: 0.3955\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.2003 - val_loss: 0.4026\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 69us/sample - loss: 0.1987 - val_loss: 0.4098\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.1972 - val_loss: 0.4092\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.1969 - val_loss: 0.3984\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 67us/sample - loss: 0.1965 - val_loss: 0.4068\n","3040/3040 [==============================] - 0s 95us/sample - loss: 0.3795\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 3s 289us/sample - loss: 0.5242 - val_loss: 0.7650\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.3667 - val_loss: 0.7408\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.3227 - val_loss: 0.7177\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 93us/sample - loss: 0.2985 - val_loss: 0.7082\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.2821 - val_loss: 0.7088\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2713 - val_loss: 0.7043\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2602 - val_loss: 0.6908\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2555 - val_loss: 0.6797\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2473 - val_loss: 0.6780\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2384 - val_loss: 0.6589\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.2320 - val_loss: 0.6448\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.2266 - val_loss: 0.6490\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2227 - val_loss: 0.6483\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.2194 - val_loss: 0.6365\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 93us/sample - loss: 0.2148 - val_loss: 0.6348\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 93us/sample - loss: 0.2095 - val_loss: 0.6291\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.2082 - val_loss: 0.6228\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2036 - val_loss: 0.6243\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2002 - val_loss: 0.6165\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.1994 - val_loss: 0.6136\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.1976 - val_loss: 0.6151\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.1963 - val_loss: 0.6104\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.1932 - val_loss: 0.6141\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.1905 - val_loss: 0.6099\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.1881 - val_loss: 0.6021\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.1851 - val_loss: 0.6086\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.1862 - val_loss: 0.6070\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.1842 - val_loss: 0.6057\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.1812 - val_loss: 0.6001\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.1788 - val_loss: 0.5984\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.1770 - val_loss: 0.6008\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.1783 - val_loss: 0.5934\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.1772 - val_loss: 0.5954\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.1749 - val_loss: 0.5920\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 98us/sample - loss: 0.1737 - val_loss: 0.5929\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.1712 - val_loss: 0.5966\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.1697 - val_loss: 0.6040\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.1707 - val_loss: 0.5944\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.1684 - val_loss: 0.5977\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.1662 - val_loss: 0.5985\n","3040/3040 [==============================] - 0s 116us/sample - loss: 0.5153\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.6892 - val_loss: 0.6844\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.5091 - val_loss: 0.6238\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 97us/sample - loss: 0.4467 - val_loss: 0.6331\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.4187 - val_loss: 0.6092\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.3971 - val_loss: 0.5975\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 98us/sample - loss: 0.3798 - val_loss: 0.6190\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 97us/sample - loss: 0.3682 - val_loss: 0.6030\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.3581 - val_loss: 0.6340\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.3485 - val_loss: 0.6198\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.3398 - val_loss: 0.6021\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.3317 - val_loss: 0.6142\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.3274 - val_loss: 0.6075\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.3208 - val_loss: 0.6191\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.3158 - val_loss: 0.5852\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.3109 - val_loss: 0.5811\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.3066 - val_loss: 0.5704\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.2997 - val_loss: 0.5584\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2953 - val_loss: 0.5982\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2912 - val_loss: 0.5790\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2903 - val_loss: 0.5968\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 93us/sample - loss: 0.2878 - val_loss: 0.5844\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 93us/sample - loss: 0.2831 - val_loss: 0.5609\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 92us/sample - loss: 0.2777 - val_loss: 0.5768\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 93us/sample - loss: 0.2741 - val_loss: 0.5671\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 92us/sample - loss: 0.2720 - val_loss: 0.5656\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2685 - val_loss: 0.5496\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2659 - val_loss: 0.5591\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2634 - val_loss: 0.5554\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2603 - val_loss: 0.5706\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2573 - val_loss: 0.5542\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2559 - val_loss: 0.5615\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2552 - val_loss: 0.5451\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2511 - val_loss: 0.5451\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 92us/sample - loss: 0.2499 - val_loss: 0.5468\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 97us/sample - loss: 0.2472 - val_loss: 0.5432\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2443 - val_loss: 0.5486\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 97us/sample - loss: 0.2418 - val_loss: 0.5490\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.2411 - val_loss: 0.5444\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2400 - val_loss: 0.5547\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.2395 - val_loss: 0.5507\n","3040/3040 [==============================] - 0s 118us/sample - loss: 0.4570\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.6602 - val_loss: 0.4834\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.4746 - val_loss: 0.4440\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.4263 - val_loss: 0.4191\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.3968 - val_loss: 0.4015\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 93us/sample - loss: 0.3788 - val_loss: 0.4111\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 93us/sample - loss: 0.3665 - val_loss: 0.3847\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.3546 - val_loss: 0.3805\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.3469 - val_loss: 0.3733\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 93us/sample - loss: 0.3396 - val_loss: 0.3590\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 93us/sample - loss: 0.3277 - val_loss: 0.3587\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.3220 - val_loss: 0.3508\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.3146 - val_loss: 0.3481\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.3104 - val_loss: 0.3405\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 93us/sample - loss: 0.3043 - val_loss: 0.3365\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2996 - val_loss: 0.3375\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2940 - val_loss: 0.3287\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2913 - val_loss: 0.3267\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2865 - val_loss: 0.3267\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2821 - val_loss: 0.3233\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2777 - val_loss: 0.3217\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.2768 - val_loss: 0.3166\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 97us/sample - loss: 0.2715 - val_loss: 0.3166\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2685 - val_loss: 0.3167\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2647 - val_loss: 0.3094\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.2625 - val_loss: 0.3083\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 97us/sample - loss: 0.2610 - val_loss: 0.3084\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2572 - val_loss: 0.3073\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2535 - val_loss: 0.3104\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2527 - val_loss: 0.3069\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2501 - val_loss: 0.3028\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2477 - val_loss: 0.3028\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2447 - val_loss: 0.3065\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2438 - val_loss: 0.3001\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2391 - val_loss: 0.3014\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.2378 - val_loss: 0.2967\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2352 - val_loss: 0.2988\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 96us/sample - loss: 0.2352 - val_loss: 0.2970\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2347 - val_loss: 0.2945\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 95us/sample - loss: 0.2304 - val_loss: 0.2995\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 94us/sample - loss: 0.2282 - val_loss: 0.3000\n","3040/3040 [==============================] - 0s 119us/sample - loss: 0.4624\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 95us/sample - loss: 0.6071 - val_loss: 0.6739\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.4605 - val_loss: 0.5939\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.4156 - val_loss: 0.5707\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 98us/sample - loss: 0.3895 - val_loss: 0.5518\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 95us/sample - loss: 0.3718 - val_loss: 0.5527\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.3594 - val_loss: 0.5373\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 95us/sample - loss: 0.3509 - val_loss: 0.5263\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 95us/sample - loss: 0.3404 - val_loss: 0.5194\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.3332 - val_loss: 0.5188\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 92us/sample - loss: 0.3235 - val_loss: 0.5069\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.3176 - val_loss: 0.5130\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.3106 - val_loss: 0.4936\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.3055 - val_loss: 0.5070\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.2993 - val_loss: 0.4940\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.2960 - val_loss: 0.4891\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 96us/sample - loss: 0.2884 - val_loss: 0.4731\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.2846 - val_loss: 0.4833\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.2813 - val_loss: 0.4693\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.2779 - val_loss: 0.4648\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.2722 - val_loss: 0.4610\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 95us/sample - loss: 0.2681 - val_loss: 0.4576\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.2665 - val_loss: 0.4546\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.2628 - val_loss: 0.4584\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 92us/sample - loss: 0.2588 - val_loss: 0.4546\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 92us/sample - loss: 0.2574 - val_loss: 0.4542\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 95us/sample - loss: 0.2547 - val_loss: 0.4514\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 96us/sample - loss: 0.2526 - val_loss: 0.4408\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.2517 - val_loss: 0.4394\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.2458 - val_loss: 0.4333\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 92us/sample - loss: 0.2428 - val_loss: 0.4335\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.2417 - val_loss: 0.4462\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 95us/sample - loss: 0.2398 - val_loss: 0.4340\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.2370 - val_loss: 0.4264\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.2336 - val_loss: 0.4193\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.2320 - val_loss: 0.4249\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.2318 - val_loss: 0.4221\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.2295 - val_loss: 0.4188\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.2278 - val_loss: 0.4193\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.2261 - val_loss: 0.4243\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 95us/sample - loss: 0.2251 - val_loss: 0.4225\n","3040/3040 [==============================] - 0s 118us/sample - loss: 0.4256\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 95us/sample - loss: 0.6822 - val_loss: 0.6576\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.5001 - val_loss: 0.5941\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 92us/sample - loss: 0.4456 - val_loss: 0.6187\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.4178 - val_loss: 0.5946\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 95us/sample - loss: 0.3978 - val_loss: 0.5788\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.3813 - val_loss: 0.5895\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.3679 - val_loss: 0.5792\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.3589 - val_loss: 0.5501\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.3488 - val_loss: 0.5524\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.3399 - val_loss: 0.5554\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 92us/sample - loss: 0.3341 - val_loss: 0.5599\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 92us/sample - loss: 0.3267 - val_loss: 0.5582\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.3198 - val_loss: 0.5446\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 92us/sample - loss: 0.3147 - val_loss: 0.5377\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 96us/sample - loss: 0.3084 - val_loss: 0.5479\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.3045 - val_loss: 0.5404\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.3000 - val_loss: 0.5528\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.2963 - val_loss: 0.5470\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.2925 - val_loss: 0.5307\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.2889 - val_loss: 0.5363\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.2854 - val_loss: 0.5254\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 96us/sample - loss: 0.2806 - val_loss: 0.5307\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.2770 - val_loss: 0.5395\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.2751 - val_loss: 0.5390\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.2715 - val_loss: 0.5268\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.2682 - val_loss: 0.5207\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.2664 - val_loss: 0.5230\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.2635 - val_loss: 0.5220\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.2635 - val_loss: 0.5182\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 92us/sample - loss: 0.2582 - val_loss: 0.5246\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.2573 - val_loss: 0.5197\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 95us/sample - loss: 0.2548 - val_loss: 0.5250\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.2524 - val_loss: 0.5101\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.2501 - val_loss: 0.5047\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 93us/sample - loss: 0.2479 - val_loss: 0.5234\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.2472 - val_loss: 0.5213\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 96us/sample - loss: 0.2441 - val_loss: 0.5153\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 96us/sample - loss: 0.2426 - val_loss: 0.5175\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.2413 - val_loss: 0.5164\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 94us/sample - loss: 0.2382 - val_loss: 0.5110\n","3040/3040 [==============================] - 0s 116us/sample - loss: 0.4645\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 3s 340us/sample - loss: 0.5602 - val_loss: 0.8064\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.4067 - val_loss: 0.7678\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.3501 - val_loss: 0.7597\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.3187 - val_loss: 0.7378\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.3016 - val_loss: 0.7380\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.2832 - val_loss: 0.7243\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 116us/sample - loss: 0.2744 - val_loss: 0.7300\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.2628 - val_loss: 0.7168\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.2546 - val_loss: 0.7225\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.2483 - val_loss: 0.7133\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.2431 - val_loss: 0.7037\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 116us/sample - loss: 0.2381 - val_loss: 0.6935\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 116us/sample - loss: 0.2350 - val_loss: 0.6991\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.2311 - val_loss: 0.6905\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.2275 - val_loss: 0.6888\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.2261 - val_loss: 0.6927\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.2207 - val_loss: 0.6897\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.2204 - val_loss: 0.6945\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 116us/sample - loss: 0.2167 - val_loss: 0.6806\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 120us/sample - loss: 0.2150 - val_loss: 0.6930\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.2167 - val_loss: 0.6997\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.2112 - val_loss: 0.6961\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.2118 - val_loss: 0.6857\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.2112 - val_loss: 0.6862\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.2093 - val_loss: 0.6838\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.2059 - val_loss: 0.6985\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 121us/sample - loss: 0.2077 - val_loss: 0.6886\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.2045 - val_loss: 0.6870\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.2030 - val_loss: 0.6855\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.2023 - val_loss: 0.6895\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.2023 - val_loss: 0.6954\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.1982 - val_loss: 0.6931\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.1988 - val_loss: 0.6928\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.1977 - val_loss: 0.6852\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.1981 - val_loss: 0.6878\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 120us/sample - loss: 0.1963 - val_loss: 0.6912\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.1938 - val_loss: 0.6889\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.1959 - val_loss: 0.6994\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.1949 - val_loss: 0.6980\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.1941 - val_loss: 0.6897\n","3040/3040 [==============================] - 0s 134us/sample - loss: 0.5980\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 116us/sample - loss: 0.9512 - val_loss: 0.8449\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 116us/sample - loss: 0.8589 - val_loss: 0.8361\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8520 - val_loss: 0.8264\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8499 - val_loss: 0.8265\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 120us/sample - loss: 0.8491 - val_loss: 0.8316\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8479 - val_loss: 0.8250\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8473 - val_loss: 0.8268\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8467 - val_loss: 0.8232\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8465 - val_loss: 0.8252\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8462 - val_loss: 0.8211\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8462 - val_loss: 0.8249\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8457 - val_loss: 0.8247\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8456 - val_loss: 0.8254\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8458 - val_loss: 0.8225\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8456 - val_loss: 0.8255\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 116us/sample - loss: 0.8456 - val_loss: 0.8229\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8448 - val_loss: 0.8242\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 120us/sample - loss: 0.8448 - val_loss: 0.8243\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 122us/sample - loss: 0.8451 - val_loss: 0.8276\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 120us/sample - loss: 0.8453 - val_loss: 0.8263\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 121us/sample - loss: 0.8451 - val_loss: 0.8247\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 116us/sample - loss: 0.8451 - val_loss: 0.8233\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8444 - val_loss: 0.8197\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 116us/sample - loss: 0.8446 - val_loss: 0.8276\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8452 - val_loss: 0.8245\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8444 - val_loss: 0.8236\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8448 - val_loss: 0.8269\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8475 - val_loss: 0.8261\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8498 - val_loss: 0.8294\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8496 - val_loss: 0.8222\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 121us/sample - loss: 0.8501 - val_loss: 0.8269\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8520 - val_loss: 0.8268\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 116us/sample - loss: 0.8527 - val_loss: 0.8287\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8456 - val_loss: 0.8285\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8451 - val_loss: 0.8258\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8449 - val_loss: 0.8304\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 120us/sample - loss: 0.8449 - val_loss: 0.8268\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 120us/sample - loss: 0.8449 - val_loss: 0.8300\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 122us/sample - loss: 0.8450 - val_loss: 0.8297\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8451 - val_loss: 0.8300\n","3040/3040 [==============================] - 0s 137us/sample - loss: 0.9018\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 1.0231 - val_loss: 0.7441\n","Epoch 2/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8955 - val_loss: 0.7473\n","Epoch 3/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8839 - val_loss: 0.7414\n","Epoch 4/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8825 - val_loss: 0.7415\n","Epoch 5/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8793 - val_loss: 0.7356\n","Epoch 6/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8789 - val_loss: 0.7405\n","Epoch 7/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8777 - val_loss: 0.7348\n","Epoch 8/40\n","9726/9726 [==============================] - 1s 120us/sample - loss: 0.8772 - val_loss: 0.7386\n","Epoch 9/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8781 - val_loss: 0.7301\n","Epoch 10/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8769 - val_loss: 0.7324\n","Epoch 11/40\n","9726/9726 [==============================] - 1s 116us/sample - loss: 0.8773 - val_loss: 0.7287\n","Epoch 12/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8784 - val_loss: 0.7279\n","Epoch 13/40\n","9726/9726 [==============================] - 1s 120us/sample - loss: 0.8795 - val_loss: 0.7332\n","Epoch 14/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8782 - val_loss: 0.7229\n","Epoch 15/40\n","9726/9726 [==============================] - 1s 116us/sample - loss: 0.8803 - val_loss: 0.7261\n","Epoch 16/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8829 - val_loss: 0.7277\n","Epoch 17/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8826 - val_loss: 0.7210\n","Epoch 18/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8824 - val_loss: 0.7205\n","Epoch 19/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8829 - val_loss: 0.7303\n","Epoch 20/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8831 - val_loss: 0.7248\n","Epoch 21/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8807 - val_loss: 0.7295\n","Epoch 22/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8810 - val_loss: 0.7287\n","Epoch 23/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8802 - val_loss: 0.7302\n","Epoch 24/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8794 - val_loss: 0.7312\n","Epoch 25/40\n","9726/9726 [==============================] - 1s 116us/sample - loss: 0.8800 - val_loss: 0.7276\n","Epoch 26/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8799 - val_loss: 0.7295\n","Epoch 27/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8795 - val_loss: 0.7281\n","Epoch 28/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8794 - val_loss: 0.7294\n","Epoch 29/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8804 - val_loss: 0.7329\n","Epoch 30/40\n","9726/9726 [==============================] - 1s 116us/sample - loss: 0.8796 - val_loss: 0.7311\n","Epoch 31/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8796 - val_loss: 0.7289\n","Epoch 32/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8791 - val_loss: 0.7276\n","Epoch 33/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8792 - val_loss: 0.7307\n","Epoch 34/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8794 - val_loss: 0.7313\n","Epoch 35/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8805 - val_loss: 0.7343\n","Epoch 36/40\n","9726/9726 [==============================] - 1s 119us/sample - loss: 0.8795 - val_loss: 0.7311\n","Epoch 37/40\n","9726/9726 [==============================] - 1s 117us/sample - loss: 0.8797 - val_loss: 0.7284\n","Epoch 38/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8799 - val_loss: 0.7297\n","Epoch 39/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8800 - val_loss: 0.7288\n","Epoch 40/40\n","9726/9726 [==============================] - 1s 118us/sample - loss: 0.8797 - val_loss: 0.7302\n","3040/3040 [==============================] - 0s 140us/sample - loss: 0.9212\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8870 - val_loss: 0.9242\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8596 - val_loss: 0.9255\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8579 - val_loss: 0.9197\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8583 - val_loss: 0.9318\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 116us/sample - loss: 0.8681 - val_loss: 0.9428\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8655 - val_loss: 0.9415\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 116us/sample - loss: 0.8655 - val_loss: 0.9379\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8530 - val_loss: 0.9120\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 116us/sample - loss: 0.8379 - val_loss: 0.9111\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8378 - val_loss: 0.9109\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 121us/sample - loss: 0.8375 - val_loss: 0.9069\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8374 - val_loss: 0.9066\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 119us/sample - loss: 0.8369 - val_loss: 0.9059\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 120us/sample - loss: 0.8368 - val_loss: 0.9053\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8365 - val_loss: 0.9059\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8362 - val_loss: 0.9049\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 116us/sample - loss: 0.8361 - val_loss: 0.9049\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8364 - val_loss: 0.9081\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8354 - val_loss: 0.9122\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8355 - val_loss: 0.9035\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 115us/sample - loss: 0.8352 - val_loss: 0.9024\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8348 - val_loss: 0.9072\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 116us/sample - loss: 0.8342 - val_loss: 0.9035\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 116us/sample - loss: 0.8349 - val_loss: 0.9083\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 119us/sample - loss: 0.8573 - val_loss: 0.9151\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8378 - val_loss: 0.9089\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8364 - val_loss: 0.9103\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8363 - val_loss: 0.9110\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8362 - val_loss: 0.9040\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 116us/sample - loss: 0.8373 - val_loss: 0.9066\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8352 - val_loss: 0.9046\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 119us/sample - loss: 0.8355 - val_loss: 0.9037\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 116us/sample - loss: 0.8390 - val_loss: 0.9049\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8379 - val_loss: 0.9048\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8368 - val_loss: 0.9135\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8359 - val_loss: 0.9062\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8357 - val_loss: 0.9099\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8364 - val_loss: 0.9105\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8361 - val_loss: 0.9107\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8357 - val_loss: 0.9031\n","3040/3040 [==============================] - 0s 135us/sample - loss: 0.9158\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8662 - val_loss: 0.8774\n","Epoch 2/40\n","9727/9727 [==============================] - 1s 116us/sample - loss: 0.8403 - val_loss: 0.8811\n","Epoch 3/40\n","9727/9727 [==============================] - 1s 122us/sample - loss: 0.8763 - val_loss: 0.9195\n","Epoch 4/40\n","9727/9727 [==============================] - 1s 120us/sample - loss: 0.8735 - val_loss: 0.9610\n","Epoch 5/40\n","9727/9727 [==============================] - 1s 121us/sample - loss: 0.8781 - val_loss: 0.9572\n","Epoch 6/40\n","9727/9727 [==============================] - 1s 121us/sample - loss: 0.8789 - val_loss: 0.9725\n","Epoch 7/40\n","9727/9727 [==============================] - 1s 120us/sample - loss: 0.8784 - val_loss: 0.9585\n","Epoch 8/40\n","9727/9727 [==============================] - 1s 120us/sample - loss: 0.8770 - val_loss: 0.9598\n","Epoch 9/40\n","9727/9727 [==============================] - 1s 120us/sample - loss: 0.8762 - val_loss: 0.9589\n","Epoch 10/40\n","9727/9727 [==============================] - 1s 120us/sample - loss: 0.8750 - val_loss: 0.9560\n","Epoch 11/40\n","9727/9727 [==============================] - 1s 119us/sample - loss: 0.8719 - val_loss: 0.9523\n","Epoch 12/40\n","9727/9727 [==============================] - 1s 116us/sample - loss: 0.8884 - val_loss: 1.0023\n","Epoch 13/40\n","9727/9727 [==============================] - 1s 116us/sample - loss: 0.9298 - val_loss: 1.0225\n","Epoch 14/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.9448 - val_loss: 1.0073\n","Epoch 15/40\n","9727/9727 [==============================] - 1s 120us/sample - loss: 0.9269 - val_loss: 1.0073\n","Epoch 16/40\n","9727/9727 [==============================] - 1s 119us/sample - loss: 0.9322 - val_loss: 1.0173\n","Epoch 17/40\n","9727/9727 [==============================] - 1s 119us/sample - loss: 0.8995 - val_loss: 0.9235\n","Epoch 18/40\n","9727/9727 [==============================] - 1s 119us/sample - loss: 0.8623 - val_loss: 0.9141\n","Epoch 19/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8428 - val_loss: 0.8792\n","Epoch 20/40\n","9727/9727 [==============================] - 1s 120us/sample - loss: 0.8412 - val_loss: 0.8867\n","Epoch 21/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8401 - val_loss: 0.8781\n","Epoch 22/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8482 - val_loss: 0.8900\n","Epoch 23/40\n","9727/9727 [==============================] - 1s 119us/sample - loss: 0.8352 - val_loss: 0.8737\n","Epoch 24/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8335 - val_loss: 0.8817\n","Epoch 25/40\n","9727/9727 [==============================] - 1s 119us/sample - loss: 0.8351 - val_loss: 0.8703\n","Epoch 26/40\n","9727/9727 [==============================] - 1s 116us/sample - loss: 0.8341 - val_loss: 0.8725\n","Epoch 27/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8395 - val_loss: 0.8942\n","Epoch 28/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8315 - val_loss: 0.8751\n","Epoch 29/40\n","9727/9727 [==============================] - 1s 116us/sample - loss: 0.8329 - val_loss: 0.8786\n","Epoch 30/40\n","9727/9727 [==============================] - 1s 116us/sample - loss: 0.8340 - val_loss: 0.8854\n","Epoch 31/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8339 - val_loss: 0.8807\n","Epoch 32/40\n","9727/9727 [==============================] - 1s 119us/sample - loss: 0.8331 - val_loss: 0.8772\n","Epoch 33/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8335 - val_loss: 0.8783\n","Epoch 34/40\n","9727/9727 [==============================] - 1s 116us/sample - loss: 0.8325 - val_loss: 0.8722\n","Epoch 35/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8327 - val_loss: 0.8738\n","Epoch 36/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8327 - val_loss: 0.8769\n","Epoch 37/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8329 - val_loss: 0.8789\n","Epoch 38/40\n","9727/9727 [==============================] - 1s 116us/sample - loss: 0.8327 - val_loss: 0.8821\n","Epoch 39/40\n","9727/9727 [==============================] - 1s 118us/sample - loss: 0.8330 - val_loss: 0.8817\n","Epoch 40/40\n","9727/9727 [==============================] - 1s 117us/sample - loss: 0.8332 - val_loss: 0.8830\n","3040/3040 [==============================] - 0s 139us/sample - loss: 0.9007\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 4s 432us/sample - loss: 0.7400 - val_loss: 0.9533\n","Epoch 2/40\n","9726/9726 [==============================] - 2s 158us/sample - loss: 0.6620 - val_loss: 0.9245\n","Epoch 3/40\n","9726/9726 [==============================] - 2s 156us/sample - loss: 0.6312 - val_loss: 0.8882\n","Epoch 4/40\n","9726/9726 [==============================] - 2s 160us/sample - loss: 0.5819 - val_loss: 0.8734\n","Epoch 5/40\n","9726/9726 [==============================] - 2s 157us/sample - loss: 0.5285 - val_loss: 0.8917\n","Epoch 6/40\n","9726/9726 [==============================] - 2s 159us/sample - loss: 0.4807 - val_loss: 0.8852\n","Epoch 7/40\n","9726/9726 [==============================] - 2s 164us/sample - loss: 0.4373 - val_loss: 0.8699\n","Epoch 8/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.4079 - val_loss: 0.8702\n","Epoch 9/40\n","9726/9726 [==============================] - 2s 156us/sample - loss: 0.3805 - val_loss: 0.8610\n","Epoch 10/40\n","9726/9726 [==============================] - 2s 157us/sample - loss: 0.3552 - val_loss: 0.8547\n","Epoch 11/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.3437 - val_loss: 0.8627\n","Epoch 12/40\n","9726/9726 [==============================] - 2s 158us/sample - loss: 0.3400 - val_loss: 0.8609\n","Epoch 13/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.3286 - val_loss: 0.8609\n","Epoch 14/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.3257 - val_loss: 0.8633\n","Epoch 15/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.3178 - val_loss: 0.8646\n","Epoch 16/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.3150 - val_loss: 0.8706\n","Epoch 17/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.3186 - val_loss: 0.8602\n","Epoch 18/40\n","9726/9726 [==============================] - 2s 160us/sample - loss: 0.3120 - val_loss: 0.8508\n","Epoch 19/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.3097 - val_loss: 0.8628\n","Epoch 20/40\n","9726/9726 [==============================] - 2s 159us/sample - loss: 0.3015 - val_loss: 0.8548\n","Epoch 21/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.3012 - val_loss: 0.8547\n","Epoch 22/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.2971 - val_loss: 0.8737\n","Epoch 23/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.2955 - val_loss: 0.8636\n","Epoch 24/40\n","9726/9726 [==============================] - 2s 159us/sample - loss: 0.2952 - val_loss: 0.8600\n","Epoch 25/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.2917 - val_loss: 0.8491\n","Epoch 26/40\n","9726/9726 [==============================] - 2s 164us/sample - loss: 0.2884 - val_loss: 0.8612\n","Epoch 27/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.2866 - val_loss: 0.8554\n","Epoch 28/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.2907 - val_loss: 0.8631\n","Epoch 29/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.2891 - val_loss: 0.8706\n","Epoch 30/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.2864 - val_loss: 0.8628\n","Epoch 31/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.2789 - val_loss: 0.8719\n","Epoch 32/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.2763 - val_loss: 0.8738\n","Epoch 33/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.2756 - val_loss: 0.8694\n","Epoch 34/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.2756 - val_loss: 0.8596\n","Epoch 35/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.2804 - val_loss: 0.8596\n","Epoch 36/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.2765 - val_loss: 0.8512\n","Epoch 37/40\n","9726/9726 [==============================] - 2s 160us/sample - loss: 0.2817 - val_loss: 0.8602\n","Epoch 38/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.2756 - val_loss: 0.8502\n","Epoch 39/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.2753 - val_loss: 0.8664\n","Epoch 40/40\n","9726/9726 [==============================] - 2s 159us/sample - loss: 0.2824 - val_loss: 0.8603\n","3040/3040 [==============================] - 1s 166us/sample - loss: 0.7833\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 1.0140 - val_loss: 0.8931\n","Epoch 2/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.9106 - val_loss: 0.8811\n","Epoch 3/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.9026 - val_loss: 0.8896\n","Epoch 4/40\n","9726/9726 [==============================] - 2s 160us/sample - loss: 0.9022 - val_loss: 0.8679\n","Epoch 5/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.9023 - val_loss: 0.8741\n","Epoch 6/40\n","9726/9726 [==============================] - 2s 158us/sample - loss: 0.8991 - val_loss: 0.8856\n","Epoch 7/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.9016 - val_loss: 0.8605\n","Epoch 8/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.9011 - val_loss: 0.8765\n","Epoch 9/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.8990 - val_loss: 0.8563\n","Epoch 10/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.8974 - val_loss: 0.8781\n","Epoch 11/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.9002 - val_loss: 0.8573\n","Epoch 12/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.8981 - val_loss: 0.8544\n","Epoch 13/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.8957 - val_loss: 0.8784\n","Epoch 14/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.8976 - val_loss: 0.8641\n","Epoch 15/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.8945 - val_loss: 0.8612\n","Epoch 16/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.8957 - val_loss: 0.8640\n","Epoch 17/40\n","9726/9726 [==============================] - 2s 160us/sample - loss: 0.8988 - val_loss: 0.8552\n","Epoch 18/40\n","9726/9726 [==============================] - 2s 164us/sample - loss: 0.8968 - val_loss: 0.8940\n","Epoch 19/40\n","9726/9726 [==============================] - 2s 165us/sample - loss: 0.8959 - val_loss: 0.8863\n","Epoch 20/40\n","9726/9726 [==============================] - 2s 164us/sample - loss: 0.8975 - val_loss: 0.8774\n","Epoch 21/40\n","9726/9726 [==============================] - 2s 164us/sample - loss: 0.8955 - val_loss: 0.8714\n","Epoch 22/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.8947 - val_loss: 0.8649\n","Epoch 23/40\n","9726/9726 [==============================] - 2s 165us/sample - loss: 0.8952 - val_loss: 0.8658\n","Epoch 24/40\n","9726/9726 [==============================] - 2s 166us/sample - loss: 0.8941 - val_loss: 0.8652\n","Epoch 25/40\n","9726/9726 [==============================] - 2s 164us/sample - loss: 0.8941 - val_loss: 0.8736\n","Epoch 26/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.8952 - val_loss: 0.8609\n","Epoch 27/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.8948 - val_loss: 0.8663\n","Epoch 28/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.8950 - val_loss: 0.8985\n","Epoch 29/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.8966 - val_loss: 0.8560\n","Epoch 30/40\n","9726/9726 [==============================] - 2s 164us/sample - loss: 0.8972 - val_loss: 0.8636\n","Epoch 31/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.8955 - val_loss: 0.8696\n","Epoch 32/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.8951 - val_loss: 0.8598\n","Epoch 33/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.8955 - val_loss: 0.8665\n","Epoch 34/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.8946 - val_loss: 0.8593\n","Epoch 35/40\n","9726/9726 [==============================] - 2s 164us/sample - loss: 0.8945 - val_loss: 0.8662\n","Epoch 36/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.8951 - val_loss: 0.8608\n","Epoch 37/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.8947 - val_loss: 0.8742\n","Epoch 38/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.8933 - val_loss: 0.8591\n","Epoch 39/40\n","9726/9726 [==============================] - 2s 159us/sample - loss: 0.8928 - val_loss: 0.8552\n","Epoch 40/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.8930 - val_loss: 0.8595\n","3040/3040 [==============================] - 0s 158us/sample - loss: 0.9407\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.9312 - val_loss: 0.7613\n","Epoch 2/40\n","9726/9726 [==============================] - 2s 159us/sample - loss: 0.8970 - val_loss: 0.7638\n","Epoch 3/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.8947 - val_loss: 0.7656\n","Epoch 4/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.8963 - val_loss: 0.7665\n","Epoch 5/40\n","9726/9726 [==============================] - 2s 165us/sample - loss: 0.8962 - val_loss: 0.7660\n","Epoch 6/40\n","9726/9726 [==============================] - 2s 164us/sample - loss: 0.8947 - val_loss: 0.7635\n","Epoch 7/40\n","9726/9726 [==============================] - 2s 164us/sample - loss: 0.8953 - val_loss: 0.7570\n","Epoch 8/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.8921 - val_loss: 0.7607\n","Epoch 9/40\n","9726/9726 [==============================] - 2s 164us/sample - loss: 0.8912 - val_loss: 0.7637\n","Epoch 10/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.8933 - val_loss: 0.7629\n","Epoch 11/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.8962 - val_loss: 0.7585\n","Epoch 12/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.8943 - val_loss: 0.7656\n","Epoch 13/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.8931 - val_loss: 0.7650\n","Epoch 14/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.8921 - val_loss: 0.7575\n","Epoch 15/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.8929 - val_loss: 0.7619\n","Epoch 16/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.8920 - val_loss: 0.7658\n","Epoch 17/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.8917 - val_loss: 0.7679\n","Epoch 18/40\n","9726/9726 [==============================] - 2s 164us/sample - loss: 0.8913 - val_loss: 0.7528\n","Epoch 19/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.8924 - val_loss: 0.7607\n","Epoch 20/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.8930 - val_loss: 0.7662\n","Epoch 21/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.8929 - val_loss: 0.7601\n","Epoch 22/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.8918 - val_loss: 0.7617\n","Epoch 23/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.8917 - val_loss: 0.7556\n","Epoch 24/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.8924 - val_loss: 0.7579\n","Epoch 25/40\n","9726/9726 [==============================] - 2s 165us/sample - loss: 0.8915 - val_loss: 0.7568\n","Epoch 26/40\n","9726/9726 [==============================] - 2s 160us/sample - loss: 0.8907 - val_loss: 0.7583\n","Epoch 27/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.8926 - val_loss: 0.7706\n","Epoch 28/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.8914 - val_loss: 0.7600\n","Epoch 29/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.8918 - val_loss: 0.7587\n","Epoch 30/40\n","9726/9726 [==============================] - 2s 161us/sample - loss: 0.8921 - val_loss: 0.7638\n","Epoch 31/40\n","9726/9726 [==============================] - 2s 163us/sample - loss: 0.8929 - val_loss: 0.7626\n","Epoch 32/40\n","9726/9726 [==============================] - 2s 160us/sample - loss: 0.8910 - val_loss: 0.7601\n","Epoch 33/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.8912 - val_loss: 0.7622\n","Epoch 34/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.8909 - val_loss: 0.7646\n","Epoch 35/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.8911 - val_loss: 0.7592\n","Epoch 36/40\n","9726/9726 [==============================] - 2s 159us/sample - loss: 0.8920 - val_loss: 0.7585\n","Epoch 37/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.8906 - val_loss: 0.7583\n","Epoch 38/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.8904 - val_loss: 0.7557\n","Epoch 39/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.8917 - val_loss: 0.7763\n","Epoch 40/40\n","9726/9726 [==============================] - 2s 162us/sample - loss: 0.8919 - val_loss: 0.7625\n","3040/3040 [==============================] - 0s 161us/sample - loss: 0.9348\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 2s 162us/sample - loss: 0.9816 - val_loss: 1.1050\n","Epoch 2/40\n","9727/9727 [==============================] - 2s 162us/sample - loss: 0.9463 - val_loss: 1.1170\n","Epoch 3/40\n","9727/9727 [==============================] - 2s 165us/sample - loss: 0.9409 - val_loss: 1.1072\n","Epoch 4/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.9366 - val_loss: 1.1002\n","Epoch 5/40\n","9727/9727 [==============================] - 2s 164us/sample - loss: 0.9304 - val_loss: 1.0889\n","Epoch 6/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.9330 - val_loss: 1.0880\n","Epoch 7/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.9317 - val_loss: 1.0898\n","Epoch 8/40\n","9727/9727 [==============================] - 2s 162us/sample - loss: 0.9373 - val_loss: 1.1108\n","Epoch 9/40\n","9727/9727 [==============================] - 2s 162us/sample - loss: 0.9314 - val_loss: 1.0897\n","Epoch 10/40\n","9727/9727 [==============================] - 2s 162us/sample - loss: 0.9328 - val_loss: 1.0923\n","Epoch 11/40\n","9727/9727 [==============================] - 2s 163us/sample - loss: 0.9330 - val_loss: 1.0991\n","Epoch 12/40\n","9727/9727 [==============================] - 2s 164us/sample - loss: 0.9347 - val_loss: 1.1034\n","Epoch 13/40\n","9727/9727 [==============================] - 2s 163us/sample - loss: 0.9262 - val_loss: 1.0794\n","Epoch 14/40\n","9727/9727 [==============================] - 2s 164us/sample - loss: 0.9234 - val_loss: 1.1234\n","Epoch 15/40\n","9727/9727 [==============================] - 2s 164us/sample - loss: 0.9390 - val_loss: 1.1096\n","Epoch 16/40\n","9727/9727 [==============================] - 2s 165us/sample - loss: 0.9379 - val_loss: 1.0990\n","Epoch 17/40\n","9727/9727 [==============================] - 2s 160us/sample - loss: 0.9387 - val_loss: 1.1060\n","Epoch 18/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.9374 - val_loss: 1.1185\n","Epoch 19/40\n","9727/9727 [==============================] - 2s 164us/sample - loss: 0.9379 - val_loss: 1.1290\n","Epoch 20/40\n","9727/9727 [==============================] - 2s 163us/sample - loss: 0.9382 - val_loss: 1.1122\n","Epoch 21/40\n","9727/9727 [==============================] - 2s 163us/sample - loss: 0.9148 - val_loss: 1.0333\n","Epoch 22/40\n","9727/9727 [==============================] - 2s 164us/sample - loss: 0.8827 - val_loss: 1.0052\n","Epoch 23/40\n","9727/9727 [==============================] - 2s 162us/sample - loss: 0.8794 - val_loss: 1.0319\n","Epoch 24/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.8802 - val_loss: 0.9976\n","Epoch 25/40\n","9727/9727 [==============================] - 2s 163us/sample - loss: 0.8804 - val_loss: 1.0059\n","Epoch 26/40\n","9727/9727 [==============================] - 2s 162us/sample - loss: 0.8785 - val_loss: 0.9968\n","Epoch 27/40\n","9727/9727 [==============================] - 2s 162us/sample - loss: 0.8807 - val_loss: 1.0082\n","Epoch 28/40\n","9727/9727 [==============================] - 2s 162us/sample - loss: 0.8790 - val_loss: 1.0142\n","Epoch 29/40\n","9727/9727 [==============================] - 2s 163us/sample - loss: 0.8784 - val_loss: 1.0105\n","Epoch 30/40\n","9727/9727 [==============================] - 2s 163us/sample - loss: 0.8817 - val_loss: 0.9963\n","Epoch 31/40\n","9727/9727 [==============================] - 2s 163us/sample - loss: 0.8811 - val_loss: 0.9897\n","Epoch 32/40\n","9727/9727 [==============================] - 2s 165us/sample - loss: 0.8800 - val_loss: 1.0148\n","Epoch 33/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.8793 - val_loss: 0.9942\n","Epoch 34/40\n","9727/9727 [==============================] - 2s 163us/sample - loss: 0.8789 - val_loss: 0.9993\n","Epoch 35/40\n","9727/9727 [==============================] - 2s 160us/sample - loss: 0.8796 - val_loss: 0.9989\n","Epoch 36/40\n","9727/9727 [==============================] - 2s 164us/sample - loss: 0.8782 - val_loss: 1.0307\n","Epoch 37/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.8791 - val_loss: 0.9823\n","Epoch 38/40\n","9727/9727 [==============================] - 2s 163us/sample - loss: 0.8781 - val_loss: 1.0162\n","Epoch 39/40\n","9727/9727 [==============================] - 2s 163us/sample - loss: 0.8789 - val_loss: 1.0012\n","Epoch 40/40\n","9727/9727 [==============================] - 2s 165us/sample - loss: 0.8789 - val_loss: 0.9884\n","3040/3040 [==============================] - 0s 160us/sample - loss: 0.9893\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 2s 166us/sample - loss: 1.0012 - val_loss: 1.0644\n","Epoch 2/40\n","9727/9727 [==============================] - 2s 169us/sample - loss: 0.9524 - val_loss: 1.0069\n","Epoch 3/40\n","9727/9727 [==============================] - 2s 162us/sample - loss: 0.9553 - val_loss: 1.0576\n","Epoch 4/40\n","9727/9727 [==============================] - 2s 169us/sample - loss: 0.9468 - val_loss: 1.0052\n","Epoch 5/40\n","9727/9727 [==============================] - 2s 164us/sample - loss: 0.9463 - val_loss: 1.0420\n","Epoch 6/40\n","9727/9727 [==============================] - 2s 165us/sample - loss: 0.9443 - val_loss: 1.0003\n","Epoch 7/40\n","9727/9727 [==============================] - 2s 165us/sample - loss: 0.9457 - val_loss: 1.0252\n","Epoch 8/40\n","9727/9727 [==============================] - 2s 168us/sample - loss: 0.9466 - val_loss: 0.9789\n","Epoch 9/40\n","9727/9727 [==============================] - 2s 163us/sample - loss: 0.9456 - val_loss: 0.9937\n","Epoch 10/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.9426 - val_loss: 0.9806\n","Epoch 11/40\n","9727/9727 [==============================] - 2s 162us/sample - loss: 0.9422 - val_loss: 1.0039\n","Epoch 12/40\n","9727/9727 [==============================] - 2s 158us/sample - loss: 0.9424 - val_loss: 0.9914\n","Epoch 13/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.9418 - val_loss: 1.0457\n","Epoch 14/40\n","9727/9727 [==============================] - 2s 160us/sample - loss: 0.9429 - val_loss: 1.0126\n","Epoch 15/40\n","9727/9727 [==============================] - 2s 159us/sample - loss: 0.9430 - val_loss: 1.0271\n","Epoch 16/40\n","9727/9727 [==============================] - 2s 158us/sample - loss: 0.9416 - val_loss: 1.1014\n","Epoch 17/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.9433 - val_loss: 0.9862\n","Epoch 18/40\n","9727/9727 [==============================] - 2s 160us/sample - loss: 0.9444 - val_loss: 1.0027\n","Epoch 19/40\n","9727/9727 [==============================] - 2s 160us/sample - loss: 0.9435 - val_loss: 0.9749\n","Epoch 20/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.9419 - val_loss: 1.0169\n","Epoch 21/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.9418 - val_loss: 1.0509\n","Epoch 22/40\n","9727/9727 [==============================] - 2s 160us/sample - loss: 0.9435 - val_loss: 0.9902\n","Epoch 23/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.9414 - val_loss: 0.9835\n","Epoch 24/40\n","9727/9727 [==============================] - 2s 160us/sample - loss: 0.9426 - val_loss: 1.0148\n","Epoch 25/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.9441 - val_loss: 1.0417\n","Epoch 26/40\n","9727/9727 [==============================] - 2s 159us/sample - loss: 0.9419 - val_loss: 1.0027\n","Epoch 27/40\n","9727/9727 [==============================] - 2s 160us/sample - loss: 0.9435 - val_loss: 0.9980\n","Epoch 28/40\n","9727/9727 [==============================] - 2s 163us/sample - loss: 0.9421 - val_loss: 1.0564\n","Epoch 29/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.9425 - val_loss: 1.0394\n","Epoch 30/40\n","9727/9727 [==============================] - 2s 160us/sample - loss: 0.9452 - val_loss: 1.0594\n","Epoch 31/40\n","9727/9727 [==============================] - 2s 160us/sample - loss: 0.9413 - val_loss: 0.9920\n","Epoch 32/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.9422 - val_loss: 1.0637\n","Epoch 33/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.9411 - val_loss: 1.0073\n","Epoch 34/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.9424 - val_loss: 0.9999\n","Epoch 35/40\n","9727/9727 [==============================] - 2s 158us/sample - loss: 0.9421 - val_loss: 1.0118\n","Epoch 36/40\n","9727/9727 [==============================] - 2s 162us/sample - loss: 0.9396 - val_loss: 1.0012\n","Epoch 37/40\n","9727/9727 [==============================] - 2s 157us/sample - loss: 0.9411 - val_loss: 0.9725\n","Epoch 38/40\n","9727/9727 [==============================] - 2s 162us/sample - loss: 0.9433 - val_loss: 0.9799\n","Epoch 39/40\n","9727/9727 [==============================] - 2s 161us/sample - loss: 0.9430 - val_loss: 0.9959\n","Epoch 40/40\n","9727/9727 [==============================] - 2s 160us/sample - loss: 0.9431 - val_loss: 1.0322\n","3040/3040 [==============================] - 0s 162us/sample - loss: 0.9822\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 5s 490us/sample - loss: 0.8006 - val_loss: 0.9652\n","Epoch 2/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.6672 - val_loss: 0.9374\n","Epoch 3/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.6864 - val_loss: 0.9391\n","Epoch 4/40\n","9726/9726 [==============================] - 2s 173us/sample - loss: 0.6549 - val_loss: 0.9315\n","Epoch 5/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.6349 - val_loss: 0.9238\n","Epoch 6/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.5718 - val_loss: 0.9129\n","Epoch 7/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.5335 - val_loss: 0.9331\n","Epoch 8/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.5111 - val_loss: 0.9251\n","Epoch 9/40\n","9726/9726 [==============================] - 2s 175us/sample - loss: 0.5074 - val_loss: 0.9590\n","Epoch 10/40\n","9726/9726 [==============================] - 2s 180us/sample - loss: 0.5051 - val_loss: 0.9566\n","Epoch 11/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.4780 - val_loss: 0.9211\n","Epoch 12/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.4934 - val_loss: 0.9508\n","Epoch 13/40\n","9726/9726 [==============================] - 2s 179us/sample - loss: 0.4866 - val_loss: 0.9456\n","Epoch 14/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.4808 - val_loss: 0.9520\n","Epoch 15/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.4692 - val_loss: 0.9307\n","Epoch 16/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.5043 - val_loss: 0.9224\n","Epoch 17/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.5023 - val_loss: 0.9615\n","Epoch 18/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.4721 - val_loss: 0.9439\n","Epoch 19/40\n","9726/9726 [==============================] - 2s 175us/sample - loss: 0.4441 - val_loss: 0.9283\n","Epoch 20/40\n","9726/9726 [==============================] - 2s 175us/sample - loss: 0.4225 - val_loss: 0.9420\n","Epoch 21/40\n","9726/9726 [==============================] - 2s 179us/sample - loss: 0.4123 - val_loss: 0.9227\n","Epoch 22/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.3998 - val_loss: 0.9289\n","Epoch 23/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.3874 - val_loss: 0.9186\n","Epoch 24/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.3832 - val_loss: 0.9268\n","Epoch 25/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.3807 - val_loss: 0.9428\n","Epoch 26/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.3809 - val_loss: 0.9330\n","Epoch 27/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.3714 - val_loss: 0.9304\n","Epoch 28/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.3661 - val_loss: 0.9355\n","Epoch 29/40\n","9726/9726 [==============================] - 2s 179us/sample - loss: 0.3606 - val_loss: 0.9592\n","Epoch 30/40\n","9726/9726 [==============================] - 2s 175us/sample - loss: 0.3595 - val_loss: 0.9525\n","Epoch 31/40\n","9726/9726 [==============================] - 2s 179us/sample - loss: 0.3585 - val_loss: 0.9322\n","Epoch 32/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.3597 - val_loss: 0.9688\n","Epoch 33/40\n","9726/9726 [==============================] - 2s 179us/sample - loss: 0.3752 - val_loss: 0.9651\n","Epoch 34/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.3682 - val_loss: 0.9559\n","Epoch 35/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.4002 - val_loss: 0.9479\n","Epoch 36/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.3922 - val_loss: 0.9600\n","Epoch 37/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.3651 - val_loss: 0.9723\n","Epoch 38/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.3606 - val_loss: 0.9542\n","Epoch 39/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.3765 - val_loss: 0.9483\n","Epoch 40/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.3624 - val_loss: 0.9828\n","3040/3040 [==============================] - 1s 173us/sample - loss: 0.8675\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 1.1235 - val_loss: 0.8567\n","Epoch 2/40\n","9726/9726 [==============================] - 2s 179us/sample - loss: 0.8926 - val_loss: 0.8701\n","Epoch 3/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.8917 - val_loss: 0.8756\n","Epoch 4/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8827 - val_loss: 0.8677\n","Epoch 5/40\n","9726/9726 [==============================] - 2s 179us/sample - loss: 0.8819 - val_loss: 0.8589\n","Epoch 6/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.8872 - val_loss: 0.8759\n","Epoch 7/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.8817 - val_loss: 0.8628\n","Epoch 8/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.8803 - val_loss: 0.8759\n","Epoch 9/40\n","9726/9726 [==============================] - 2s 180us/sample - loss: 0.8814 - val_loss: 0.8664\n","Epoch 10/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.8878 - val_loss: 0.8712\n","Epoch 11/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8828 - val_loss: 0.8702\n","Epoch 12/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.8800 - val_loss: 0.8815\n","Epoch 13/40\n","9726/9726 [==============================] - 2s 180us/sample - loss: 0.8806 - val_loss: 0.8626\n","Epoch 14/40\n","9726/9726 [==============================] - 2s 180us/sample - loss: 0.8865 - val_loss: 0.8875\n","Epoch 15/40\n","9726/9726 [==============================] - 2s 179us/sample - loss: 0.8821 - val_loss: 0.8735\n","Epoch 16/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.8799 - val_loss: 0.8544\n","Epoch 17/40\n","9726/9726 [==============================] - 2s 180us/sample - loss: 0.8811 - val_loss: 0.8699\n","Epoch 18/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.8805 - val_loss: 0.8643\n","Epoch 19/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.8805 - val_loss: 0.8822\n","Epoch 20/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8845 - val_loss: 0.8666\n","Epoch 21/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.8848 - val_loss: 0.8618\n","Epoch 22/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8802 - val_loss: 0.8648\n","Epoch 23/40\n","9726/9726 [==============================] - 2s 188us/sample - loss: 0.8808 - val_loss: 0.8661\n","Epoch 24/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8813 - val_loss: 0.8625\n","Epoch 25/40\n","9726/9726 [==============================] - 2s 179us/sample - loss: 0.8816 - val_loss: 0.8591\n","Epoch 26/40\n","9726/9726 [==============================] - 2s 182us/sample - loss: 0.8830 - val_loss: 0.8731\n","Epoch 27/40\n","9726/9726 [==============================] - 2s 181us/sample - loss: 0.8805 - val_loss: 0.8925\n","Epoch 28/40\n","9726/9726 [==============================] - 2s 181us/sample - loss: 0.8810 - val_loss: 0.8617\n","Epoch 29/40\n","9726/9726 [==============================] - 2s 199us/sample - loss: 0.8797 - val_loss: 0.8862\n","Epoch 30/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8828 - val_loss: 0.8680\n","Epoch 31/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.8805 - val_loss: 0.8634\n","Epoch 32/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8814 - val_loss: 0.8601\n","Epoch 33/40\n","9726/9726 [==============================] - 2s 179us/sample - loss: 0.8809 - val_loss: 0.8686\n","Epoch 34/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.8792 - val_loss: 0.8620\n","Epoch 35/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.8827 - val_loss: 0.8576\n","Epoch 36/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.8813 - val_loss: 0.8702\n","Epoch 37/40\n","9726/9726 [==============================] - 2s 179us/sample - loss: 0.8809 - val_loss: 0.8758\n","Epoch 38/40\n","9726/9726 [==============================] - 2s 179us/sample - loss: 0.8801 - val_loss: 0.8687\n","Epoch 39/40\n","9726/9726 [==============================] - 2s 179us/sample - loss: 0.8838 - val_loss: 0.8576\n","Epoch 40/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8810 - val_loss: 0.8623\n","3040/3040 [==============================] - 1s 183us/sample - loss: 0.9397\n","Train on 9726 samples, validate on 2432 samples\n","Epoch 1/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.9149 - val_loss: 0.7372\n","Epoch 2/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8900 - val_loss: 0.7551\n","Epoch 3/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.8868 - val_loss: 0.7529\n","Epoch 4/40\n","9726/9726 [==============================] - 2s 179us/sample - loss: 0.8890 - val_loss: 0.7394\n","Epoch 5/40\n","9726/9726 [==============================] - 2s 175us/sample - loss: 0.8840 - val_loss: 0.7492\n","Epoch 6/40\n","9726/9726 [==============================] - 2s 180us/sample - loss: 0.8827 - val_loss: 0.7366\n","Epoch 7/40\n","9726/9726 [==============================] - 2s 179us/sample - loss: 0.8837 - val_loss: 0.7426\n","Epoch 8/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.8881 - val_loss: 0.7465\n","Epoch 9/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8853 - val_loss: 0.7481\n","Epoch 10/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.8837 - val_loss: 0.7428\n","Epoch 11/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8882 - val_loss: 0.7385\n","Epoch 12/40\n","9726/9726 [==============================] - 2s 179us/sample - loss: 0.8838 - val_loss: 0.7471\n","Epoch 13/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.8816 - val_loss: 0.7423\n","Epoch 14/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8815 - val_loss: 0.7418\n","Epoch 15/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.8801 - val_loss: 0.7411\n","Epoch 16/40\n","9726/9726 [==============================] - 2s 175us/sample - loss: 0.8780 - val_loss: 0.7365\n","Epoch 17/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.8769 - val_loss: 0.7363\n","Epoch 18/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8763 - val_loss: 0.7381\n","Epoch 19/40\n","9726/9726 [==============================] - 2s 179us/sample - loss: 0.8757 - val_loss: 0.7358\n","Epoch 20/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8745 - val_loss: 0.7331\n","Epoch 21/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8745 - val_loss: 0.7332\n","Epoch 22/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.8738 - val_loss: 0.7347\n","Epoch 23/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.8737 - val_loss: 0.7307\n","Epoch 24/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.8731 - val_loss: 0.7353\n","Epoch 25/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8726 - val_loss: 0.7286\n","Epoch 26/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.8729 - val_loss: 0.7322\n","Epoch 27/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.8725 - val_loss: 0.7339\n","Epoch 28/40\n","9726/9726 [==============================] - 2s 178us/sample - loss: 0.9392 - val_loss: 0.7534\n","Epoch 29/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.8965 - val_loss: 0.7625\n","Epoch 30/40\n","9726/9726 [==============================] - 2s 174us/sample - loss: 0.8885 - val_loss: 0.7469\n","Epoch 31/40\n","9726/9726 [==============================] - 2s 183us/sample - loss: 0.8834 - val_loss: 0.7514\n","Epoch 32/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.8822 - val_loss: 0.7412\n","Epoch 33/40\n","9726/9726 [==============================] - 2s 174us/sample - loss: 0.8805 - val_loss: 0.7431\n","Epoch 34/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8795 - val_loss: 0.7392\n","Epoch 35/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.8780 - val_loss: 0.7387\n","Epoch 36/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8791 - val_loss: 0.7407\n","Epoch 37/40\n","9726/9726 [==============================] - 2s 181us/sample - loss: 0.8770 - val_loss: 0.7368\n","Epoch 38/40\n","9726/9726 [==============================] - 2s 175us/sample - loss: 0.8760 - val_loss: 0.7340\n","Epoch 39/40\n","9726/9726 [==============================] - 2s 177us/sample - loss: 0.8756 - val_loss: 0.7371\n","Epoch 40/40\n","9726/9726 [==============================] - 2s 176us/sample - loss: 0.8761 - val_loss: 0.7369\n","3040/3040 [==============================] - 1s 176us/sample - loss: 0.9148\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 2s 177us/sample - loss: 0.8675 - val_loss: 0.9150\n","Epoch 2/40\n","9727/9727 [==============================] - 2s 175us/sample - loss: 0.8340 - val_loss: 0.9162\n","Epoch 3/40\n","9727/9727 [==============================] - 2s 175us/sample - loss: 0.8296 - val_loss: 0.9010\n","Epoch 4/40\n","9727/9727 [==============================] - 2s 176us/sample - loss: 0.8261 - val_loss: 0.8983\n","Epoch 5/40\n","9727/9727 [==============================] - 2s 177us/sample - loss: 0.8242 - val_loss: 0.9040\n","Epoch 6/40\n","9727/9727 [==============================] - 2s 174us/sample - loss: 0.8229 - val_loss: 0.8979\n","Epoch 7/40\n","9727/9727 [==============================] - 2s 175us/sample - loss: 0.8215 - val_loss: 0.8993\n","Epoch 8/40\n","9727/9727 [==============================] - 2s 175us/sample - loss: 0.8517 - val_loss: 0.9321\n","Epoch 9/40\n","9727/9727 [==============================] - 2s 178us/sample - loss: 0.8409 - val_loss: 0.9055\n","Epoch 10/40\n","9727/9727 [==============================] - 2s 177us/sample - loss: 0.8314 - val_loss: 0.9069\n","Epoch 11/40\n","9727/9727 [==============================] - 2s 175us/sample - loss: 0.8287 - val_loss: 0.9023\n","Epoch 12/40\n","9727/9727 [==============================] - 2s 174us/sample - loss: 0.8267 - val_loss: 0.9071\n","Epoch 13/40\n","9727/9727 [==============================] - 2s 176us/sample - loss: 0.8260 - val_loss: 0.9028\n","Epoch 14/40\n","9727/9727 [==============================] - 2s 175us/sample - loss: 0.8258 - val_loss: 0.9026\n","Epoch 15/40\n","9727/9727 [==============================] - 2s 176us/sample - loss: 0.8251 - val_loss: 0.9003\n","Epoch 16/40\n","9727/9727 [==============================] - 2s 175us/sample - loss: 0.8246 - val_loss: 0.9033\n","Epoch 17/40\n","9727/9727 [==============================] - 2s 176us/sample - loss: 0.8243 - val_loss: 0.9014\n","Epoch 18/40\n","9727/9727 [==============================] - 2s 174us/sample - loss: 0.8240 - val_loss: 0.9028\n","Epoch 19/40\n","9727/9727 [==============================] - 2s 175us/sample - loss: 0.8238 - val_loss: 0.8986\n","Epoch 20/40\n","9727/9727 [==============================] - 2s 174us/sample - loss: 0.8233 - val_loss: 0.8996\n","Epoch 21/40\n","9727/9727 [==============================] - 2s 179us/sample - loss: 0.8232 - val_loss: 0.8973\n","Epoch 22/40\n","9727/9727 [==============================] - 2s 178us/sample - loss: 0.8233 - val_loss: 0.9010\n","Epoch 23/40\n","9727/9727 [==============================] - 2s 179us/sample - loss: 0.8227 - val_loss: 0.9026\n","Epoch 24/40\n","9727/9727 [==============================] - 2s 176us/sample - loss: 0.8224 - val_loss: 0.9008\n","Epoch 25/40\n","9727/9727 [==============================] - 2s 179us/sample - loss: 0.8225 - val_loss: 0.9045\n","Epoch 26/40\n","9727/9727 [==============================] - 2s 178us/sample - loss: 0.8231 - val_loss: 0.9007\n","Epoch 27/40\n","9727/9727 [==============================] - 2s 178us/sample - loss: 0.8230 - val_loss: 0.9019\n","Epoch 28/40\n","9727/9727 [==============================] - 2s 173us/sample - loss: 0.8223 - val_loss: 0.9007\n","Epoch 29/40\n","9727/9727 [==============================] - 2s 174us/sample - loss: 0.8221 - val_loss: 0.9016\n","Epoch 30/40\n","9727/9727 [==============================] - 2s 175us/sample - loss: 0.8224 - val_loss: 0.9040\n","Epoch 31/40\n","9727/9727 [==============================] - 2s 175us/sample - loss: 0.8229 - val_loss: 0.8986\n","Epoch 32/40\n","9727/9727 [==============================] - 2s 175us/sample - loss: 0.8230 - val_loss: 0.8959\n","Epoch 33/40\n","9727/9727 [==============================] - 2s 174us/sample - loss: 0.8231 - val_loss: 0.9010\n","Epoch 34/40\n","9727/9727 [==============================] - 2s 176us/sample - loss: 0.8222 - val_loss: 0.8989\n","Epoch 35/40\n","9727/9727 [==============================] - 2s 173us/sample - loss: 0.8221 - val_loss: 0.8979\n","Epoch 36/40\n","9727/9727 [==============================] - 2s 173us/sample - loss: 0.8228 - val_loss: 0.8976\n","Epoch 37/40\n","9727/9727 [==============================] - 2s 174us/sample - loss: 0.8221 - val_loss: 0.9015\n","Epoch 38/40\n","9727/9727 [==============================] - 2s 175us/sample - loss: 0.8221 - val_loss: 0.9036\n","Epoch 39/40\n","9727/9727 [==============================] - 2s 178us/sample - loss: 0.8223 - val_loss: 0.9005\n","Epoch 40/40\n","9727/9727 [==============================] - 2s 178us/sample - loss: 0.8224 - val_loss: 0.9010\n","3040/3040 [==============================] - 1s 176us/sample - loss: 0.9028\n","Train on 9727 samples, validate on 2431 samples\n","Epoch 1/40\n","9727/9727 [==============================] - 2s 176us/sample - loss: 0.9702 - val_loss: 0.9371\n","Epoch 2/40\n","9727/9727 [==============================] - 2s 174us/sample - loss: 0.8736 - val_loss: 0.9153\n","Epoch 3/40\n","9727/9727 [==============================] - 2s 176us/sample - loss: 0.8656 - val_loss: 0.8961\n","Epoch 4/40\n","9727/9727 [==============================] - 2s 176us/sample - loss: 0.8643 - val_loss: 0.9097\n","Epoch 5/40\n","9727/9727 [==============================] - 2s 175us/sample - loss: 0.8606 - val_loss: 0.9136\n","Epoch 6/40\n","9727/9727 [==============================] - 2s 177us/sample - loss: 0.8605 - val_loss: 0.9241\n","Epoch 7/40\n","9727/9727 [==============================] - 2s 175us/sample - loss: 0.8592 - val_loss: 0.8905\n","Epoch 8/40\n","9727/9727 [==============================] - 2s 178us/sample - loss: 0.8592 - val_loss: 0.8916\n","Epoch 9/40\n","9727/9727 [==============================] - 2s 180us/sample - loss: 0.8622 - val_loss: 0.9034\n","Epoch 10/40\n","9727/9727 [==============================] - 2s 178us/sample - loss: 0.8590 - val_loss: 0.9121\n","Epoch 11/40\n","9727/9727 [==============================] - 2s 198us/sample - loss: 0.8581 - val_loss: 0.9044\n","Epoch 12/40\n","9727/9727 [==============================] - 2s 181us/sample - loss: 0.8563 - val_loss: 0.9222\n","Epoch 13/40\n","9727/9727 [==============================] - 2s 176us/sample - loss: 0.8582 - val_loss: 0.9085\n","Epoch 14/40\n","9727/9727 [==============================] - 2s 176us/sample - loss: 0.8577 - val_loss: 0.9180\n","Epoch 15/40\n","9727/9727 [==============================] - 2s 176us/sample - loss: 0.8563 - val_loss: 0.8955\n","Epoch 16/40\n","9727/9727 [==============================] - 2s 179us/sample - loss: 0.8556 - val_loss: 0.9064\n","Epoch 17/40\n","9727/9727 [==============================] - 2s 177us/sample - loss: 0.8563 - val_loss: 0.9097\n","Epoch 18/40\n","9727/9727 [==============================] - 2s 176us/sample - loss: 0.8572 - val_loss: 0.9063\n","Epoch 19/40\n","9727/9727 [==============================] - 2s 176us/sample - loss: 0.8554 - val_loss: 0.8999\n","Epoch 20/40\n","9727/9727 [==============================] - 2s 179us/sample - loss: 0.8543 - val_loss: 0.9121\n","Epoch 21/40\n","9727/9727 [==============================] - 2s 178us/sample - loss: 0.8571 - val_loss: 0.8926\n","Epoch 22/40\n","9727/9727 [==============================] - 2s 177us/sample - loss: 0.8562 - val_loss: 0.8873\n","Epoch 23/40\n","9727/9727 [==============================] - 2s 178us/sample - loss: 0.8541 - val_loss: 0.9116\n","Epoch 24/40\n","9727/9727 [==============================] - 2s 177us/sample - loss: 0.8542 - val_loss: 0.9079\n","Epoch 25/40\n","9727/9727 [==============================] - 2s 176us/sample - loss: 0.8544 - val_loss: 0.9081\n","Epoch 26/40\n","9727/9727 [==============================] - 2s 179us/sample - loss: 0.8548 - val_loss: 0.9096\n","Epoch 27/40\n","9727/9727 [==============================] - 2s 178us/sample - loss: 0.8540 - val_loss: 0.9188\n","Epoch 28/40\n","9727/9727 [==============================] - 2s 178us/sample - loss: 0.8543 - val_loss: 0.9011\n","Epoch 29/40\n","9727/9727 [==============================] - 2s 182us/sample - loss: 0.8548 - val_loss: 0.8975\n","Epoch 30/40\n","9727/9727 [==============================] - 2s 179us/sample - loss: 0.8551 - val_loss: 0.9005\n","Epoch 31/40\n","9727/9727 [==============================] - 2s 177us/sample - loss: 0.8555 - val_loss: 0.9106\n","Epoch 32/40\n","9727/9727 [==============================] - 2s 181us/sample - loss: 0.8541 - val_loss: 0.8929\n","Epoch 33/40\n","9727/9727 [==============================] - 2s 180us/sample - loss: 0.8560 - val_loss: 0.9032\n","Epoch 34/40\n","9727/9727 [==============================] - 2s 178us/sample - loss: 0.8539 - val_loss: 0.8932\n","Epoch 35/40\n","9727/9727 [==============================] - 2s 177us/sample - loss: 0.8567 - val_loss: 0.8968\n","Epoch 36/40\n","9727/9727 [==============================] - 2s 179us/sample - loss: 0.8539 - val_loss: 0.9068\n","Epoch 37/40\n","9727/9727 [==============================] - 2s 179us/sample - loss: 0.8542 - val_loss: 0.8988\n","Epoch 38/40\n","9727/9727 [==============================] - 2s 182us/sample - loss: 0.8534 - val_loss: 0.8941\n","Epoch 39/40\n","9727/9727 [==============================] - 2s 179us/sample - loss: 0.8545 - val_loss: 0.9038\n","Epoch 40/40\n","9727/9727 [==============================] - 2s 179us/sample - loss: 0.8551 - val_loss: 0.8970\n","3040/3040 [==============================] - 1s 210us/sample - loss: 0.9458\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2CAN7Uk2vwVu","colab_type":"code","outputId":"fe60c5c6-aa1a-46fd-b65c-7c65427ed30a","executionInfo":{"status":"ok","timestamp":1571352712956,"user_tz":300,"elapsed":992,"user":{"displayName":"Brendan Sanderson","photoUrl":"","userId":"05405297474563600402"}},"colab":{"base_uri":"https://localhost:8080/","height":350}},"source":["plt.figure(figsize=(8, 5))\n","plt.plot(list(range(1,19,3)),testing_errors)\n","plt.title(\"Keras Autoencoder with Different Depths\")\n","plt.xlabel(\"Depth\")\n","plt.ylabel(\"Testing loss\")\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAfUAAAFNCAYAAAAZ0fYJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 576x360 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"DhF0aA8C66eY","colab_type":"text"},"source":["Interestingly, it appears that the loss at different depths has a minimum at d = 4. This is likely because it is a good median point for where the model isn't too complex for data, but has enough layers to properly funnel the data. If there are less than 4 layers, more data is probably lost as the feature reduction takes too long. If there are more than 4 layers, 40 epochs is probably not enough to fully train all of the layers. At higher numbers of layers like d > 8, the error spikes upward. This is likely because the model is just way too deep to train at all without an extremely large amount of epochs."]},{"cell_type":"markdown","metadata":{"id":"V3KegXZKdYE-","colab_type":"text"},"source":["### d. What are a two uses of an autoencoder for gene expression data?"]},{"cell_type":"markdown","metadata":{"id":"1XuvEFL8itK_","colab_type":"text"},"source":["One use of autoencoders in gene expression data is to reduce noise in the data. Through autoencoding, we can get rid of any abnormalities in the data. This is extremely useful as a model may percieve these abnormalities to be predictive of the classification.\n","\n","Another use is to reduce dimensions through encoding. There are a lot of dimensions in gene expression data, so reducing dimensions in an efficient way is extremely valuable as autoencoding perserves only the more meaningful data when it reduces dimensions."]},{"cell_type":"markdown","metadata":{"id":"xSrr30kzudyK","colab_type":"text"},"source":["## Writeup"]},{"cell_type":"markdown","metadata":{"id":"E6vPH5LmuzMl","colab_type":"text"},"source":["In part 1 of this homework, I learned a lot about machine learning classification with 2 classes. In part (a), I showed that sometimes more data is not better. Using some reason because you decide upon your final dataset can be useful. In this case, one may expect all of the genes to be a better dataset. However, if you think about, a dataset that only includes the protein encoding genes is more relevant. In part (b), I looked at different classical machine learning algorithms. I used LinearSVC, SVC, AdaBoost, KNeighbors, DecisionTree and RandomForest. I was not surprised to see LinearSVC perform the best, but it was interesting to see how quick RandomForest is while maintaining a high degree of accuracy. Also, it was noteworthy that KNeighbors had the least false positives. In part (c), I looked at the effect of feature reduction. It was interesting to see how many features could be removed and only have less than a 0.5% effect on accuracy while running in 1/8 the time. Finally in part (d), I looked at the effect of training with less samples. Similar to part (c), the testing error didnt increase dramatically until we only used around 400 samples. Thus, this proves that removing samples also works as a technique to reduce runtime if there are a significant number of samples and the runtime is too long. \n","\n","In part 2 of the homework, I do the same thing, but with many classes. In (a), we get the same results as I did in part 1. This is not surprising, given the results in part 1a. In (b), I used AdaBoost, KNeighbors, DecisionTree and RandomForest. It was interesting to see how the results were similar except for AdaBoost, which performed worse. RandomForest performed the best, which wasnt surprising as it did in part 1. Overall, it was cool to see the difference in the results between functions in part 1 and part 2. In (c), the testing error actually seemed to generally decrease as the number of features decreased. This is surprising as I expected the opposite as in part 1. One potential hypothesis I have for why this is the case is that a lot of features are only affiliated with having a tumor versus not having a tumor. Thus, they may provide misinformation in actually classifying the tumor. The decrease is very little, so the other option is just that the RandomTree model doesnt use a lot of these features and thus the effect of removing features is minimal. In (d), the learning curve shows the effect of using less datasets. As in (d) of part 1, the testing error increases with less datasets. However, the training error increases at a faster pace than it does in part 1. This makes sense as there is more classifications and thus, it takes more data to train the model. The most significant difference to me between the many classifications than the 2 classifications is the runtime. All models took significantly longer in part 2. This shows the difficulty of using so many classifications. It appears that it is exponentially harder to train with more classifications.  \n","\n","In part 3, I looked at the effect of an autoencoder. In (b), I looked at the effect of changing the bottleneck size. The effect was as expected where the smaller the bottleneck, the greater the loss. However, I was surprised that the loss was still so low even when the bottleneck was so small. In part (c), I look at the effect of the depth of the bottleneck. It was interesting to see that the both a small depth and a high depth dont perform well. The best was actually a median depth where it was still able to condense the data at a good rate, but wasnt too large to train. Overall, it is clear that autoencoding is a great way to reduce the noise in data and even condense data if desired. \n","\n","Exploring the gene expression datasets was extremely insightful. It was cool to see how predictive the gene data could be and how easy it was to train a model that could determine whether the person has a tumor or not and what kind of tumor they have if they do. This is something that is broadly applicable. One thing that was so surprising was that there were so many ways to do this. A variety of different models were able to reasonably predict the tumor status, which truly shows the power of machine learning. If tumor status is something that is so easy to determine, the medical possibilities of more complex machine learning must be endless.\n"]}]}